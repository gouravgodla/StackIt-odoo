[
  {
    "id": 173,
    "title": "Time series for count data, with counts < 20",
    "body": "<p>I recently started working for a tuberculosis clinic.  We meet periodically to discuss the number of TB cases we're currently treating, the number of tests administered, etc.  I'd like to start modeling these counts so that we're not just guessing whether something is unusual or not.  Unfortunately, I've had very little training in time series, and most of my exposure has been to models for very continuous data (stock prices) or very large numbers of counts (influenza).  But we deal with 0-18 cases per month (mean 6.68, median 7, var 12.3), which are distributed like this:</p>\n\n<p>[image lost to the mists  of time]</p>\n\n<p>[image eaten by a grue]</p>\n\n<p>I've found a few articles that address models like this, but I'd greatly appreciate hearing suggestions from you - both for approaches and for R packages that I could use to implement those approaches.</p>\n\n<p><strong>EDIT:</strong>  mbq's answer has forced me to think more carefully about what I'm asking here; I got too hung-up on the monthly counts and lost the actual focus of the question.  What I'd like to know is: does the (fairly visible) decline from, say, 2008 onward reflect a downward trend in the overall number of cases?  It looks to me like the number of cases monthly from 2001-2007 reflects a stable process; maybe some seasonality, but overall stable.  From 2008 through the present, it looks like that process is changing: the overall number of cases is declining, even though the monthly counts might wobble up and down due to randomness and seasonality.  How can I test if there's a real change in the process?  And if I can identify a decline, how could I use that trend and whatever seasonality there might be to estimate the number of cases we might see in the upcoming months?</p>\n\n<p>Whew.  Thanks for bearing with me.</p>\n",
    "tags": "r,time series",
    "answers": [
      "<p>I'm going to leave the main question alone, because I think I will get it wrong (although I too analyse data for a healthcare provider, and to be honest, if I had these data, I would just analyse them using standard techniques and hope for the best, they look pretty okay to me).</p>\n\n<p>As for R packages, I have found the TSA library and it's accompanying <a href=\"http://rads.stackoverflow.com/amzn/click/0387759581\" rel=\"nofollow\">book</a> very useful indeed. The <code>armasubsets</code> command, particularly, I think is a great time saver.</p>\n",
      "<p>Does it really need some advanced model? Based on what I know about TB, in case there is no epidemy the infections are stochastic acts and so the count form month N shouldn't be correlated with count from month N-1. (You can check this assumption with autocorrelation). If so, analyzing just the distribution of monthly counts may be sufficient to decide if some count is significantly higher than normal.<br>\nOn the other hand you can look for correlations with some other variables, like season, travel traffic, or anything that you can imagine that may be correlated. If you would found something like this, it could be then used for some data normalization.</p>\n",
      "<p>You might consider applying a <a href=\"http://gunston.gmu.edu/708/frTukey.asp\" rel=\"nofollow\">Tukey Control chart</a> to the data.</p>\n",
      "<p>You might want to have a look at <a href=\"http://cran.r-project.org/web/packages/strucchange/index.html\" rel=\"nofollow\">strucchange</a>: </p>\n\n<blockquote>\n  <p>Testing, monitoring and dating structural changes in (linear) regression models. strucchange features tests/methods from the generalized fluctuation test framework as well as from the F test (Chow test) framework. This includes methods to fit, plot and test fluctuation processes (e.g., CUSUM, MOSUM, recursive/moving estimates) and F statistics, respectively. It is possible to monitor incoming data online using fluctuation processes. Finally, the breakpoints in regression models with structural changes can be estimated together with confidence intervals. Emphasis is always given to methods for visualizing the data.\"</p>\n</blockquote>\n\n<p>PS. Nice graphics ;)</p>\n",
      "<p>To assess the historical trend, I'd use a gam with trend and seasonal components. For example</p>\n\n<pre><code>require(mgcv)\nrequire(forecast)\nx &lt;- ts(rpois(100,1+sin(seq(0,3*pi,l=100))),f=12)\ntt &lt;- 1:100\nseason &lt;- seasonaldummy(x)\nfit &lt;- gam(x ~ s(tt,k=5) + season, family=\"poisson\")\nplot(fit)\n</code></pre>\n\n<p>Then <code>summary(fit)</code> will give you a test of significance of the change in trend and the plot will give you some confidence intervals. The assumptions here are that the observations are independent and the conditional distribution is Poisson. Because the mean is allowed to change smoothly over time, these are not particularly strong assumptions.</p>\n\n<p>To forecast is more difficult as you need to project the trend into the future. If you are willing to accept a linear extrapolation of the trend at the end of the data (which is certainly dodgy but probably ok for a few months), then use</p>\n\n<pre><code>fcast &lt;- predict(fit,se.fit=TRUE,\n               newdata=list(tt=101:112,season=seasonaldummyf(x,h=12)))\n</code></pre>\n\n<p>To see the forecasts on the same graph:</p>\n\n<pre><code>plot(x,xlim=c(0,10.5))\nlines(ts(exp(fcast$fit),f=12,s=112/12),col=2)\nlines(ts(exp(fcast$fit-2*fcast$se),f=12,s=112/12),col=2,lty=2)\nlines(ts(exp(fcast$fit+2*fcast$se),f=12,s=112/12),col=2,lty=2)\n</code></pre>\n\n<p>You can spot the unusual months by looking for outliers in the (deviance) residuals of the fit.</p>\n",
      "<p>You may try to model your data using a Dynamic Generalized Linear Model (DGLM). In R, you can fit this kind of models using packages sspir and KFAS. In a sense, this is similar to the gam approach suggested by Rob, except that instead of assuming that the log mean of the Poisson observations be a smooth function of time, it assumes that it follows a stochastic dynamics.</p>\n",
      "<p>In response to your direct question \"How can I test if there's a real change in the process? And if I can identify a decline, how could I use that trend and whatever seasonality there might be to estimate the number of cases we might see in the upcoming months?\" Develop a Transfer Function Model ( ARMAX ) that readily explains period-to-period dependency including and seasonal ARIMA structure. Incorporate any Identifiable Level Shifts , Seasonal Pulses, Local Time Trends and PUlses that may have been suggested by empirical/analystical methods like Intervention Detection. IF THIS ROBUST MODEL INCLUDES A FACTOR/SERIES matching up with \"declines\" Then your prayers have been answerered. In the alternative simply add an hypothesized structure e.g. to test a time trend change at point T1 construct two dummies X1 = 1,1,2,3,,,,,,T and X2 = 0,0,0,0,0,0,0,1,2,3,4,5,.... WHERE THE ZEROES END AT PERIOD T1-1 . The test of the hypothesis of a significant trend change at time period T1 will be assessed using the \"t value\" for X2 .</p>\n\n<p>Edited 9/22/11</p>\n\n<p>Often, disease data like this has monthly effects since weather/temperature is often an unspecified causal . In the omission of the true caudsal series ARIMA models use memory or seasonal dummies as a surrogate. Additionally series like this can have level shifts and/or local time trends reflecting structural change over time. Exploiting the autoregressive structure in the data rather than imposing various artifacts like time and time square and time cubic etc. have been found to be quite useful and less presumptive and ad hoc. Care should also be taken to identify \"unusual values\" as they can often be useful in suggestng additional cause variables and at a minimum lead to robust estimates of the other model parameters. Finally we have found that the variability/paramaters may vary over times thus these model refinements may be in order.</p>\n",
      "<p>Escape from traditional enumerative statistics as Deming would suggest and venture into traditional analytical statistics - in this case, control charts.  See any books by Donald Wheeler PhD, particularly his \"Advanced Topics in SPC\" for more info.</p>\n",
      "<p>Often, disease data like this is performed with a generalized linear model, as its not necessarily a great application of time series analysis - the months often aren't all that correlated with each other.</p>\n\n<p>If I were given this data, here's what I would do (and indeed, have done with data similar to it):</p>\n\n<p>Create a \"time\" variable that's more accurately described as \"Months since 1/1/2000\" if I'm eyeballing your data correctly. Then I'd run a general linear model in R using the Poisson distribution (or Negative Binomial) and a log link with roughly the following form:</p>\n\n<p><code>log(Counts) = b0 + b1*t + b2*(t^2) + b3*cos(2pi*w*t) + b4*sin(2pi*w*t)</code></p>\n\n<p>Where t is the time described above, and w is 1/365 for a yearly disease like flu. Generally its 1/n, where n is the length of your disease's cycle. I don't know offhand what it is for TB.</p>\n\n<p>The two time trends will show you - outside normal seasonal variation - if you have meaningful variation over time.</p>\n"
    ],
    "createdAt": "2025-07-12T08:02:08Z",
    "updatedAt": "2025-07-12T08:02:08Z",
    "similar_questions": [
      109222,
      190106,
      26790,
      135573,
      880
    ]
  },
  {
    "id": 298,
    "title": "In linear regression, when is it appropriate to use the log of an independent variable instead of the actual values?",
    "body": "<p>Am I looking for a better behaved distribution for the independent variable in question, or to reduce the effect of outliers, or something else?</p>\n",
    "tags": "regression,distributions",
    "answers": [
      "<p>One typically takes the log of an input variable to scale it and change the distribution (e.g. to make it normally distributed).  It cannot be done blindly however; you need to be careful when making any scaling to ensure that the results are still interpretable.  </p>\n\n<p>This is discussed in most introductory statistics texts.  You can also read Andrew Gelman's paper on <a href=\"http://www.stat.columbia.edu/~gelman/research/published/standardizing7.pdf\">\"Scaling regression inputs by dividing by two standard deviations\"</a> for a discussion on this.  He also has a very nice discussion on this at the beginning of <a href=\"http://www.stat.columbia.edu/~gelman/arm/\">\"Data Analysis Using Regression and Multilevel/Hierarchical Models\"</a>.</p>\n\n<p>Taking the log is not an appropriate method for dealing with bad data/outliers.</p>\n",
      "<p>You tend to take logs of the data when there is a problem with the residuals. For example, if you plot the residuals against a particular covariate and observe an increasing/decreasing pattern (a funnel shape), then a transformation may be appropriate. Non-random residuals usually indicate that your model assumptions are wrong, i.e. non-normal data.</p>\n\n<p>Some data types automatically lend themselves to logarithmic transformations. For example, I usually take logs when dealing with concentrations or age. </p>\n\n<p>Although transformations aren't primarily used to deal outliers, they do help since taking logs squashes your data.</p>\n",
      "<p>Shane's point that taking the log to deal with bad data is well taken.  As is Colin's regarding the importance of normal residuals.  In practice I find that usually you can get normal residuals if the input and output variables are also relatively normal.  In practice this means eyeballing the distribution of the transformed and untransformed datasets and assuring oneself that they have become more normal and/or conducting tests of normality (e.g. Shapiro-Wilk or Kolmogorov-Smirnov tests) and determining whether the outcome is more normal.  Interpretablity and tradition are also important.  For example, in cognitive psychology log transforms of reaction time are often used, however, to me at least, the interpretation of a log RT is unclear.  Furthermore, one should be cautious using log transformed values as the shift in scale can change a main effect into an interaction and vice versa.</p>\n",
      "<p>I always tell students there are three reasons to transform a variable by taking the natural logarithm. The reason for logging the variable will determine whether you want to log the independent variable(s), dependent or both. To be clear throughout I'm talking about taking the natural logarithm. </p>\n\n<p>Firstly, to improve model fit as other posters have noted. For instance if your residuals aren't normally distributed then taking the logarithm of a skewed variable may improve the fit by altering the scale and making the variable more \"normally\" distributed. For instance, earnings is truncated at zero and often exhibits positive skew. If the variable has negative skew you could firstly invert the variable before taking the logarithm. I'm thinking here particularly of Likert scales that are inputed as continuous variables. While this usually applies to the dependent variable you occasionally have problems with the residuals (e.g. heteroscedasticity) caused by an independent variable which can be sometimes corrected by taking the logarithm of that variable. For example when running a model that explained lecturer evaluations on a set of lecturer and class covariates the variable \"class size\" (i.e. the number of students in the lecture) had outliers which induced heteroscedasticity because the variance in the lecturer evaluations was smaller in larger cohorts than smaller cohorts. Logging the student variable would help, although in this example either calculating Robust Standard Errors or using Weighted Least Squares may make interpretation easier.</p>\n\n<p>The second reason for logging one or more variables in the model is for interpretation. I call this convenience reason. If you log both your dependent (Y) and independent (X) variable(s) your regression coefficients ($\\beta$) will be elasticities and interpretation would go as follows: a 1% increase in X would lead to a <em>ceteris paribus</em> $\\beta$% increase in Y (on average). Logging only one side of the regression \"equation\" would lead to alternative interpretations as outlined below:</p>\n\n<p>Y and X -- a one unit increase in X would lead to a $\\beta$ increase/decrease in Y</p>\n\n<p>Log Y and Log X -- a 1% increase in X would lead to a $\\beta$% increase/decrease in Y </p>\n\n<p>Log Y and X -- a one unit increase in X would lead to a $\\beta*100$ % increase/decrease in Y</p>\n\n<p>Y and Log X -- a 1% increase in X would lead to a $\\beta/100$ increase/decrease in Y</p>\n\n<p>And finally there could be a theoretical reason for doing so. For example some models that we would like to estimate are multiplicative and therefore nonlinear. Taking logarithms allows these models to be estimated by linear regression. Good examples of this include the Cobb-Douglas production function in economics and the Mincer Equation in education. The Cobb-Douglas production function explains how inputs are converted into outputs:</p>\n\n<p>$$Y = A L^\\alpha K^\\beta $$</p>\n\n<p>where</p>\n\n<p>$Y$ is the total production or output of some entity e.g. firm, farm, etc.</p>\n\n<p>$A$ is the total factor productivity (the change in output not caused by the inputs e.g. by technology change or weather)</p>\n\n<p>$L$ is the labour input</p>\n\n<p>$K$ is the capital input</p>\n\n<p>$\\alpha$ &amp; $\\beta$ are output elasticities.</p>\n\n<p>Taking logarithms of this makes the function easy to estimate using OLS linear regression as such:</p>\n\n<p>$$\\log(Y) = \\log(A) + \\alpha\\log(L) + \\beta\\log(K)$$</p>\n",
      "<p>I always hesitate to jump into a thread with as many excellent responses as this, but it strikes me that few of the answers provide any reason to prefer the logarithm to some other transformation that \"squashes\" the data, such as a root or reciprocal.</p>\n\n<p>Before getting to that, let's <strong>recapitulate</strong> the wisdom in the existing answers in a more general way.  <em>Some</em> non-linear re-expression of the dependent variable is indicated when any of the following apply:</p>\n\n<ul>\n<li><p>The residuals have a skewed distribution.  The purpose of a transformation is to obtain residuals that are approximately symmetrically distributed (about zero, of course).</p></li>\n<li><p>The spread of the residuals changes systematically with the values of the dependent variable (\"heteroscedasticity\").  The purpose of the transformation is to remove that systematic change in spread, achieving approximate \"homoscedasticity.\"</p></li>\n<li><p>To linearize a relationship.</p></li>\n<li><p>When scientific theory indicates.  For example, chemistry often suggests expressing concentrations as logarithms (giving activities or even the well-known pH).</p></li>\n<li><p>When a more nebulous statistical theory suggests the residuals reflect \"random errors\" that do not accumulate additively.</p></li>\n<li><p>To simplify a model.  For example, sometimes a logarithm can simplify the number and complexity of \"interaction\" terms.</p></li>\n</ul>\n\n<p>(These indications can conflict with one another; in such cases, judgment is needed.)</p>\n\n<p>So, <strong>when is a <em>logarithm</em> specifically indicated</strong> instead of some other transformation?</p>\n\n<ul>\n<li><p>The residuals have a \"strongly\" positively skewed distribution.  In his book on EDA, John Tukey provides quantitative ways to estimate the transformation (within the family of Box-Cox, or power, transformations) based on rank statistics of the residuals.  It really comes down to the fact that if taking the log symmetrizes the residuals, it was probably the right form of re-expression; otherwise, some other re-expression is needed.</p></li>\n<li><p>When the SD of the residuals is directly proportional to the fitted values (and not to some power of the fitted values).</p></li>\n<li><p>When the relationship is close to exponential.</p></li>\n<li><p>When residuals are believed to reflect multiplicatively accumulating errors.</p></li>\n<li><p>You really want a model in which marginal changes in the explanatory variables are interpreted in terms of multiplicative (percentage) changes in the dependent variable.</p></li>\n</ul>\n\n<p>Finally, some <strong><em>non</em> - reasons to use a re-expression</strong>:</p>\n\n<ul>\n<li><p>Making outliers not look like outliers.  An outlier is a datum that does not fit some parsimonious, relatively simple description of the data.  Changing one's description in order to make outliers look better is usually an incorrect reversal of priorities: first obtain a scientifically valid, statistically good description of the data and then explore any outliers.  Don't let the occasional outlier determine how to describe the rest of the data!</p></li>\n<li><p>Because the software automatically did it.  (Enough said!)</p></li>\n<li><p>Because all the data are positive.  (Positivity often implies positive skewness, but it does not have to.  Furthermore, other transformations can work better.  For example, a root often works best with counted data.)</p></li>\n<li><p>To make \"bad\" data (perhaps of low quality) appear well behaved.</p></li>\n<li><p>To be able to plot the data.  (If a transformation is needed to be able to plot the data, it's probably needed for one or more good reasons already mentioned.  If the only reason for the transformation truly is for plotting, go ahead and do it--but <em>only</em> to plot the data.  Leave the data untransformed for analysis.)</p></li>\n</ul>\n",
      "<p>For more on whuber's excellent point about reasons to prefer the logarithm to some other transformations such as a root or reciprocal, but focussing on the unique <em>interpretability</em> of the regression coefficients resulting from log-transformation compared to other transformations, see:</p>\n\n<p>Oliver N. Keene. The log transformation is special. <em>Statistics in Medicine</em> 1995; 14(8):811-819. DOI:<a href=\"http://dx.doi.org/10.1002/sim.4780140810\">10.1002/sim.4780140810</a>. (PDF of dubious legality available at <a href=\"http://rds.epi-ucsf.org/ticr/syllabus/courses/25/2009/04/21/Lecture/readings/log.pdf\">http://rds.epi-ucsf.org/ticr/syllabus/courses/25/2009/04/21/Lecture/readings/log.pdf</a>).</p>\n\n<p>If you log the <em>independent</em> variable <em>x</em>  to base <em>b</em>, you can interpret the regression coefficient (and CI) as the change in the dependent variable <em>y</em>  per <em>b</em>-fold increase in <em>x</em>. (Logs to base 2 are therefore often useful as they correspond to the change in <em>y</em> per doubling in <em>x</em>, or logs to base 10 if <em>x</em> varies over many orders of magnitude, which is rarer). Other transformations, such as square root, have no such simple interpretation.</p>\n\n<p>If you log the <em>dependent</em> variable <em>y</em> (not the original question but one which several of the previous answers have addressed), then I find Tim Cole's idea of 'sympercents' attractive for presenting the results (i even used them in a paper once), though they don't seem to have caught on all that widely:</p>\n\n<p>Tim J Cole. Sympercents: symmetric percentage differences on the 100 log(e) scale simplify the presentation of log transformed data. <em>Statistics in Medicine</em> 2000; 19(22):3109-3125. DOI:<a href=\"http://www3.interscience.wiley.com/journal/75500451/abstract\">10.1002/1097-0258(20001130)19:22&lt;3109::AID-SIM558>3.0.CO;2-F</a> [I'm so glad <em>Stat Med</em> stopped using <a href=\"http://en.wikipedia.org/wiki/SICI\">SICIs</a> as DOIs...]</p>\n",
      "<p>I would like to respond to user1690130's question that was left as a comment to the first answer on Oct 26 '12 and reads as follows: <em>\"What about variables like population density in a region or the child-teacher ratio for each school district or the number of homicides per 1000 in the population? I have seen professors take the log of these variables. It is not clear to me why. For example, isn't the homicide rate already a percentage? The log would the the percentage change of the rate? Why would the log of child-teacher ratio be preferred?\"</em></p>\n\n<p>I was looking to answer a similar problem and wanted to share what my old stats coursebook (<em>Jeffrey Wooldridge. 2006. Introductory Econometrics - A Modern Approach, 4th Edition. Chapter 6 Multiple Regression Analysis: Further Issues. 191</em>) says about it. Wooldridge advises:</p>\n\n<blockquote>\n  <p>Variables that appear in a proportion or percent form, such as the unemployment rate, the participation rate in a pension plan, the percentage of students passing a standardized exam, and the arrest rate on reported crimes - <strong>can appear in either the original or logarithmic form,</strong> <strong>although there is a tendency to use them in level forms</strong>. This is because any regression coefficients involving the original variable - whether it is the dependent or the independent variable - will have a percentage point change interpretation. If we use, say, log(<em>unem</em>) in a regression, where <em>unem</em> is the percentage of unemployed individuals, we must be very careful to distinguish between a percentage point change and a percentage change. Remember, if <em>unem</em> goes from 8 to 9, this is an increase of one percentage point, but a 12.5% increase from the initial unemployment level. Using the log means that we are looking at the percentage change in the unemployment rate: log(9) - log(8) = 0.118 or 11.8%, which is the logarithmic approximation to the actual 12.5% increase.</p>\n</blockquote>\n\n<p>Based on this and piggybanking on whuber's earlier comment to user1690130's question, I would avoid using the logarithm of a density or percentage rate variable to keep interpretation simple unless using the log form produces a major tradeoff such as being able to reduce skewness of the density or rate variable.</p>\n",
      "<p>Transformation of an independent variable $X$ is one occasion where one can just be empirical without distorting inference as long as one is honest about the number of degrees of freedom in play.  One way is to use regression splines for continuous $X$ not already known to act linearly.  To me it's not a question of log vs. original scale; it's a question of which transformation of $X$ fits the data.  Normality of residuals is not a criterion here.</p>\n\n<p>When $X$ is extremely skewed, cubing $X$ as is needed in cubic spline functions results in extreme values that can sometimes cause numerical problems.  I solve this by fitting the cubic spline function on $\\sqrt[3]{X}$.  The R <code>rms</code> package considers the innermost variable as the predictor, so plotting predicted values will have $X$ on the $x$-axis.  Example:</p>\n\n<pre><code>require(rms)\ndd &lt;- datadist(mydata); options(datadist='dd')\ncr &lt;- function(x) x ^ (1/3)\nf &lt;- ols(y ~ rcs(cr(X), 5), data=mydata)\nggplot(Predict(f))  # plot spline of cr(X) against X\n</code></pre>\n\n<p>This fits a restricted cubic spline in $\\sqrt[3]{X}$ with 5 knots at default quantile locations.  The $X$ fit has 4 d.f. (one linear term, 3 nonlinear terms).  Confidence bands and tests of association respect these 4 d.f., fully recognizing \"transformation uncertainty\".</p>\n"
    ],
    "createdAt": "2025-07-12T08:02:08Z",
    "updatedAt": "2025-07-12T08:02:08Z",
    "similar_questions": [
      200822,
      156680,
      211696,
      70699,
      209369
    ]
  },
  {
    "id": 411,
    "title": "Motivation for Kolmogorov distance between distributions",
    "body": "<p>There are many ways to measure how similar two probability distributions are.  Among methods which are popular (in different circles) are:</p>\n\n<ol>\n<li><p>the Kolmogorov distance: the sup-distance between the distribution functions;</p></li>\n<li><p>the Kantorovich-Rubinstein distance: the maximum difference between the expectations w.r.t. the two distributions of functions with Lipschitz constant $1$, which also turns out to be the $L^1$ distance between the distribution functions;</p></li>\n<li><p>the bounded-Lipschitz distance: like the K-R distance but the functions are also required to have absolute value at most $1$.</p></li>\n</ol>\n\n<p>These have different advantages and disadvantages.  Only convergence in the sense of 3. actually corresponds precisely to convergence in distribution; convergence in the sense of 1. or 2. is slightly stronger in general.  (In particular, if $X_n=\\frac{1}{n}$ with probability $1$, then $X_n$ converges to $0$ in distribution, but not in the Kolmogorov distance. However, if the limit distribution is continuous then this pathology doesn't occur.) </p>\n\n<p>From the perspective of elementary probability or measure theory, 1. is very natural because it compares the probabilities of being in some set. A more sophisticated probabilistic perspective, on the other hand, tends to focus more on expectations than probabilities. Also, from the perspective of functional analysis, distances like 2. or 3. based on duality with some function space are very appealing, because there is a large set of mathematical tools for working with such things.</p>\n\n<p>However, my impression (correct me if I'm wrong!) is that in statistics, the Kolmogorov distance is the usually preferred way of measuring similarity of distributions.  I can guess one reason: if one of the distributions is discrete with finite support -- in particular, if it is the distribution of some real-world data -- then the Kolmogorov distance to a model distribution is easy to compute.  (The K-R distance would be slightly harder to compute, and the B-L distance would probably be impossible in practical terms.)</p>\n\n<p>So my question (finally) is, are there other reasons, either practical or theoretical, to favor the Kolmogorov distance (or some other distance) for statistical purposes?</p>\n",
    "tags": "distributions,probability,hypothesis testing",
    "answers": [
      "<p>Computational issues are the strongest argument I've heard one way or the other.  The single biggest advantage of the Kolmogorov distance is that it's very easy to compute analytically for pretty much any CDF.  Most other distance metrics don't have a closed-form expression except, sometimes, in the Gaussian case.</p>\n\n<p>The Kolmogorov distance of a sample also has a known sampling distribution given the CDF (I don't think most other ones do), which ends up being related to the Wiener process.  This is the basis for the Kolmogorov-Smirnoff test for comparing a sample to a distribution or two samples to each other.</p>\n\n<p>On a more functional-analysis note, the sup norm is nice in that (as you mention) it basically defines uniform convergence.  This leaves you with norm convergence implying pointwise convergence, and so you if you're clever about how you define your function sequences you can work within a RKHS and use all of the nice tools that that provides as well.</p>\n",
      "<p>Mark,</p>\n\n<p>the main reason of which I am aware for the use of K-S is because it arises naturally from Glivenko-Cantelli theorems in univariate empirical processes. The one reference I'd recommend is A.W.van der Vaart \"Asymptotic Statistics\", ch. 19. A more advanced monograph is \"Weak Convergence and Empirical Processes\" by Wellner and van der Vaart. </p>\n\n<p>I'd add two quick notes:</p>\n\n<ol>\n<li>another measure of distance commonly used in univariate distributions is the Cramer-von Mises distance, which is an L^2 distance;</li>\n<li>in general vector spaces different distances are employed; the space of interest in many papers is polish. A very good introduction is Billingsley's \"Convergence of Probability Measures\". </li>\n</ol>\n\n<p>I apologize if I can't be more specific. I hope this helps.</p>\n",
      "<p><strong>As a summary</strong>, my answer is : if you have an explicit expression or can figure out some how what your distance is measuring (what \"differences\" it gives weigth to), then you can say what it is better for. An other complementary way to analyse and compare such test is the minimax theory. </p>\n\n<p>At the end some test will be good for some alternatives and some for others. For a given set of alternatives it is sometime possible to show if your test has optimal property in the worst case: this is the minimax theory. </p>\n\n<hr>\n\n<p><strong>Some details</strong> </p>\n\n<p>Hence You can tell about the properties of two different test by regarding the set of alternative for which they are minimax (if such alternative exist) i.e. (using the word of Donoho and Jin) by comparing their \"optimal detection boudary\" <a href=\"http://projecteuclid.org/DPubS?service=UI&amp;version=1.0&amp;verb=Display&amp;handle=euclid.aos/1085408492\">http://projecteuclid.org/DPubS?service=UI&amp;version=1.0&amp;verb=Display&amp;handle=euclid.aos/1085408492</a>.</p>\n\n<p>Let me go distance by distance:</p>\n\n<ol>\n<li><p>KS distance is obtained calculating supremum of difference between empirical cdf and cdf. Being a suppremum it will be highly sensitive to local alternatives (local change in the cdf) but not with global change (at least using L2 distance between cdf would be less local (Am I openning open door ?)). However, the most important thing is that is uses the cdf. This implies an asymetry: you give more importance to the changes in the tail of your distribution.</p></li>\n<li><p>Wassertein metric  (what you meant by Kantorovitch Rubinstein ? )  <a href=\"http://en.wikipedia.org/wiki/Wasserstein_metric\">http://en.wikipedia.org/wiki/Wasserstein_metric</a> is ubiquitous and hence hard to compare. </p>\n\n<ul>\n<li>For the particular case of W2 it has been uses in <a href=\"http://projecteuclid.org/DPubS?service=UI&amp;version=1.0&amp;verb=Display&amp;handle=euclid.aos/1017938923\">http://projecteuclid.org/DPubS?service=UI&amp;version=1.0&amp;verb=Display&amp;handle=euclid.aos/1017938923</a>  and it is related to the L2 distance to inverse of cdf. My understanding is that it gives even more weight to the tails but I think you should read the paper to know more about it. </li>\n<li>For the case of the L1 distance between density function it will highly depend on how you estimate your dentity function from the data... but otherwise it seems to be a \"balanced test\" not giving importance to tails. </li>\n</ul></li>\n</ol>\n\n<hr>\n\n<p>To recall and extend the comment I made which complete the answer: </p>\n\n<p>I know you did not meant to be exhaustive but you could add Anderson darling statistic (see <a href=\"http://en.wikipedia.org/wiki/Anderson%E2%80%93Darling_test\">http://en.wikipedia.org/wiki/Anderson%E2%80%93Darling_test</a>). This made me remind of a paper fromo Jager and Wellner (see <a href=\"http://projecteuclid.org/DPubS?service=UI&amp;version=1.0&amp;verb=Display&amp;handle=euclid.aos/1194461721\">http://projecteuclid.org/DPubS?service=UI&amp;version=1.0&amp;verb=Display&amp;handle=euclid.aos/1194461721</a>) which extands/generalises Anderson darling statistic (and include in particular higher criticism of Tukey). Higher criticism was already shown to be minimax for a wide range of alternatives and the same is done by Jager and Wellner for their extention. I don't think that minimax property has been shown for Kolmogorov test. Anyway, understanding for which type of alternative your test is minimax helps you to know where is its strength, so you should read the paper above.. </p>\n",
      "<p>I can't give you additional reasons to use the Kolmogorov-Smirnov test.  But, I can give you an important reason not to use it.  It does not fit the tail of the distribution well.  In this regard, a superior distribution fitting test is Anderson-Darling.  As a second best, the Chi Square test is pretty good.  Both are deemed much superior to the K-S test in this regard.</p>\n",
      "<p>I think you have to consider the theoretical vs applied advantages of the different notions of distance.  Mathematically natural objects don\u00e2\u0080\u0099t necessarily translate well into application. \nKolmogorov-Smirnov is the most well-known for application, and is entrenched in testing for goodness of fit.  I suppose that one of the reasons for this is that when the underlying distribution $F$ is continuous the distribution of the statistic is independent of $F$.\nAnother is that it can be easily inverted to give confidence bands for the CDF.</p>\n\n<p>But it\u00e2\u0080\u0099s often used in a different way where $F$ is estimated by $\\hat{F}$, and the test statistic takes the form\n$$\\sup_x | F_n(x) - \\hat{F}(x)|.$$\r\nThe interest is in seeing how well $\\hat{F}$ fit the data and acting as if $\\hat{F} = F$, even though the asymptotic theory does not necessarily apply.</p>\n",
      "<p>From the point of view of functional analysis and measure theory the $L^p$ type distances do not define measurable sets on spaces of functions (infinite dimensional spaces loose countable additive in the metric ball coverings). This firmly disqualifies any sort of measurable interpretation of the distances of choices 2 &amp; 3.</p>\n\n<p>Of course Kolomogorov, being much brighter than any of us posting, especially including myself, anticipated this. The clever bit is that while the distance in the KS test is of the $L^0$ variety, the uniform norm itself is not used to define the measurable sets. Rather the sets are part of a stochastic filtration on the differences between the distributions evaluated at the observed values; which is equivalent to the stopping time problem.</p>\n\n<p>In short the uniform norm distance of choice 1 is preferable because the test it implies is equivalent to the stopping time problem, which itself produces computationally tractable probabilities. Where as choices 2 &amp; 3 cannot define measurable subsets of functions.</p>\n"
    ],
    "createdAt": "2025-07-12T08:02:08Z",
    "updatedAt": "2025-07-12T08:02:08Z",
    "similar_questions": [
      222278,
      156633,
      121408,
      133552,
      211709
    ]
  },
  {
    "id": 57627,
    "title": "Hypothesis Testing",
    "body": "<blockquote>\n  <p>A Lab has been asked to evaluate the claim that drinking water in a\n  local restaurant has a lead concentration of 6 parts per billion\n  (ppb). Repeated measurements follow a normal distribution and the\n  population standard deviation is taken to be 0.25 ppb. \u00ce\u00b1 = 0.01. </p>\n  \n  <p>\u00e2\u0080\u00a2 A sample of three measurements is taken and finds:\n  6.79;  6.13;  7.17</p>\n  \n  <ol>\n  <li>Is there evidence to suggest that the lead concentration  is different from 6 ppb?</li>\n  </ol>\n</blockquote>\n\n<p>This is a homework question that I'm really stuck on, so if someone could point me in the right direction that would be great.</p>\n\n<p>I realise that it is a hypothesis testing question and that the null hypothesis is that \u00c2\u00b5 = 6 ppb while the alternative is that \u00c2\u00b5 does not equal 6 ppb. However, I really don't know what to do with the three sample measurements. <strong>Do I get the mean of the three figures or do I use each in a separate hypothesis test?</strong></p>\n",
    "tags": "hypothesis testing,self study",
    "answers": [
      "<p>Here's the Bayesian answer. How would the Bayesian estimate the population mean? Assuming a very noninformative prior (with precision zero, see comments), that's simply the average:\n$$\\mu'=1/n\\sum_{i=1}^n x_i\\approx 6.7$$\nSince you know the population standard deviation, you know the population variance $Var=SD^2$, and you know the precision $\\rho=1/Var=16$. The Bayesian learning rule for the precision is the sum of the precisions of your observations. You have 3, so $\\rho'=3*16=48$, $Var=1/48$, $SD=\\sqrt{1/48}\\approx 0.144$.  So the Bayesian thinks the population mean is normally distributed with mean $\\mu'$ and SD $0.144$.</p>\n\n<p>The question is: Given your estimate from the observations, how likely is it that the test value 6ppb or less can originate from $N(\\mu',\\rho')$? We check a standard normal table. Computing the Z score: $$Z=(6-\\mu')/sd=-0.7/0.144\\approx -4.86.$$\nThe <a href=\"http://en.wikipedia.org/wiki/Standard_normal_table#Cumulative_table\" rel=\"nofollow\">cumulative table</a> actually only goes as far as $Z=3$, at which point $0.9987$ of the values are below $Z$, hence $1-0.9987$ of the values are below $Z=-3$ (by symmetry of the normal) and even fewer below $Z=-4.8$. Thus, you can reject the hypothesis that the estimated population mean is equal to the test value $6$ with your $\\alpha$ level, since $1-0.9987&lt;\\alpha$.</p>\n",
      "<p>From a frequentist perspective you should calculate the standard error of the sample mean (using the known population standard deviation).  You can then work out how many standard errors the sample mean is away from six ppb and look at the corresponding Normal probability density function tails (note the plural).</p>\n\n<p>The assumption is that there's a fixed lead concentration in the water &amp; the measurement error is what's random - distributed normally with mean zero, i.e. there's no systematic error in the measurements.</p>\n"
    ],
    "createdAt": "2025-07-12T08:02:09Z",
    "updatedAt": "2025-07-12T08:02:09Z",
    "similar_questions": [
      185624,
      238627,
      222278,
      156619,
      77851
    ]
  },
  {
    "id": 25316,
    "title": "Relative predictive power of predictors used in time series models like kalman filter",
    "body": "<p>How would we measure the predictive power of predictors in time series models. For e.g. in linear regression we have the magnitude and direction of the regression co-efficients and their p-values.</p>\n\n<p>Is there any measure like that to evaluate the performance of predictors in kalman filter?</p>\n",
    "tags": "r,regression,time series",
    "answers": [
      "<p>The magnitude and direction of the regression coefficients, and their p-values, do not provide any reliable measure of the predictive power of the associated predictors. A reliable measure is to look at how well the models predict with and without each predictor included.</p>\n\n<p>The Kalman filter is not a model, but an algorithm applied to a state space model. So presumably you mean the predictive power of some component of the underlying state space model. Again, the best way is to see how well it predicts with and without that component.</p>\n\n<p>I suggest you use some form of <a href=\"http://robjhyndman.com/researchtips/crossvalidation/\">time series cross-validation</a> when doing the comparison.</p>\n"
    ],
    "createdAt": "2025-07-12T08:02:09Z",
    "updatedAt": "2025-07-12T08:02:09Z",
    "similar_questions": [
      26790,
      211636,
      200822,
      211709,
      77851
    ]
  },
  {
    "id": 94519,
    "title": "How to produce the minimum forecast error using R?",
    "body": "<p>Considering that we want to use optimize() on the interval [0,1] how can I write an R code for finding the value of \u00ce\u00b2 that produces the minimum forecast error without using external packages like <code>forecast</code>?</p>\n\n<p><img src=\"http://i.stack.imgur.com/DE1Hn.png\" alt=\"enter image description here\"></p>\n\n<p><img src=\"http://i.stack.imgur.com/gb7Bh.png\" alt=\"enter image description here\"></p>\n\n<p>For simplicity assume that:\n<img src=\"http://i.stack.imgur.com/B9c2A.png\" alt=\"enter image description here\"></p>\n\n<p>I want to use the following package:</p>\n\n<pre><code>&gt; require(datasets)\n&gt; str(nhtemp)\n Time-Series [1:60] from 1912 to 1971: 49.9 52.3 49.4 51.1 49.4 47.9 49.8 50.9 49.3 51.9 ...\n</code></pre>\n\n<p>in which <code>nhtemp</code> is the <code>Yearly Average Temperatures in New Haven CT</code>.</p>\n",
    "tags": "r,time series,self study",
    "answers": [
      "<p>Two simple ways. Use the <code>forecast</code> package:</p>\n\n<pre><code>library(forecast)\nfit &lt;- ses(nhtemp,initial='simple')\nbeta &lt;- fit$model$par[1]\n</code></pre>\n\n<p>Or use the <code>stats</code> package:</p>\n\n<pre><code>fit &lt;- HoltWinters(nhtemp,gamma=FALSE,beta=FALSE)\nbeta &lt;- fit$alpha\n</code></pre>\n",
      "<p>I hope you attempted to solve the HW problem yourself. See below for the complete code on how to do this using R and Optim function for your reference. I'm also providing you some nice references for you to try solving optimization problems in the context of exponential smoothing that would assist you to learn to solve optimization problems by hand. Based on my personal experience I have to tell that Optimization is a specialized field, you would want to take a course/reading on that before you start applying these techniques to forecasting problems.</p>\n\n<p>There is an excellent article <a href=\"http://www.sciencedirect.com/science/article/pii/S0305048303001130\" rel=\"nofollow\">click here</a> by Rasmussen that appeared in the Omega: The international journal of management science that would provide necessary back ground on how to optimize smoothing parameters as well as initial parameters using a nonlinear optimization tool like excel solver, you could try to replicate this yourself to learn how these parameters are optimized for exponential smoothing models.</p>\n\n<p>If you  are interested in solving the problem that you stated in your post using R, as Rob pointed out you need to write a function to optimize the parameter beta and then using optim function to solve for that parameter.</p>\n\n<p>You can also compare the beta values with a simple exponential smoothing in the forecast package. Both are similar (Beta value: 0.186088).</p>\n\n<pre><code>require(datasets)\nstr(nhtemp)\n\ny &lt;- as.matrix(nhtemp) ## Convert time series data to a matrix format\n\nn = length(y)\n\n## initialize\nyh = rep(NA,60) ## Y hat\nfe = rep(NA,60)\nyh[[1]] = y[1] ## Initialize Yhat[1] with y [1]\n\n## function to optimize\n\nmind &lt;- function (b){ for (i in 2:n)  {\n                        yh[i] &lt;- b*y[i-1]+(1-b)*yh[i-1]\n                      } \n                      fe &lt;- (yh-y)^2\n                      obj &lt;- mean(fe)\n                      return(obj)\n}\n## Optimize to obtain parameter b\n\noptm &lt;- optim(par=c(0.2), fn=mind, gr = NULL, lower = 0, upper = 1,\n              method = c(\"L-BFGS-B\"))\n\n## Print beta value\n\noptm$par\n\n## Check your answer with simple exponential smoothing from \n##forecast package from Rob's code above\n\nlibrary(forecast)\nfit &lt;- ses(nhtemp,initial='simple')\nbeta &lt;- fit$model$par[1]\nbeta\n</code></pre>\n"
    ],
    "createdAt": "2025-07-12T08:02:09Z",
    "updatedAt": "2025-07-12T08:02:09Z",
    "similar_questions": [
      179329,
      55788,
      94572,
      94521,
      156619
    ]
  },
  {
    "id": 109209,
    "title": "Birthday \"Paradox\" -- with a different perspective",
    "body": "<p>Background: Many people are familiar with the so-called Birthday \"Paradox\" that, in a room of 23 people, there is a better than 50/50 chance that two of them will share the same birthday. In its more general form for n people, the probability of no two people sharing the same birthday is p(n) = 365! / (365^n *(365-n)!). Similar calculations are used for understanding hash-space sizes, cryptographic attacks, etc.</p>\n\n<p>Motivation: The reason for asking the following question is actually related to understanding a specific financial market behavior. However a variant on the \"Birthday Paradox\" problem fits exactly as an analogy and is likely to be of wider interest to people with different backgrounds. My question is therefore framed along the lines of the more familiar  \"Birthday Paradox\", as follows.</p>\n\n<p>Situation: There are a total of 60 people in a room. Of these, it turns out that there are 11 (eleven) PAIRS of people who share the same birthday, and one TRIPLE (i.e. group of 3 people) who have the same birthday. The remaining 60 - 2*11 - 3 = 35 people have different birthdays. Assuming a population in which any day is equally likely for a birthday (i.e. ignore Feb 29th &amp; possible seasonal effects) and, given the specified distribution of birthdays, the questioner would like to understand how likely (or unlikely) it is that these 60 people were really chosen at random. This question was originally posed on another site where it was left unanswered, but the questioner was advised to re-state the question in the form that now follows below.</p>\n\n<p>Question: \"If 60 people are chosen at random from a population in which any day is equally likely to be a person's birthday, what is the probability that there are 11 days on which exactly 2 people share a birthday, one day on which exactly 3 of them share a birthday, and no days on which 4 or more people share a birthday?\"</p>\n",
    "tags": "probability,hypothesis testing",
    "answers": [
      "No answers available."
    ],
    "createdAt": "2025-07-12T08:02:09Z",
    "updatedAt": "2025-07-12T08:02:09Z",
    "similar_questions": [
      133493,
      200827,
      94642,
      156703,
      298
    ]
  },
  {
    "id": 156619,
    "title": "Modeling prices with the Hedonic regression",
    "body": "<p>I'm using the concept of Hedonic regression in order to model the prices for real estates. I'm having some trouble with my approach.</p>\n\n<p><strong>What I have and what I do</strong></p>\n\n<ul>\n<li>my data consists out of real estates with following charcteristics: <code>price | livingArea | propertyArea | condoFloorNumber | roomCount | elevator | garage | quiet | etc.</code></li>\n<li>I run a robust regression without intercept <code>lmRob(price ~ . -1)</code></li>\n</ul>\n\n<p><strong>What I want</strong></p>\n\n<ul>\n<li>a model with which I can predict the price of real estates, but which are not in the used data set</li>\n<li>also it would be nice to have some constraints on the coefficients</li>\n</ul>\n\n<p><strong>Problems</strong></p>\n\n<ul>\n<li>very often I get bad values for the coefficients <code>ex: bathroomCount = -80000</code>. it's not possible that with a additive bathroom , the price of the house will sink with <code>80.000\u00e2\u0082\u00ac</code></li>\n<li><p>also I tried to use the function <code>pcls</code> in order to put some constraints on the coefficients, but this method gave very bad results. In the plot <code>Y = price</code> and <code>X = livingArea</code>. as you can see, the regression line isn't correct.\n<img src=\"http://i.stack.imgur.com/7PHp1.png\" alt=\"enter image description here\"></p>\n\n<ul>\n<li>another thought was to transform the regression problem into a maximization or minimization problem, but didn't managed to do it</li>\n<li>also I tried to use different regression methods <code>lm, lmrob, ltsReg, MARS</code>, but they also give me bad coefficients. (sometimes this bad coefficients make a good price estimation)</li>\n<li>I think that the big number of dummy variables damages a little bit the regression</li>\n</ul></li>\n</ul>\n\n<p>Is my approach false?</p>\n\n<p>Does someone have some hints, tricks for me? (<em>I'm not a statistician</em>)</p>\n\n<p><strong>[UPDATE]</strong></p>\n\n<p><img src=\"http://i.stack.imgur.com/a2kLe.png\" alt=\"price ~ livingArea\"></p>\n\n<p>This is how the plotted data looks like. LivingArea is the only non-dummy variable.</p>\n\n<p><strong>[UPDATE 2]</strong></p>\n\n<pre><code>y = bX \n\n     means\n\ny = b_0*X_0 + b_1*X_1 + ... + b_k*X_k\n\n     which is an equation system like this:\n\ny[0] = b_0*X_0[0] + b_1*X_1[0] + ... + b_k*X_k[0]\n.\n.\n.\ny[n] = b_0*X_0[n] + b_1*X_1[n] + ... + b_k*X_k[n]\n</code></pre>\n\n<p>Did I got it right? </p>\n\n<p>If so, isn't possible to add some inequality constraints equation to it. example:</p>\n\n<pre><code>b_0 &gt;= 2000\nb_2 &lt;= b_0/2\n</code></pre>\n\n<p><strong>[UPDATE 3]</strong></p>\n\n<p>I'm running the regression without intercept, because if all the characteristics of a real estate = 0, then of course it'S price = 0. Nobody would pay for an apartment with 0m\u00c2\u00b2.\n<img src=\"http://i.stack.imgur.com/LYPB0.png\" alt=\"enter image description here\">\nbut it seems that the regression line where it was used an intercept (blue) looks far more better than the regression line without intercept (green). I can't understand why it is so. and why doesn't the regression line without intercept start at the point (0,0)?</p>\n",
    "tags": "r,regression",
    "answers": [
      "<p>I think your last remark (\"I think that the big number of dummy variables damages a little bit the regression\") is spot on. The very anormal values you observe for some regession coefficients clearly points to multicollinearity.\nYou might want to try ridge regression or principal components regression.</p>\n",
      "<p>This type of approach clearly can work (and has evidently been used by tax authorities to set property taxes on my house for many years), so there needs to be some investigation of the sources of this difficulty.</p>\n\n<p>Understanding the nature of this data set is very important. If it is to be used for predicting prices of properties not in the data set you must be very certain that it is adequately representative of the population of properties of interest. It's possible there is some peculiarity in the way this particular sample was collected, so that some particular combinations of co-linear factors are leading to things like the negative coefficients for bathroom numbers. Re-evaluate the sample collection and the data coding, an oft-overlooked source of difficulty. Also, for your PCA-based approaches, the signs of coefficients for principal components depend on the directions of the associated eigenvectors, making it all too easy to create errors when you try to go back to the space of the original factors. Check that, too.</p>\n\n<p>You didn't specify the standard errors of your coefficient estimates, so some of your apparently anomalous coefficients might not be significantly different from 0. For example, a -80K coefficient per bathroom with a standard error of +/- 100K would not really be an issue; that probably just means that the high co-linearity makes it difficult to determine a value per bathroom, given its high association with land area, numbers of bedrooms, and so forth. If that's the case you should retain the coefficient when making predictions, as the apparently anomalous coefficient for bathrooms is probably helping to correct for price over-estimates based on some of its co-linear factors alone.</p>\n\n<p>You could try to figure out which combinations of factors are leading to these problems. Although stepwise selection of factors is not wise for building a final model, for troubleshooting you might consider starting with a simple model of price-bathroom relations and adding more factors to see which combinations of factors are leading to your problem.</p>\n\n<p>You also should take advantage of information from structured re-sampling of your data set to evaluate these issues. You don't say whether or how you have approached this crucial aspect of model validation. If you have, then cross-validation or bootstrap resampling may have already provided insights into the sources of your difficulty. If you haven't, consult <a href=\"http://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf\" rel=\"nofollow\">An Introduction to Statistical Learning</a> or similar references to see how to proceed. </p>\n"
    ],
    "createdAt": "2025-07-12T08:02:09Z",
    "updatedAt": "2025-07-12T08:02:09Z",
    "similar_questions": [
      185583,
      133552,
      211709,
      94519,
      94521
    ]
  },
  {
    "id": 145657,
    "title": "Determining whether to use one line or two in linear regression",
    "body": "<p>Basically I'm attempting to recreate the results of an example from class in R. What I'm trying to do is decide whether it's best to use a single regression line for an entire data set or two lines based on a categorical variable. The teacher indicates there are three steps to this:</p>\n\n<ol>\n<li>Determine if two different lines are required</li>\n<li>If yes, determine if they differ in slope</li>\n<li>If yes, determine if they differ in intercept</li>\n</ol>\n\n<p>Here is my data:</p>\n\n<pre><code>&gt; example\n   Predictor Response Group\n1         21       11     A\n2         24       21     A\n3         26       23     A\n4         29       29     A\n5         35       34     A\n6         45       51     A\n7         51       59     A\n8         68       73     A\n9         72       83     A\n10        76       95     A\n11        17       11     B\n12        21       55     B\n13        26       34     B\n14        28       44     B\n15        32       26     B\n16        36       34     B\n17        40       15     B\n18        45       21     B\n19        51       16     B\n20        68       21     B\n</code></pre>\n\n<p>I've realized that if I add the interaction and group terms to the model:</p>\n\n<pre><code>ex_mod &lt;- lm(Response ~ Predictor,data = example)\nex_mod2 &lt;- lm(Response ~ Predictor + Group + Predictor:Group,data = example)\n</code></pre>\n\n<p>And then perform ANOVA on this. I get the right answer for step 1:</p>\n\n<pre><code>&gt; anova(ex_mod,ex_mod2)\nAnalysis of Variance Table\n\nModel 1: Response ~ Predictor\nModel 2: Response ~ Predictor + Group + Predictor:Group\n  Res.Df    RSS Df Sum of Sq     F    Pr(&gt;F)    \n1     18 6616.4                                 \n2     16 1583.8  2    5032.6 25.42 1.078e-05 ***   \n</code></pre>\n\n<p>Which means I need different lines, but now I need to know if they differ in slope or y-intercept or both. And here is where I'm stuck. I cant seem to get the right answer (F = 293.17 for slope, and F = 170.77 for intercept). </p>\n\n<p>The teacher indicates that the next steps are: 1) to generate RSS in which the slope is fixed,but the y-intercepts are allowed to vary; and 2) generate RSS in which the y-intercept is fixed, but the slopes are allowed to vary.</p>\n\n<p>I apologize if the question is confusing or simplistic, but I dont know how to proceed from here.</p>\n\n<p>Thanks</p>\n",
    "tags": "r,regression",
    "answers": [
      "No answers available."
    ],
    "createdAt": "2025-07-12T08:02:09Z",
    "updatedAt": "2025-07-12T08:02:09Z",
    "similar_questions": [
      94638,
      156619,
      185583,
      133552,
      222278
    ]
  },
  {
    "id": 94521,
    "title": "How to interpret results if a reference category of a categorical variable in multivariable logistic regression is not significant?",
    "body": "<p>I am trying to do a multivariable logistic regression and using a normal binomial logistic regression, using binomial variable <code>X</code> (coded <code>0</code>/<code>1</code>) and <code>Y</code> (4 sub-categories). Unfortunately, the reference category was shown to be not significant. Should I then reject the null hypothesis for this variable?</p>\n\n<pre><code>Cat.,   Sig.,   Exp(B)\nY,         1,   .161\nY(1),   .032,   .226\nY(2),   .106,   .370\nY(3),   .061,   .309\n</code></pre>\n",
    "tags": "regression,logistic",
    "answers": [
      "<p>As an aside, people usually use Y for the dependent variable and X for the independent variable. It's not necessary to do so, but switching them will confuse people.</p>\n\n<p>What are \"subcategories\" of Y? For this to be correct, they should be levels of a variable. </p>\n\n<p>What parameterization did you use? (Dummy coding? Effect coding? Something else?)</p>\n\n<p>Next, the reference category is never significant. If you are using dummy coding it is what you are comparing the other levels to. Thus, your results show that Y(1) is significantly different from Y(0), Y(2) is not, and Y(3) is borderline.</p>\n"
    ],
    "createdAt": "2025-07-12T08:02:09Z",
    "updatedAt": "2025-07-12T08:02:09Z",
    "similar_questions": [
      70699,
      185583,
      94619,
      104978,
      57807
    ]
  },
  {
    "id": 121408,
    "title": "Regression model for predicting life expectancy",
    "body": "<p>I have average life expectancy at birth data for an 8 year period and I would like to use that 8 year period to predict the trend for average life expectancy for the next 5 years. I would then like to ask whether this deviates significantly from the actual average life expectancy over the next 5 years.</p>\n\n<ol>\n<li>What's the best regression model to fit to the observation base data in order to get predictions for next 5 years?</li>\n<li>How can I assess whether the difference between the predicted and observed trend is significant?</li>\n<li>How can I implement #1 and #2 in R?</li>\n</ol>\n",
    "tags": "r,regression,time series",
    "answers": [
      "No answers available."
    ],
    "createdAt": "2025-07-12T08:02:09Z",
    "updatedAt": "2025-07-12T08:02:09Z",
    "similar_questions": [
      222278,
      133552,
      156619,
      211709,
      156633
    ]
  },
  {
    "id": 81840,
    "title": "How Many Samples Needed for Classification",
    "body": "<p>I am currently trying to perform classification on a set of data.</p>\n\n<p>I have 19 category_1 observations and 15 category_2 observations. What is the best way to sample from this dataset to train and test my classifier, or are there simply not enough observations?</p>\n",
    "tags": "machine learning,classification",
    "answers": [
      "<p>The answer to this question depends on whether your categories overlap one another and how much. </p>\n\n<p>If your categories are completely separated, then I'd say you have enough data. Just use 70% for training and 30% for testing - perhaps in a bootstrap. </p>\n\n<p>However, in most cases that I've seen in life sciences the categories usually overlap considerably, and 19+15 observations is just not enough to train a good classifier.</p>\n",
      "<p>This definitely depends on too many factors to give a good answer. The data's specificities and the classifying power of the feature can make a lot of difference. Usually such a low number of samples (34) would be too low for any classifying method, which will likely overfit however hard you try to use any variance reduction techniques (or just have terrible predictive performance). </p>\n\n<p>But if your features classify your data exceptionally well, you might train a decent classifier notwithsanding.</p>\n\n<p>Imagine the following situation : you are trying to classify whether a person is male or female. One of your features is \"has male sexual organs\". Then it really won't matter how many samples you have, your classifier will always be correct (this is somewhat equivalent to Michal's explanation, as in this case the classes do not overlap at all in the space of features, and there is no variance within a class). Of course this is a gross exaggeration but you get the idea.</p>\n\n<p>Bottom line - \n1) you can't tell if a classifier will work without knowing the data's specificities, and 2) you won't know until you try.</p>\n",
      "<p>The sample size of your test data can be estimated using probability inequalities. You can calculate the required sample number to get a desired accuracy with high probability. </p>\n\n<p>But it is not easy to estimate the training size since it depends on both your model and target complexity. You can try leave-one-out cross validation strategy to get a learning curve in order to choose such sample number that best balance the bias and variance issue. </p>\n\n<p>If your feature is high dimensional, this <a href=\"http://louisville.edu/sphis/bb/srcos-2013/AbstractAddoSandra.pdf\" rel=\"nofollow\">paper</a> may provide an estimation of how large a sample size is needed with low sample size.</p>\n"
    ],
    "createdAt": "2025-07-12T08:02:10Z",
    "updatedAt": "2025-07-12T08:02:10Z",
    "similar_questions": [
      55718,
      222241,
      133552,
      166892,
      211677
    ]
  },
  {
    "id": 94560,
    "title": "test for variance by using chi square test in R",
    "body": "<p>how can i know if these data -> \n93, 91, 93, 150, 80, 104, 128, 83, 88, 95, 94, 97 58, 139, 91 provide evidence that the pop variance is greater than 0.05 (a=0.05) by chi square test in R?\n please help!</p>\n",
    "tags": "r,hypothesis testing",
    "answers": [
      "No answers available."
    ],
    "createdAt": "2025-07-12T08:02:10Z",
    "updatedAt": "2025-07-12T08:02:10Z",
    "similar_questions": [
      135565,
      133441,
      133493,
      145657,
      233264
    ]
  },
  {
    "id": 25381,
    "title": "Relationship between $R^2$ and MAE in forecasting",
    "body": "<p>I have the following linear model based on multivariate timeseries:</p>\n\n<pre><code>lm(formula = Y ~ X1 + X2 + X3 + X4 + X5 + X6 + X7, data = model.data, na.action = na.omit)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0030132 -0.0002101  0.0000004  0.0002101  0.0035819 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2.355e-06  3.813e-06  -0.618  0.53683    \nX1           4.322e-02  6.964e-03   6.206 5.49e-10 ***\nX2          -5.200e-02  8.202e-03  -6.340 2.32e-10 ***\nX3          -3.222e-02  8.182e-03  -3.939 8.21e-05 ***\nX4           9.367e-07  2.016e-07   4.647 3.37e-06 ***\nX5           5.131e-02  9.980e-03   5.141 2.74e-07 ***\nX6           2.911e-02  1.005e-02   2.897  0.00377 ** \nX7          -1.677e-07  2.608e-08  -6.430 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nResidual standard error: 0.0003898 on 35773 degrees of freedom\n  (3750 observations deleted due to missingness)\nMultiple R-squared: 0.006066,   Adjusted R-squared: 0.005872 \nF-statistic: 31.19 on 7 and 35773 DF,  p-value: &lt; 2.2e-16\n</code></pre>\n\n<p>All the IVs have a low enough p-value to be considered significant. $R^2$ however is less than 0.01.</p>\n\n<p>Now let's take a look at the mean absolute error of this model. I report a \"Naive Model MAE\" calculated as the difference between the last observed $Y_t$ and the next observed value $Y_{t+1}$ and a \"Model MAE\" as the mean of the abs of the residuals.</p>\n\n<p>The same Naive MAE formula is used for the out-of-sample data set. The Fcast MAE is calculated as the difference between the predicted values based on the model fit and the actual observed values in the out of sample data. Here the MAE values are multiplied by 10000 for readability.</p>\n\n<pre><code>AIC -460198.9 \nNaive Model MAE 4.005496 (0.0004005496)\nModel MAE 2.812995 (0.0002812995)\n\nNaive Fcast MAE 2.436187 (0.0002436187)\nFcast MAE 1.710664  (0.0001710664)\nFcast Yt+1 Impr % 29.78\n</code></pre>\n\n<p>So although $R^2$ is very low, the forecast MAE relative to a naive benchmark is quite \"interesting\". Intuitively I am struggling to capture this relationship between $R^2$ and MAE.</p>\n\n<p>On mpkitas suggestion I also ran a regression based on the naive $Y_t$ forecast to compare $R^2$, in addition to MAE:</p>\n\n<pre><code> summary(lm(data$NextY ~ data$Y, na.action = na.omit))\n\nCall:\nlm(formula = data$NextY ~ data$Y, na.action = na.omit)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0029678 -0.0002104  0.0000016  0.0002113  0.0035384 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)            -1.590e-06  2.067e-06  -0.770    0.442\nY                       3.447e-03  5.231e-03   0.659    0.510\n\nResidual standard error: 0.000391 on 35781 degrees of freedom\n  (3748 observations deleted due to missingness)\nMultiple R-squared: 1.213e-05,  Adjusted R-squared: -1.581e-05 \nF-statistic: 0.4341 on 1 and 35781 DF,  p-value: 0.51 \n</code></pre>\n\n<p>How can $R^2$ be so low if the forecast is far to be random?</p>\n",
    "tags": "regression,time series",
    "answers": [
      "No answers available."
    ],
    "createdAt": "2025-07-12T08:02:10Z",
    "updatedAt": "2025-07-12T08:02:10Z",
    "similar_questions": [
      133488,
      135565,
      94638,
      94581,
      211677
    ]
  },
  {
    "id": 57703,
    "title": "Proper method for time-staggered/repeated measures data?",
    "body": "<p>I have a data set with 300 workers/subjects. There is one dependent variable: a worker's performance, by some metric, during the current week. There are three independent variables (let's call them IV1, IV2, IV3), which are all descriptors of something from the previous week pertaining to that worker. The data is over the course of 52 weeks. </p>\n\n<p>What method would you use to determine if any of IV1, IV2, and IV3 influence the DV over the course of the entire time span? Just a standard multiple regression?</p>\n",
    "tags": "time series,hypothesis testing",
    "answers": [
      "No answers available."
    ],
    "createdAt": "2025-07-12T08:02:10Z",
    "updatedAt": "2025-07-12T08:02:10Z",
    "similar_questions": [
      200822,
      211696,
      156680,
      104928,
      70699
    ]
  },
  {
    "id": 156633,
    "title": "Calculating Mutual Information for feature selection",
    "body": "<p>In order to determine the importance of some individual features coming from labelled time series, I am trying to calculate the Mutual Information (as showed in \"<a href=\"http://doi.acm.org/10.1145/2462096.2462099\" rel=\"nofollow\">Who do you sync you are?: smartphone fingerprinting via application behaviour</a>\"). Selected features will be used to train a kNN classifier.</p>\n\n<p>MI requests distributions as input but I do not understand which are dists in my context. I have:</p>\n\n<ol>\n<li><p>a list of features (e.g., mean, std, 90-quantile, etc.)</p></li>\n<li><p>a set of users (i.e., user1, user2, user3)</p></li>\n</ol>\n\n<p>I can obviously calculate single feature's values distribution but I do not understand how users can be a distribution? Aren't they just label? </p>\n\n<p>ps. Is there any Python implementation out there? </p>\n",
    "tags": "machine learning,classification",
    "answers": [
      "No answers available."
    ],
    "createdAt": "2025-07-12T08:02:10Z",
    "updatedAt": "2025-07-12T08:02:10Z",
    "similar_questions": [
      222278,
      121408,
      133552,
      211709,
      156619
    ]
  },
  {
    "id": 94570,
    "title": "Is segmented (piecewise) regression possible with a dichotomous dependent variable?",
    "body": "<p>I have created a model of American cities with a dichotomous Y variable using logistic regression. I have theoretical reasons to believe that the model will differ significantly between larger and smaller cities. I have shown this using arbitrary breakpoints, but wanted to know if a segmented (piecewise) regression strategy can identify a breakpoint for me in a model with a binary dependent variable. (I have used the \"nl\" command successfully in Stata for a different project when the Y was continuous.) </p>\n\n<p>Can I use this strategy to statistically identify a breakpoint? Or should I build the case for a number I pick otherwise? </p>\n\n<p>Many thanks.</p>\n",
    "tags": "regression,logistic",
    "answers": [
      "No answers available."
    ],
    "createdAt": "2025-07-12T08:02:10Z",
    "updatedAt": "2025-07-12T08:02:10Z",
    "similar_questions": [
      200822,
      179336,
      211696,
      57790,
      104928
    ]
  },
  {
    "id": 135442,
    "title": "Is there any neural network whose output can be probabilistic, just like multi-class logistic regression?",
    "body": "<p>I want to add nonlinear character into multi-class logistic regression. I know kernel logistic regression can do it. Is there any kind of neural network which has similar characteristic?</p>\n",
    "tags": "machine learning,logistic",
    "answers": [
      "<p>Yes, most good implementations of <a href=\"http://en.wikipedia.org/wiki/Multilayer_perceptron\" rel=\"nofollow\">multi-layer perceptron</a> (e.g. <a href=\"http://www.aston.ac.uk/eas/research/groups/ncrg/resources/netlab/\" rel=\"nofollow\">netlab</a>) and <a href=\"http://en.wikipedia.org/wiki/Radial_basis_function_network\" rel=\"nofollow\">Radial Basis Function neural networks</a> ought to support this, using a <a href=\"http://en.wikipedia.org/wiki/Softmax_function\" rel=\"nofollow\">softmax</a> output function and cross-entropy loss function.  See section 6.9 of Chris Bishop's excellent book \"<a href=\"http://ukcatalogue.oup.com/product/9780198538646.do\" rel=\"nofollow\">Neural Networks for Pattern Recognition</a>\"</p>\n"
    ],
    "createdAt": "2025-07-12T08:02:11Z",
    "updatedAt": "2025-07-12T08:02:11Z",
    "similar_questions": [
      26728,
      25513,
      200822,
      57790,
      94619
    ]
  },
  {
    "id": 25385,
    "title": "Predict future student outcomes (binary and continuous) with historic cross-sectional data?",
    "body": "<p>Using Stata 11.2, I would like to develop 2 analytic models that could be implemented by school administrators to flag students for intervention.  I'm wondering if it would be possible to develop these models based on historical cross-sectional data consisting of 650,000 unique observations (11th grade students); it was collected from 2009 through 2011.  </p>\n\n<p>First, I want to predict risk of dropping out of school (with the goal of intervening in 5% of students with the highest risk of dropout).  Second, I want to predict absence (with the goal of intervening among students in the top 5% of predicted absentee time).</p>\n\n<p>Outcome variables:  dropout (binary; 0=no, 1=yes) and absence (continuous; cumulative hours absent from school)</p>\n\n<p>Predictor variables:  sex (binary), track (3-categories), GPA (continuous)</p>\n\n<p>So far, I've just worked on predicting dropout, but am not sure if what I'm doing is theoretically correct or statistically sound.  (It's been a long time since I took stats!)  Here what I've done, using the Stata command: </p>\n\n<pre><code>logistic dropout i.sex i.track gpa\n</code></pre>\n\n<p>The output shows all independent variables are significantly associated with dropout, based on p&lt;0.05 and 95% CIs for the ORs that do not contain 0.  Sex (OR=0.95), Track (OR=0.76), GPA (OR= 1.52).</p>\n\n<p>Now I'm not sure about how to proceed in calculating the predicted probabilities of dropout, and then figuring out which students are at greatest risk of dropout. Should the command be <code>predict phat</code> ?  </p>\n\n<p>I think this gives the predicted prob of dropout for each level of each variable, holding all other variables at their mean.  Then I would just categorize the predicted values into 20 categories, and the students in the top category would be those who should be targeted for intervention? </p>\n\n<p>I would greatly appreciate recommendations on how to conduct both analyses (binary and continuous outcomes).  </p>\n",
    "tags": "regression,probability,logistic",
    "answers": [
      "<p>You have to combine, or rather to stack, the historic data and the actual data. In Stata, this can be done via the <code>append</code> command.</p>\n\n<p>Then you have to use the <code>logit</code> and <code>regress</code> commands to estimate the models.</p>\n\n<p>In a last step, you have use <code>predict</code> to obtain the out-of-sample predictions for the outcome variable. In the case of a logit model, <code>predict</code> gives you the predicted probability of observing an outcome, given a set of values for the covariates. </p>\n\n<p>Note that your understanding of the <code>predict</code> command is not quite correct. What you describe rather corresponds to the <code>margins</code> command.  </p>\n\n<p><strong>Example</strong></p>\n\n<p>I use a dataset from <a href=\"http://people.stern.nyu.edu/wgreene/Text/econometricanalysis.htm\" rel=\"nofollow\">William Greene's Econometric Analysis textbook</a>, availabe on the web:</p>\n\n<pre><code>clear\ninfile obs gpa tuce psi grade ///\n    using \"http://people.stern.nyu.edu/wgreene/Text/Edition7/TableF14-1.txt\" \n</code></pre>\n\n<p>I want to predict the probability of a grade increase. For the sake of illusration I assume that the first 20 observations of the dataset are the historical data, and that the remaining 12 observations are the ones for which you want to have a prediction. Hence, I set observations 21 to 32 missing.</p>\n\n<pre><code>replace grade = . if _n &gt; 20\n</code></pre>\n\n<p>I estimate a logistic regression model that predicts the probability of a grade increase (grade = 1) as a function of the gpa and tuce scores.</p>\n\n<pre><code>logit grade gpa tuce\n</code></pre>\n\n<p>Note that I have used the <code>logit</code> rather than the <code>logistic</code> command, because I want to resue the coefficient in a moment. Now I can compute the predicted probabilities, for <em>all</em> the observations. </p>\n\n<pre><code>predict phat1, p\n</code></pre>\n\n<p>To understand, or to be sure of what <code>predict</code> does, I compute the following.</p>\n\n<pre><code>local X \"_b[_cons] + _b[gpa]*gpa + _b[tuce]*tuce\"\ngenerate phat2 = exp(`X') /(1 + exp(`X'))\n</code></pre>\n\n<p>I could have direclty used Stata's <code>invlogit</code> function:</p>\n\n<pre><code>generate phat3 = invlogit(`X')\n</code></pre>\n\n<p>All the three methods yield the same results.</p>\n\n<pre><code>summarize phat1 phat2 phat3\n</code></pre>\n\n<p>With a continuous outcome and the <code>regress</code> command, the principle is the same. Here is an illustration. </p>\n\n<pre><code>replace gpa = . if _n &gt; 20\nregress gpa tuce psi\npredict xbhat1, xb\ngen xbhat2 = _b[_cons] + _b[tuce] * tuce + _b[psi] * psi\nsummarize xbhat1 xbhat2\n</code></pre>\n"
    ],
    "createdAt": "2025-07-12T08:02:11Z",
    "updatedAt": "2025-07-12T08:02:11Z",
    "similar_questions": [
      104978,
      57790,
      200822,
      94521,
      94581
    ]
  },
  {
    "id": 94572,
    "title": "Calculation of standard variance $s^2$",
    "body": "<p>First picture is the question and its answer key: </p>\n\n<p><img src=\"http://i.stack.imgur.com/3qWCe.jpg\" alt=\"enter image description here\"></p>\n\n<p>The second picture is my solution: </p>\n\n<p><img src=\"http://i.stack.imgur.com/cIlt5.jpg\" alt=\"enter image description here\"></p>\n\n<p>I know that $S^2=\\frac{\\sum(x_i-\\bar x)^2}{n-1}$</p>\n\n<p>But in the answer key, $s^2$ cannot be found in that way. I guess, there is a mistake here. Or, \u00c4\u00b1s there a rule that I dont know? </p>\n\n<p>Please clarify this. Thank you. </p>\n",
    "tags": "hypothesis testing,self study",
    "answers": [
      "No answers available."
    ],
    "createdAt": "2025-07-12T08:02:11Z",
    "updatedAt": "2025-07-12T08:02:11Z",
    "similar_questions": [
      55788,
      94519,
      179329,
      133493,
      185425
    ]
  },
  {
    "id": 109222,
    "title": "Conditional logistic regression model does not converge but logistic regression model does",
    "body": "<p>I am running an analysis where I have 2500 cases and 2500 controls. The cases have disease A, and the controls do not. I am trying to see if having disease A increases the odds of various diseases. For the sake of simplicity, we can focus on one disease, call it disease B.</p>\n\n<p>D = 1 if disease B present, 0 otherwise</p>\n\n<p>E = 1 if disease A present, 0 otherwise</p>\n\n<p>I am also including in the model a measure of healthcare utilization. </p>\n\n<p>F is a positive integer proportional to an individual's utilization of healthcare.</p>\n\n<p>I am running the logistic regression model as such in R:</p>\n\n<pre><code>glm(D ~ E + F, family = \"binomial\") \n</code></pre>\n\n<p>Now, this works fine. </p>\n\n<p>However, when I try to run conditional logistic regression, it gives me an error:</p>\n\n<pre><code>library(survival)\nclogit(D ~ E + F, strata(matched.pairs))\nError in fitter(X, Y, strats, offset, init, control, weights = weights,  :\n  NA/NaN/Inf in foreign function call (arg 5)\nIn addition: Warning message:\nIn fitter(X, Y, strats, offset, init, control, weights = weights,  :\n  Ran out of iterations and did not converge\n</code></pre>\n\n<p>I have tried different strata, including dividing the individuals into quantile bins based on F. It does not seem to change anything. (note: pairs are matched on age, gender, race, and F)</p>\n\n<p>This occurs only when I run it on a larger sample size. I ran this same analysis on a sample size of 200 (100 cases and 100 controls) and it worked fine. When I use a sample size of 5000, I get the above error. </p>\n\n<p>I also made sure that at least 10 cases and 10 controls had the disease in question (disease B, for this example). </p>\n\n<p>I am not sure why logistic regression runs fine when conditional logistic regression does not. Can anyone offer me any advice?</p>\n\n<p>Thank you all in advance.</p>\n",
    "tags": "r,regression,logistic",
    "answers": [
      "No answers available."
    ],
    "createdAt": "2025-07-12T08:02:11Z",
    "updatedAt": "2025-07-12T08:02:11Z",
    "similar_questions": [
      94521,
      173,
      94619,
      70699,
      94581
    ]
  },
  {
    "id": 109234,
    "title": "Durbin Watson test statistic",
    "body": "<p>I applied the DW test to my regression model in R and I got a DW test statistic of 1.78 and a p-value of 2.2e-16 = 0.  </p>\n\n<p>Does this mean there is no autocorrelation between the residuals because the stat is close to 2 with a small p-value or does it mean although the stat is close to 2 the p-value is small and thus we reject the null hypothesis of there existing no autocorrelation?</p>\n",
    "tags": "r,regression,hypothesis testing",
    "answers": [
      "<p>In R, the function <code>durbinWatsonTest()</code> from <code>car</code> package verifies if the residuals from a linear model are correlated or not:</p>\n\n<ul>\n<li>The null hypothesis ($\\text{H}_0$) is that there is no correlation among residuals, i.e., they are independent.</li>\n<li>The alternative hypothesis ($\\text{H}_a$) is that residuals are autocorrelated.</li>\n</ul>\n\n<p>As the p value was near from zero it means one can reject the null. </p>\n",
      "<p>dwtest tests against the alternative hypothesis instead to the null hypothesis. So if the p-value is bellow the level you say, then it means it accepts the alternative hypothesis and rejects the null hypothesis.</p>\n",
      "<p>If you believe the DW test, then yes, it indicates that you have serial correlation. However, remember that language of hypothesis testing you can never accept anything, you can only fail to reject it. </p>\n\n<p>Further the DW test requires the full set of classical linear model assumptions, including normality and unbiasedness in order to have any power. Almost no real life application can reasonable assume this, and therefore you will hard a time convincing others about its validity. There are many much simpler (and more robust) tests to use instead of the DW, you should use these!</p>\n\n<p>Of course the easy solution is to just to compute robust standard errors, for instance newey-west (which is easy to do in R), then you can simply ignore the problem</p>\n"
    ],
    "createdAt": "2025-07-12T08:02:11Z",
    "updatedAt": "2025-07-12T08:02:11Z",
    "similar_questions": [
      133488,
      57807,
      133441,
      222278,
      238627
    ]
  },
  {
    "id": 25387,
    "title": "Problem with e1071 libsvm?",
    "body": "<p>I have a dataset with two overlapping classes, seven points in each class, points are in two-dimensional space. In R, and I'm running <code>svm</code> from the <code>e1071</code> package to build a separating hyperplane for these classes. I'm using the following command:</p>\n\n<pre><code>svm(x, y, scale = FALSE, type = 'C-classification', kernel = 'linear', cost = 50000)\n</code></pre>\n\n<p>where <code>x</code> contains my data points and <code>y</code> contains their labels. The command returns an svm-object, which I use to calculate parameters $w$ (normal vector) and $b$ (intercept) of the separating hyperplane.</p>\n\n<p>Figure (a) below shows my points and the hyperplane returned by the <code>svm</code> command (let's call this hyperplane the optimal one). The blue point with symbol O shows the space origin, dotted lines show the margin, circled are points which have non-zero $\\xi$ (slack variables).</p>\n\n<p>Figure (b) shows another hyperplane, which is a parallel translation of the optimal one by 5 (b_new = b_optimal - 5). It is not difficult to see that for this hyperplane the objective function\n$$ 0.5||w||^2 + cost \\sum \\xi_i $$\n(which is minimized by C-classification svm) will have lower value than for the optimal hyperplane shown in figure (a). So does it look like there is a problem with this <code>svm</code> function? Or did I make a mistake somewhere?</p>\n\n<p><img src=\"http://i.stack.imgur.com/H7QB6.png\" alt=\"enter image description here\"></p>\n\n<p>Below is the R code I used in this experiment.</p>\n\n<pre><code>library(e1071)\n\nget_obj_func_info &lt;- function(w, b, c_par, x, y) {\n    xi &lt;- rep(0, nrow(x))\n\n    for (i in 1:nrow(x)) {\n        xi[i] &lt;- 1 - as.numeric(as.character(y[i]))*(sum(w*x[i,]) + b)\n        if (xi[i] &lt; 0) xi[i] &lt;- 0\n    }\n\n    return(list(obj_func_value = 0.5*sqrt(sum(w * w)) + c_par*sum(xi), \n                    sum_xi = sum(xi), xi = xi))\n}\n\nx &lt;- structure(c(41.8226593092589, 56.1773406907411, 63.3546813814822, \n66.4912298720281, 72.1002963174962, 77.649309469458, 29.0963054665561, \n38.6260575252066, 44.2351239706747, 53.7648760293253, 31.5087701279719, \n24.3314294372308, 21.9189647758150, 68.9036945334439, 26.2543850639859, \n43.7456149360141, 52.4912298720281, 20.6453186185178, 45.313889181287, \n29.7830021158501, 33.0396571934088, 17.9008386892901, 42.5694092520593, \n27.4305907479407, 49.3546813814822, 40.6090664454681, 24.2940422573947, \n36.9603428065912), .Dim = c(14L, 2L))\n\ny &lt;- structure(c(2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, \n1L), .Label = c(\"-1\", \"1\"), class = \"factor\")\n\na &lt;- svm(x, y, scale = FALSE, type = 'C-classification', kernel = 'linear', cost = 50000)\n\nw &lt;- t(a$coefs) %*% a$SV;\nb &lt;- -a$rho;\n\nobj_func_str1 &lt;- get_obj_func_info(w, b, 50000, x, y)\nobj_func_str2 &lt;- get_obj_func_info(w, b - 5, 50000, x, y)\n</code></pre>\n",
    "tags": "r,machine learning",
    "answers": [
      "<p>In the libsvm FAQ is mentioned that the labels used \"inside\" the algorithm can be different from yours. This will sometimes reverse the sign of the \"coefs\" of the model.</p>\n\n<p>For instance, if you had labels $y=[-1,+1,+1,-1,...]$, then the first label in $y$, which is \"-1\", will be classified as $+1$ for running libsvm and, obviously, your \"+1\" will be classified as $-1$ inside the algorithm.</p>\n\n<p>And recall that the coefs in the returned svm model are indeed $\\alpha_n\\,y_n$ and so your calculated $w$ vector will be affected due to reversion of the sign of the $y$'s. </p>\n\n<p>See the question \"Why the sign of predicted labels and decision values are sometimes reversed?\" <a href=\"http://www.csie.ntu.edu.tw/~cjlin/libsvm/faq.html\" rel=\"nofollow\">here</a>.</p>\n",
      "<p>I've run into the same problem using LIBSVM in MATLAB. To test it, I created a very simple, 2D linearly separable data set that happened to be translated along one axis to out around -100. Training a linear svm using LIBSVM produced a hyperplane whose intercept was still right around zero (and so the error rate was 50%, naturally). Standardizing the data (subtracting the mean) helped, though the resulting svm still did not perform perfectly...perplexing. It seems as though LIBSVM only rotates the hyperplane about the axis without translating it. Perhaps you should try subtracting the mean from your data, but it seems odd that LIBSVM would behave this way. Perhaps we are missing something.</p>\n\n<p>For what it's worth, the built-in MATLAB function <code>svmtrain</code> produced a classifier with 100% accuracy, without standardization.</p>\n"
    ],
    "createdAt": "2025-07-12T08:02:11Z",
    "updatedAt": "2025-07-12T08:02:11Z",
    "similar_questions": [
      94521,
      94519,
      185583,
      70699,
      156619
    ]
  },
  {
    "id": 166892,
    "title": "Is multiple stage binary classification a good idea if you have very few positives?",
    "body": "<p>The problem is the following: We have a set of, say 5000 documents, with a single binary label. Say that 4900 documents are negative and only 100 are positive.</p>\n\n<p>I built a binary classifier while looking out that I use evaluation metrics and parameters suited for very uneven label distributions. Say that the results are not bad, but I want more.</p>\n\n<p>So my idea is the following. What if you would decompose the classification problem into two stages. The first stage would filter out some documents that we are very sure have nothing to do with what we want to find, and in the second stage, you would be searching for what you really want.</p>\n\n<p>To propose an example: say you have a large set of animals of which only a small portion are lions.</p>\n\n<p>the original idea: construct a binary classifier to find the lions in the whole set.</p>\n\n<p>the new idea: construct a binary classifier to find mammals in the whole set. Then construct a binary classifier to find lions in the set of mammals.</p>\n\n<p>Do I have reason to believe that this would be a good idea. I mean is it at least worth trying out? Why? Why not?</p>\n\n<p>For my specific case: I have a set of documents which at least have something to do with 'Marriage' or 'Divorce'. I want to find the documents in this set which describe exactly an event where two persons get married or divorced. My idea was to first find the events that at least indicate that a pair is near marriage or near divorce, and then search for documents that describe the exact action of marriage or divorce.</p>\n",
    "tags": "machine learning,classification",
    "answers": [
      "<p>If you can assign to a subsample of your documents a score (a number from 0 to 3, for example) that represents how <em>near</em> the couple is from marriage or divorce (where 3 is the actual divorce or marriage action), then you can use regression to try to predict this value on the whole set of 5000, and then filter out documents with low values to make your label distribution less skewed. Note, however, that keeping at least some documents with low score values may be important to make sure your classifier can generalize well.</p>\n"
    ],
    "createdAt": "2025-07-12T08:02:11Z",
    "updatedAt": "2025-07-12T08:02:11Z",
    "similar_questions": [
      57790,
      55718,
      25513,
      81840,
      104978
    ]
  },
  {
    "id": 70668,
    "title": "What is the probability of winning this Jackpot Lottery?",
    "body": "<p>I am working on some more exam practice questions, and just want to see if I am approaching this question correctly?</p>\n\n<p>Here is the complete question:</p>\n\n<p>2,250 numbers are drawn at random without replacement from a barrel containing\n150,000 ticket numbers. </p>\n\n<p>After all the 2250 numbers are drawn, they are returned to the barrel and a\nsecond number (a jackpot number) is drawn at random. </p>\n\n<p>If this jackpot number is the same as one of the 2,250 previously drawn numbers, a jackpot prize is awarded. If the jackpot prize is NOT won (that is, the prize winning number is di\u000bfferent from one of the 2,250 jackpot numbers), then the number is returned to the barrel and another number is drawn at random. </p>\n\n<p>The selection process is repeated until the jackpot prize is won.</p>\n\n<p>(1) Find the probability that the jackpot prize is won at any draw.</p>\n\n<p>(2) The jackpot prize is initially 10,000, and it increases by 10,000 each time the jackpot\nprize is NOT won. Find is the probability that the jackpot prize will exceed 250,000 when the jackpot is finally won.</p>\n\n<p>(3) Let X be a random variable representing the number of draws until the jackpot prize is\nfi\fnally won. What is P(X = k)? What is the corresponding total prize money won?</p>\n\n<p>(4) What is the expected winning of this Jackpot Lottery?</p>\n\n<p>Here is my train of thought:</p>\n\n<p>(1) This one seemed like a bit of a trick question - since it explicitly states that the selection process is repeated until the jackpot is won - so the probability should be 1.</p>\n\n<p>Otherwise, I believe that this can be modeled by a geometric discrete distribution? So the probability of the jackpot prize being won at $x$, should be:</p>\n\n<p>$P(X=x)=(1- \\frac{2,250}{150,000})^{x-1}(\\frac{2,250}{150,000})$</p>\n\n<p>(2) The first \"success\" in which the prize money exceeds 250,000 would be on the 26th try and thereafter the prize money exceeds this amount, so the probability of the jackpot prize exceeding 250K is:</p>\n\n<p>$1-P(X \\le 25)$, which works out to be $0.0298$</p>\n\n<p>(3) I have this answer as essentially the same as (1) (Actually what is the difference?)</p>\n\n<p>$P(X=k)=(1- \\frac{2,250}{150,000})^{k-1}(\\frac{2,250}{150,000})$ and the prize money would be $10,000 \\cdot k$</p>\n\n<p>(4) For this question, I derive the expected value for a geometric distribution to be:</p>\n\n<p>$U_{x} = \\frac{1}{p}$ and since $p = \\frac{2,250}{150,000}$ the expected draw is $\\frac{150,000}{2,250}$ = 66.67. So the expected winning is $10,000\\cdot67$ (rounding up)</p>\n\n<p>I would greatly appreciate any feedback for the way I have answered the questions - have I approached it correctly? In particular, what would be the difference between Q1 and Q3, and is my answer to Q4 correct?</p>\n\n<p>Many thanks in advance!</p>\n",
    "tags": "probability,self study",
    "answers": [
      "No answers available."
    ],
    "createdAt": "2025-07-12T08:02:12Z",
    "updatedAt": "2025-07-12T08:02:12Z",
    "similar_questions": [
      211653,
      94642,
      173,
      209369,
      77922
    ]
  },
  {
    "id": 70670,
    "title": "Phase space distribution of heads and tails (coin toss)",
    "body": "<p>So I have the equation </p>\n\n<p>$$h(t) = 1 + vt - \\frac{1}2 gt^2 \\pm sin(\\omega t) $$</p>\n\n<p>to describe the motion of a flipped coin. It is just a kinematics equation with an angular component added to it, where $h$ is the height of the coin, $v$ is the upwards velocity of the coin, $\\omega$ is the angular velocity, and $t$ is time. </p>\n\n<p>What I want to show is a 2D graph of angular velocity against velocity ($\\omega$ vs. $v$) so that for all combinations of $v$, $\\omega$, and $t$ such that $h = 0$ (when the coin lands), the graph is divided into regions that indicate whether the coin landed on heads or tails.</p>\n\n<p>What is the approach for doing this? I prefer Mathematica if possible.</p>\n",
    "tags": "probability,distributions",
    "answers": [
      "No answers available."
    ],
    "createdAt": "2025-07-12T08:02:12Z",
    "updatedAt": "2025-07-12T08:02:12Z",
    "similar_questions": [
      77543,
      77922,
      222278,
      185624,
      94581
    ]
  },
  {
    "id": 222241,
    "title": "improving classification using examples from 'other' classes",
    "body": "<p>I'm trying to classify a a small dataset of around 150 patients each with one of four different diseases. For each patient I have around 70 different features.</p>\n\n<p>I also have an \"extra\" dataset of patients with 'other' diseases (not belonging to the 4 I need to classify). These are all different diseases that the classifier is not supposed to work on. I was wondering if I can use this extra dataset to somehow regularize the classifier for those four specific diseases that I do care about. So far these are the ideas I came up with:</p>\n\n<p>1) Do softmax regression and input the extra dataset with labels [0.25,0.25,0.25,0.25]. This way I'm telling the classifier that these extra patients have labels that are \"equally different\" from the labels it needs to classify.</p>\n\n<p>2) Use logistic regression and input the extra dataset with labels [0,0,0,0]. This way I'm telling the classifier the the extra patients have labels that are not of any of the labels that it should classify.</p>\n\n<p>I'd like to hear other methods/opinions about the methods above/any other tips you might think are relevant to the problem.</p>\n",
    "tags": "machine learning,classification",
    "answers": [
      "No answers available."
    ],
    "createdAt": "2025-07-12T08:02:12Z",
    "updatedAt": "2025-07-12T08:02:12Z",
    "similar_questions": [
      55718,
      81840,
      104928,
      133552,
      77851
    ]
  },
  {
    "id": 94581,
    "title": "Plot and interpret ordinal logistic regression",
    "body": "<p>I have a ordinal dependendent variable, easiness, that ranges from 1 (not easy) to 5 (very easy).  Increases in the values of the independent factors are associated with an increased easiness rating.</p>\n\n<p>Two of my independent variables (<code>condA</code> and <code>condB</code>) are categorical, each with 2 levels, and 2 (<code>abilityA</code>, <code>abilityB</code>) are continuous.</p>\n\n<p>I'm using the <a href=\"http://cran.r-project.org/web/packages/ordinal/index.html\">ordinal</a> package in R, where it uses what I believe to be</p>\n\n<p>$$\\text{logit}(p(Y \\leqslant g)) = \\ln \\frac{p(Y \\leqslant g)}{p(Y &gt; g)} = \\beta_{0_g} - (\\beta_{1} X_{1} + \\dots + \\beta_{p} X_{p}) \\quad(g = 1, \\ldots, k-1)$$<br>\n(from @caracal's answer <a href=\"http://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression/38130#38130\">here</a>)</p>\n\n<p>I've been learning this independently and would appreciate any help possible as I'm still struggling with it.  In addition to the tutorials accompanying the ordinal package, I've also found the following to be helpful: </p>\n\n<ul>\n<li><a href=\"http://stats.stackexchange.com/questions/89474/interpretation-of-ordinal-logistic-regression\">Interpretation of ordinal logistic regression</a></li>\n<li><a href=\"http://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression/38130#38130\">Negative coefficient in ordered logistic regression</a></li>\n</ul>\n\n<p>But I'm trying to interpret the results, and put the different resources together and am getting stuck. </p>\n\n<ol>\n<li><p>I've read many different explanations, both abstract and applied, but am still having a hard time wrapping my mind around what it means to say: </p>\n\n<blockquote>\n  <p>With a 1 unit increase in condB (i.e., changing from one level to the next of the categorical predictor), the predicted odds of observing Y = 5 versus Y = 1 to 4 (as well as the predicted odds of observed Y = 4 versus Y = 1 to 3) change by a factor of exp(beta) which, for diagram, is exp(0.457) = 1.58. </p>\n</blockquote>\n\n<p>a. Is this different for the categorical vs. continuous independent variables?<br>\nb. Part of my difficulty may be with the cumulative odds idea and those comparisons. ... Is it fair to say that going from condA = absent (reference level) to condA = present is 1.58 times more likely to be rated at a higher level of easiness?  I'm pretty sure that is NOT correct, but I'm not sure how to correctly state it.</p></li>\n</ol>\n\n<p>Graphically,<br>\n1. Implementing the code in <a href=\"http://stats.stackexchange.com/questions/89474/interpretation-of-ordinal-logistic-regression\">this post</a>, I'm confused as to why the resulting 'probability' values are so large.<br>\n2. The graph of p (Y = g) in <a href=\"http://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression/38130#38130\">this post</a> makes the most sense to me ... with an interpretation of the probability of observing a particular category of Y at a particular value of X.  The reason I am trying to get the graph in the first place is to get a better understanding of the results overall.</p>\n\n<p>Here's the output from my model:</p>\n\n<pre><code>m1c2 &lt;- clmm (easiness ~ condA + condB + abilityA + abilityB + (1|content) + (1|ID), \n              data = d, na.action = na.omit)\nsummary(m1c2)\nCumulative Link Mixed Model fitted with the Laplace approximation\n\nformula: \neasiness ~ illus2 + dx2 + abilEM_obli + valueEM_obli + (1 | content) +  (1 | ID)\ndata:    d\n\nlink  threshold nobs logLik  AIC    niter     max.grad\nlogit flexible  366  -468.44 956.88 729(3615) 4.36e-04\ncond.H \n4.5e+01\n\nRandom effects:\n Groups  Name        Variance Std.Dev.\n ID      (Intercept) 2.90     1.70    \n content  (Intercept) 0.24     0.49    \nNumber of groups:  ID 92,  content 4 \n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \ncondA              0.681      0.213    3.20   0.0014 ** \ncondB              0.457      0.211    2.17   0.0303 *  \nabilityA           1.148      0.255    4.51  6.5e-06 ***\nabilityB           0.577      0.247    2.34   0.0195 *  \n\nThreshold coefficients:\n    Estimate Std. Error z value\n1|2   -3.500      0.438   -7.99\n2|3   -1.545      0.378   -4.08\n3|4    0.193      0.366    0.53\n4|5    2.121      0.385    5.50\n</code></pre>\n",
    "tags": "r,regression,logistic",
    "answers": [
      "<p>My <a href=\"http://biostat.mc.vanderbilt.edu/rms\" rel=\"nofollow\">Regression Modeling Strategies</a> course notes has two chapters on ordinal regression that may help.  See also <a href=\"http://www.citeulike.org/user/harrelfe/article/13265110\" rel=\"nofollow\">this</a> tutorial. </p>\n\n<p>The course notes go into detail about what model assumptions mean, how they are checked, and how to interpret the fitted model.</p>\n"
    ],
    "createdAt": "2025-07-12T08:02:12Z",
    "updatedAt": "2025-07-12T08:02:12Z",
    "similar_questions": [
      156619,
      94521,
      179329,
      222278,
      57807
    ]
  },
  {
    "id": 77845,
    "title": "Inverse function for a non-decreasing CDF",
    "body": "<p>For a CDF that is not strictly increasing, i.e. its inverse is not defined, define the quantile function </p>\n\n<p>$$F^{-1} (u) =\\inf \\{x: F(x) \\geq u \\},\\quad 0&lt;u&lt;1. $$</p>\n\n<p>Where U has a uniform $(0,1)$ distribution. Prove that the random variable $F^{-1} (u)$ has cdf $F(x)$.</p>\n\n<p>In case of a strictly increasing CDF the proof is quite easy because the inverse is defined. Define $X=F^{-1} (u)$</p>\n\n<p>$$ P\\left[X&lt;x \\right]=  P \\left [F^{-1} (U) \\leq x \\right]= P \\left[U \\leq F(x) \\right] =F(x)  $$</p>\n\n<p>But how do I accomondate the nondecreasing CDF whose inverse is given by the quantile function? I am a begginer so any help is welcome. Thank you.</p>\n",
    "tags": "probability,self study",
    "answers": [
      "<p>Let $U$ be a $\\mathrm{U}[0,1]$ r.v. Let $F$ be a distribution function. Remember that every distribution function is non decreasing and right continuous. Define the quantile function\n$$\n  F^{-1}(u) = \\inf\\{x:u \\leq F(x)\\}.\n$$\nDrawing a picture</p>\n\n<p><img src=\"http://i.stack.imgur.com/k1klQ.png\" alt=\"enter image description here\"></p>\n\n<p>we see that $F^{-1}(u)\\leq x$ if and only if $u\\leq F(x)$. Please, make sure that you understand both implications. Therefore, if $X=F^{-1}(U)$, then\n$$\n  P(X\\leq x)=P(F^{-1}(U)\\leq x)=P(U\\leq F(x))=F(x) \\, .\n$$</p>\n"
    ],
    "createdAt": "2025-07-12T08:02:12Z",
    "updatedAt": "2025-07-12T08:02:12Z",
    "similar_questions": [
      156680,
      25387,
      209369,
      94642,
      209413
    ]
  },
  {
    "id": 179329,
    "title": "Pr(Z>|z|) values and the level of significance",
    "body": "<p>If your logistic regression fit has coefficients with the following attributes, do you look at the values of <code>Pr(Z&gt;|z|)</code> are smaller than 0.95 to determine whether that variable is needed at a 5% level of significance? </p>\n\n<p>ie. If <code>Pr(&gt;|z|)</code> is 0.964, this variable is not needed at 5% significance.</p>\n\n<p><a href=\"http://i.stack.imgur.com/6UTBa.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/6UTBa.png\" alt=\"enter image description here\"></a></p>\n",
    "tags": "r,regression",
    "answers": [
      "<p>You are using the normal approximation and specifically the Wald test so you do what you would do in a regular t-test. That is, you reject the null hypothesis if the probability of the event $\\left\\{Z \\geq |z| \\right\\}$ is lower than the conventional threshold of $0.05$. Alternatively you fail to reject the null hypothesis if your p-value is not small enough.</p>\n",
      "<p>The value of the coefficient and its large standard error suggest that what we are seeing here is separation or the Hauck-Donner effect which has its own tag <a href=\"/questions/tagged/hauck-donner-effect\" class=\"post-tag\" title=\"show questions tagged &#39;hauck-donner-effect&#39;\" rel=\"tag\">hauck-donner-effect</a> which has a clear and helpful wiki excerpt. I think therefore the debate about $t$ versus $z$ is a red herring. Profile likelihood would be the way to go or reformulating the problem.</p>\n",
      "<p>Firstly, the p-value given for the Z-statistic would have to be interpreted as how likely it is that a result as extreme or more extreme than that observed would have occured under the null hypothesis. I.e. 0.96 would in principle mean that the data are providing very little evidence that the variable is needed (while small values such as, say, $p\\leq 0.05$ would provide evidence for the likely relevance of the variable, as pointed out by others already). However, a lack of clear evidence that the variable is needed in the model to explain the this particular data set would not imply evidence that the variable is not needed. That would require a difference approach and with a very larege standard error one would not normally be able to say that the variable does not have an effect. Also, it is a very bad idea to decide which variables are to be included in a model based on p-values and then fitting the model with or without them as if no model selection had occurred.</p>\n\n<p>Secondly, as also pointed out by others, when you get this huge a coefficient (corresponds to an odds ratio of $e^{-14.29}$) and standard error from logistic regression, you typically have some problem. E.g. the algorithm did not converge or there is complete separation in the data. If your model really did only include an intercept, then perhaps there are no events at all, and all records did not have an outcome? If so, then a standard logistic regression may not be able to tell you a lot. There are some alternatives for such sparse data situations (e.g. a Bayesian analysis including the available prior information).</p>\n"
    ],
    "createdAt": "2025-07-12T08:02:12Z",
    "updatedAt": "2025-07-12T08:02:12Z",
    "similar_questions": [
      94519,
      55788,
      185425,
      94572,
      94521
    ]
  },
  {
    "id": 233264,
    "title": "Relative importance of predictors - Standardized coefficients in Ordinal Logistic Regression",
    "body": "<p>I am a 4th year psychology student. I need some help in understanding the coefficients in ORDINAL logistic regression. According to Williams (2009) \"Using Heterogeneous Choice Models To Compare Logit and Probit Coefficients Across Groups\", the predictor variables and residuals are already standardized to the logit distribution (variance = \u00cf\u0080*\u00cf\u0080/3 ), and, therefore, so are the reported coefficients in SPSS. Therefore, in order to compare the relative predictive strength of my variables in the model, I should just be able to directly compare the coefficients (or the odds ratios). However, how do I account for the differences in CI/standard error in my comparisons? </p>\n\n<p>For example,</p>\n\n<p>variable 1: B=.021, std error = .0068,  Exp(B) = 1.022, 95% CI = 1.008 to 1.035</p>\n\n<p>variable 2: B=.051, std error = .0174,  Exp(B) = 1.052, 95% CI = 1.017 to 1.089</p>\n\n<p>From comparison of Bs - Variable 2 is the stronger predictor, but the std error and CI are much larger. So, what conclusion can I make?</p>\n",
    "tags": "regression,logistic",
    "answers": [
      "<p>If i understand you correctly, and if you have two models to choose from with relatively different Betas, the important is too choose the model based on its accuracy rather than merely strength of Betas... I suggest you compare the models based on other fit indicators such as AIC, BIC as well as the classification accuracy.</p>\n\n<p>The confidence interval and std errors of <strong>Individual Betas</strong>, are not a good indicator when it comes to what model to choose.</p>\n\n<p>But if you are speaking about two Beta's within the same model, the predictive power is measured through it's <strong>coefficient</strong> and it's <strong>P- value</strong> rather than its CI or Std Error of betas.</p>\n\n<p>Also there's a good reponse to similar question of yours here: <a href=\"http://stats.stackexchange.com/questions/18208/how-to-interpret-coefficient-standard-errors-in-linear-regression\">How to interpret coefficient standard errors in linear regression?</a></p>\n",
      "<p>In general, if your predictors are on different metrics, then the subjective assessment of variable importance can not be easily made by simply comparing the raw sizes of the odds ratios.</p>\n\n<p>If all your predictors are continuous, then I think converting the variables to z-scores would be useful for getting a sense of their relative importance. You mention that you have a skewed numeric predictor. I don't think changes anything too much for whether z-scores are appropriate. Ultimately, you have a separate issue of whether you want to apply a shape transformation (z-scores just change mean and variance). If your variable is highly skewed, then consider a transformation, and then z-score the transformed variable.</p>\n\n<p>Some times, you have binary predictors. In that case, the 0-1 scoring is quite intuitive, especially if you have a few such variables.</p>\n"
    ],
    "createdAt": "2025-07-12T08:02:12Z",
    "updatedAt": "2025-07-12T08:02:12Z",
    "similar_questions": [
      94581,
      109249,
      200822,
      179336,
      25381
    ]
  },
  {
    "id": 109249,
    "title": "How can I (Should I?) use logistic regression/the logit function to predict outcome of a tennis match in a simple simulator?",
    "body": "<p>I am trying to create a tennis simulator. Specifically I am trying to make a 'random' simulator so that I can see how many times streaks of wins or losses occur, and then compare this to historical data to see if 'The Hot Hand Effect' exists or not. i.e. Do streaks of wins or losses occur more or less often than would be expected by chance. </p>\n\n<p>I create a pool of players and assign them a strength score between 0 and 10 from a normal distribution of mean 5 standard deviation 2. </p>\n\n<p>When I come to matching up players and deciding the outcome, my supervisor suggested using a logit model/logistic regression. I am not quite sure how this would work practically/mathematically though. I thought to somehow use the difference in strength scores of the two players with the logit function to give me a probability to put into a cointoss, with the probability of a head, i.e. a win for Player A, being equal to the logit model output. 'Somehow' is obviously very vague as I do not know how this would work, or even if it mathematically makes any sense. Does what I intend to do sound valid conceptually? How do I use the strength scores with the logit function to get the probability?</p>\n",
    "tags": "regression,logistic",
    "answers": [
      "<p>I'm not quite sure I understand your approach or the problem you are trying to solve.</p>\n\n<p>Seems like the only way your \"hot streak\" effect can exist is if players change their strength level overtime (meaning you can't just characterize their strength by taking an average of their strength). Is this the question you are really asking? Otherwise you need to define the \"hot streak\" more.</p>\n\n<p>However one thing to note is that you can absolutely extend logistic regression to the multivariate case, so you may not have to do the \"difference in strengths\" thing. You can actually just have the two strengths as separate parameters.</p>\n\n<p>Logistic regression is really just a form of linear regression. You are finding a function that takes in inputs <code>x</code>, and predicts the output <code>P</code>. The output <code>P</code> represents the two outcomes, win and lose. The logistic function takes the form: </p>\n\n<pre><code>P=\\frac{1}{1+e^{-\\omega x + c}}\n</code></pre>\n\n<p>If you extend it the multinomial case, <code>x</code> is just a vector of parameters and you are looking for the weights <code>\\omega</code> and <code>c</code></p>\n\n<p>It seems to me like you need to do the following:</p>\n\n<p>1) Train using existing statistics about strengths of the two players and the outcome of the game to find your <code>\\omega</code> and <code>c</code>. You can add more parameters you want even stuff like weather, etc. if you have enough of it</p>\n\n<p>2) Use your random generator to find the strengths of your two players. Not sure if this distribution should be completely normal, since the strength distribution of tennis players is not normal (you have a bunch of people who are mediocre and then a few extraordinary players and not really any terrible players). Then use your logistic function to simulate outcomes of the games.</p>\n\n<p>3) Record the number of hotstreaks</p>\n\n<p>If you provide more information on where you are confused I can try to help more.</p>\n"
    ],
    "createdAt": "2025-07-12T08:02:12Z",
    "updatedAt": "2025-07-12T08:02:12Z",
    "similar_questions": [
      81901,
      233264,
      156703,
      209369,
      94581
    ]
  },
  {
    "id": 155546,
    "title": "Online mean shift algorithms",
    "body": "<p>I am looking for an online algorithm which can identify mean shifting in a time series quickly, I have seen some algorithms that do so but they require 50+ samples in order to flag that the mean has changed</p>\n\n<p>any suggestions?</p>\n\n<p>Thanks</p>\n",
    "tags": "regression,time series,machine learning,hypothesis testing",
    "answers": [
      "<p>AUTOBOX <a href=\"http://www.autobox.com/cms/index.php/afs-university/intro-to-forecasting/doc_download/53-capabilities-presentation\" rel=\"nofollow\">http://www.autobox.com/cms/index.php/afs-university/intro-to-forecasting/doc_download/53-capabilities-presentation</a> can be very useful for this. As @EdM wisely suggested one needs to be able to control for false positives i.e. level of confidence BUT also be able to incorporate/specify a   minimum magnitude of the identified/proposed level shift. This is a very important and unique (to my knowledge) option when using AUTOBOX a program that I have helped develop. Their 30 day free version could be useful to you as a possible springboard to help you solve your (very common but often unstated) problem. An interesting feature is the capability to identify/isolate local time trends which often are falsely reported as level shifts. Hope this helps.</p>\n\n<p>There is no hard and fast rule for a minimum sample size as the important idea is not necessarily the number of observations but the signal to noise ratio for the effect versus the error process. The more pronounced/larger the ratio the smaller the number of observations that are needed to \"identify/capture\" the effect. I have examined many of the earlier attempts to perform this task and find that collectively their greatest failure is failing to seamlessly/correctly incorporate memory and remedies for various alternative Gaussian Violations into the heuristic mix.</p>\n"
    ],
    "createdAt": "2025-07-12T08:02:13Z",
    "updatedAt": "2025-07-12T08:02:13Z",
    "similar_questions": [
      26728,
      94570,
      173,
      298,
      109234
    ]
  },
  {
    "id": 880,
    "title": "Cross validation in very high dimension (to select the number of used variables in very high dimensional classification)",
    "body": "<p>My question is about cross validation when there are many more variables than observations. To fix ideas, I propose to restrict to the classification framework in very high dimension (more features than observation).</p>\n\n<p><strong>Problem:</strong> Assume that for each variable $i=1,\\dots,p$ you have a measure of importance $T[i]$ than exactly measure the interest of feature $i$ for the classification problem. The problem of selecting a subset of feature to reduce optimally the classification error is then reduced to that of finding the number of features. </p>\n\n<p><strong>Question:</strong> What is the most efficient way to run cross validation in this case (cross validation scheme)? My question is not about how to write the code but on the version of cross validation to use when trying to find the number of selected feature (to minimize the classification error) but how to deal with the high dimension when doing cross validation (hence the problem above may be a bit like a 'toy problem' to discuss CV in high dimension). </p>\n\n<p><strong>Notations:</strong>  $n$ is the size of the learning set, p the number of features (i.e. the dimension of the feature space).  By very high dimension I mean p>>n (for example $p=10000$ and $n=100$). </p>\n",
    "tags": "machine learning,classification",
    "answers": [
      "<p>You miss one important issue -- there is almost never such thing as T[i]. Think of a simple problem in which the sum of two attributes (of a similar amplitude) is important; if you'd remove one of them the importance of the other will suddenly drop. Also, big amount of irrelevant attributes is the accuracy of most classifiers, so along their ability to assess importance. Last but not least, stochastic algorithms will return stochastic results, and so even the T[i] ranking can be unstable. So in principle you should at least recalculate T[i] after each (or at least after each non trivially redundant) attribute is removed.</p>\n\n<p>Going back to the topic, the question which CV to choose is mostly problem dependent; with very small number of cases LOO may be the best choice because all other start to reduce to it; still small is rather n=10 not n=100. So I would just recommend random subsampling (which I use most) or K-fold (then with recreating splits on each step). Still, you should also collect not only mean but also the standard deviation of error estimates; this can be used to (approximately) judge which changes of mean are significant ans so help you decide when to cease the process.</p>\n"
    ],
    "createdAt": "2025-07-12T08:02:13Z",
    "updatedAt": "2025-07-12T08:02:13Z",
    "similar_questions": [
      55718,
      185624,
      185625,
      156619,
      133493
    ]
  },
  {
    "id": 25435,
    "title": "How to classify country names given possible alternate spellings or abbreviations?",
    "body": "<p>Let's say I have a list of users who have specified the country they reside in by typing in something. I want to find the total number of users who came from the US, the UK, and everywhere else. But, since users typed in their country manually my data looks like this:</p>\n\n<pre><code>USER   COUNTRY\n1      USA\n2      United States\n3      US\n4      UK\n5      England\n... \n</code></pre>\n\n<p>Are there any public datasets or good algorithms to solve this type of problem?</p>\n",
    "tags": "machine learning,classification",
    "answers": [
      "<p><a href=\"http://code.google.com/p/google-refine/\" rel=\"nofollow\">Google Refine</a> is a nice free solution for this, but it is likely that you will still have to do a fair bit of manual work.</p>\n",
      "<p>I would suggest calculating the <a href=\"http://en.wikipedia.org/wiki/Edit_distance\" rel=\"nofollow\">edit distance</a> between the entered string and a master list of known countries and country abbreviations.</p>\n"
    ],
    "createdAt": "2025-07-12T08:02:13Z",
    "updatedAt": "2025-07-12T08:02:13Z",
    "similar_questions": [
      156633,
      94521,
      70699,
      94519,
      185583
    ]
  },
  {
    "id": 77851,
    "title": "Finding optimal cutoff point with two linear regression models",
    "body": "<p>I am trying to get an optimal cut-off value dividing group with minimum sums of squares of residuals (=observed y - estimated y) the model is like below.</p>\n\n<blockquote>\n  <p>In group 1 : model y= a1x + b1z + C1v ...</p>\n  \n  <p>In group 2 : model y= a2x + b1z + C1v ...</p>\n</blockquote>\n\n<p>I have the data of y, x, z, v... The problem is the group 1 and 2 are not divided yet and the purpose of the analysis is finding optimal cut-off point of x using regression models.</p>\n\n<p>I searched again and again, but couldn't find the way to make models varying 'a' and share b1 and c1... and fitting it to data.</p>\n\n<p>I asked similar quesion in stackexchange, and somebody advised me the problems of this kind of approach, however, I need this approach, because it's some clinical research want to 'find' optimal (not perfect) cut-off point of x.</p>\n\n<p>The article I read described below\nThe authors of the article mentioned that they used R, but I cannot find any reference or examples about this kind of analysis.</p>\n\n<blockquote>\n  <p>To determine the relationship between serum 25(OH)D and iPTH concen-\n  trations while adjusting for confounders that could affect serum\n  25(OH)D concentrations (i.e., age, gender, body weight, calcium\n  intake, physical activity, and season of year), we considered two\n  linear regression models, one for subjects below a certain\n  concentration of serum 25(OH)D and the other for subjects above that\n  concentration. To determine the specific cutoffs, we fitted the two\n  linear regression models described above and calculated the sums of\n  squares of residuals (=observed PTH - estimated PTH) from the two\n  models for each concentration of serum 25(OH)D. The models with the\n  lowest residual sums of squares were our best models, and the\n  corresponding concentrations of serum 25(OH)D were defined as the\n  optimal cutoff values.</p>\n</blockquote>\n\n<p>Somebody said that this question is already answered in \"regression model fitting for define cut-off\" but, I don't think so... It's not regression discontinued design, because there is no a-priori cut-off. Finding cutoff is the purpose of analysis.\nThanks.</p>\n",
    "tags": "r,regression",
    "answers": [
      "<p>Package <code>segmented</code> could help you:</p>\n\n<blockquote>\n  <p>Given a linear regression model (of class \"lm\" or \"glm\"), segmented\n  tries to estimate a new model having broken-line relationships with\n  the variables specified in seg.Z. A segmented (or broken-line)\n  relationship is defined by the slope parameters and the break-points\n  where the linear relation changes. The number of breakpoints of each\n  segmented relationship is fixed via the psi argument, where initial\n  values for the break-points must be specified. The model is estimated\n  simultaneously yielding point estimates and relevant approximate\n  standard errors of all the model parameters, including the\n  break-points.</p>\n  \n  <p>[...] segmented implements the bootstrap restarting algorithm\n  described in Wood (2001). The bootstrap restarting is expected to escape the local optima of the objective function when the segmented relationship is flat and the log likelihood can have multiple local optima.</p>\n</blockquote>\n\n<p>Here is an example (simplified from the documentation):</p>\n\n<pre><code>library(segmented)\n\nset.seed(12)\nxx&lt;-1:100\nyy&lt;-2+1.5*pmax(xx-35,0)-1.5*pmax(xx-70,0)+rnorm(100,0,2)\ndati&lt;-data.frame(x=xx,y=yy)\n\nplot(y~x, data=dati)\n\nout.lm&lt;-lm(y~x,data=dati)\no&lt;-segmented(out.lm,seg.Z=~x,psi=list(x=c(30,60)),\n             control=seg.control(display=FALSE))\n\nsummary(o)\n# ***Regression Model with Segmented Relationship(s)***\n#   \n#   Call: \n#   segmented.lm(obj = out.lm, seg.Z = ~x, psi = list(x = c(30, 60)), \n#                control = seg.control(display = FALSE))\n# \n# Estimated Break-Point(s):\n#         Est. St.Err\n# psi1.x 36.00 0.5469\n# psi2.x 69.18 0.5455\n# \n# t value for the gap-variable(s) V:  0 0 \n# \n# Meaningful coefficients of the linear terms:\n#             Estimate Std. Error t value Pr(&gt;|t|)  \n# (Intercept)  0.81994    0.58906   1.392   0.1672  \n# x            0.05358    0.02854   1.877   0.0636 .\n# U1.x         1.50166    0.04127  36.387       NA  \n# U2.x        -1.55553    0.04540 -34.263       NA  \n# ---\n#   Signif. codes:  0 \u00e2\u0080\u0098***\u00e2\u0080\u0099 0.001 \u00e2\u0080\u0098**\u00e2\u0080\u0099 0.01 \u00e2\u0080\u0098*\u00e2\u0080\u0099 0.05 \u00e2\u0080\u0098.\u00e2\u0080\u0099 0.1 \u00e2\u0080\u0098 \u00e2\u0080\u0099 1\n# \n# Residual standard error: 1.705 on 94 degrees of freedom\n# Multiple R-Squared: 0.9949,  Adjusted R-squared: 0.9946 \n# \n# Convergence attained in 4 iterations with relative change 5.311563e-05 \n\nlines(predict(o))\n</code></pre>\n\n<p><img src=\"http://i.stack.imgur.com/U7eq8.png\" alt=\"enter image description here\"></p>\n\n<p>Including additional predictors is possible.</p>\n"
    ],
    "createdAt": "2025-07-12T08:02:13Z",
    "updatedAt": "2025-07-12T08:02:13Z",
    "similar_questions": [
      26790,
      200822,
      179336,
      135573,
      135514
    ]
  },
  {
    "id": 104928,
    "title": "Modelling a variable that is both categorical and continious",
    "body": "<p>I am attempting to design a study in which the impact of a variable on patient decision making is evaluated having adjusted for some other clinical variables. To give more detail, a clinical continuous variable (call it X) that is known to be associated with disease recurrence is used along some other clinical variables. Suppose, I have 100 patients and over the course of the study, the patients are randomized in such a way that half of them receive the variable X along other variables and choose to undergo treatment or not. While, the other half do not receive the info on variable X and based on other variables revealed to them choose to undergo treatment or not. Now, I'm interested to evaluate the impact of X on the decision (treatment vs no treatment), but as a continuous variable and the problem is half the patients did not receive the variable X. So, what is the appropriate approach here? Can I use a logistic regression model where I adjust for 1)other clinical variables, 2)whether someone received X or not (binary), and then also 3) X as continuous variable (for those who did not receive X, this value would be 0). I would appreciate your thoughts on this. thanks</p>\n",
    "tags": "regression,logistic",
    "answers": [
      "<p>This is just an interaction. You want to model the effect of being treated vs. not being treated on the outcome. But the effect of being treated is, coarsely, a function of how the treatment was administered.</p>\n\n<p>Let's say you were going to use logistic regression (which you probably should). Call $T=1$ when treated and $T=0$ otherwise, and $\\operatorname{Pr}(Y=1)=p$ where $Y$ is your binary outcome. You're asserting that\n$$\n\\log\\left(\\frac{p}{1-p}\\right)\\equiv\\operatorname{logit}\\left(p\\right)=\\alpha+\\beta T\n$$</p>\n\n<p>That is, you're saying that when someone is treated, the log odds of $Y$, for that someone, increase by $\\beta$. But you also want to say that, given $T=1$, the log odds of $Y$ will also vary with respect to treatment intensity $S$. The easiest way to incorporate this is to say\n$$\n\\operatorname{logit}\\left(p\\right)=\\alpha+\\beta T+\\gamma ST\n$$\nThat is, $Y|T=1$ varies linearly with $S$. The fact that $S$ is \"unobserved\" when $T=0$ doesn't matter because it would get multiplied by zero anyway. Importantly, this model is just one step of algebra away from\n$$\n\\operatorname{logit}\\left(p\\right)=\\alpha+(\\beta + \\gamma S)T\n$$\nwhich, again, is mathematically equivalent but also reveals a different interpretation: the coefficient on $T$ itself is a function of $S$.</p>\n\n<p>Interpreting this in terms of log odds is easy, because the effect is linear. But translating those log odds back to probabilities is somewhat more complicated. There is also an issue with statistical significance, in that p-values can be different depending on whether you're talking about log odds or raw probabilities. There's a thorough, if somewhat dense, walkthrough of these issue <a href=\"http://www.ats.ucla.edu/stat/stata/seminars/interaction_sem/interaction_sem.htm\" rel=\"nofollow\">here</a>, demonstrated in Stata.</p>\n\n<p>Finally, you will often hear that you should never include interaction terms in a regression without also including the \"main effects.\" That prescription obviously doesn't apply here.</p>\n"
    ],
    "createdAt": "2025-07-12T08:02:13Z",
    "updatedAt": "2025-07-12T08:02:13Z",
    "similar_questions": [
      200822,
      209369,
      57790,
      185625,
      104978
    ]
  },
  {
    "id": 70699,
    "title": "Qualitative variable coding in regression leads to \"singularities\"",
    "body": "<p>I have an independent variable called \"quality\"; this variable has 3 modalities of response (bad quality; medium quality; high quality). I want to introduce this independent variable into my multiple linear regression. When I have a binary independent variable (dummy variable, I can code <code>0</code> / <code>1</code>) it is easy to introduce it into a multiple linear regression model.</p>\n\n<p>But with 3 modalities of response, I have tried to code this variable like this :</p>\n\n<pre><code>Bad quality      Medium quality      High quality\n\n     0                1                  0\n     1                0                  0\n     0                0                  1\n     0                1                  0\n</code></pre>\n\n<p>But there is a problem when I try to do my multiple linear regression: the modality <code>Medium quality</code> gives me <code>NA</code>:  </p>\n\n<pre><code>Coefficients: (1 not defined because of singularities) \n</code></pre>\n\n<p>How can I code this variable \"quality\" with 3 modalities? Do I have to create a variable as a factor (<code>factor</code> in <code>R</code>) but then can I introduce this factor in a multiple linear regression?</p>\n",
    "tags": "r,regression",
    "answers": [
      "<p>The problem you are having (i.e., \"singularities\") can be thought of as an instance of <a href=\"http://en.wikipedia.org/wiki/Multicollinearity\">multicollinearity</a>.  Multicollinearity is often defined as:  </p>\n\n<blockquote>\n  <p>One or more predictor variables are a linear combination of other predictor variables.  </p>\n</blockquote>\n\n<p>This is, in fact, a rather strict definition; it is <em>perfect</em> multicollinearity, and you can easily have a problem with multicollinearity without any of your variables being <em>perfect</em> linear combinations of others.  Moreover, perfect multicollinearity rarely occurs.  However, you have stumbled across an case where it can occur.  Let us see how we can <em>perfectly</em> predict <code>medium quality</code> from our knowledge of the other two categories (we'll do this with a regression model where <code>medium quality</code> is $Y$, and <code>bad quality</code> &amp; <code>high quality</code> are $X_1$ &amp; $X_2$, respectively):<br>\n$$\nY = \\beta_0 + \\beta_1X_1 + \\beta_2X_2\n$$\nNote that there is no error term, $\\varepsilon$, specified, because we can predict this perfectly.  To do so, we set $\\beta_0 = 1$, $\\beta_1 = -1$, and $\\beta_2 = -1$.  Now, when you have <code>bad quality</code>, then $X_1=1$, which cancels out $\\beta_0$ ($1\\; + \\;-1\\!\\times\\! 1$), and $X_2=0$ so that term is canceled out as well ($-1\\times 0$).  Thus, we are left with a predicted value of $0$ for $Y$ (<code>medium quality</code>), which is exactly correct.  I will leave it to you to work out the other possibilities (it always works, in your case).  </p>\n\n<p>So what then should you do?  When representing a categorical variable, we typically use <em>reference cell coding</em> (often called 'dummy coding').  To do this, we pick one level of our categorical variable as the reference level; that level does not get its own dummy code, but is simply indicated by having all $0$'s in the dummy codes for all other levels.  The other levels of your categorical variable are represented by dummy codes just as you have already done.  (For some more information on this, you can see my answer here: <a href=\"http://stats.stackexchange.com/questions/21282//21292#21292\">Regression based for example on days of week</a>.)  If you are using <code>R</code>, you can use a <code>factor</code> and <code>R</code> will do this all for you--it will be done correctly, and it's much more convenient--nonetheless, it's worth understanding that this is what is happening 'behind the scenes'.  </p>\n",
      "<p>@gung has explained the theory clearly. Here's a practical example to illustrate:</p>\n\n<pre><code>set.seed(1)\npred1 &lt;- factor(c(\"bad\", \"med\", \"high\"), levels=c(\"bad\", \"med\", \"high\"))\ndf1 &lt;- data.frame(y=20*abs(runif(6)),\n                  x=rnorm(6),\n                  q=sample(pred1, 6, replace=TRUE)\n                  )\nl1 &lt;- lm(y ~ x, data=df1)\n### add variable q    \nl2 &lt;- lm(y ~ x + q, data=df1)\n### look at dummy variables generated in creating model\nmodel.matrix(l2)\n</code></pre>\n\n<p>This shows us that the reference level (all $0$s) is <code>bad</code> as seen here in row 4:</p>\n\n<pre><code>  (Intercept)          x qmed qhigh\n1           1  1.5952808    1     0\n2           1  0.3295078    0     1\n3           1 -0.8204684    0     1\n4           1  0.4874291    0     0\n5           1  0.7383247    1     0\n6           1  0.5757814    0     0\n</code></pre>\n\n<p>Now if we code the dummy variables ourselves and try to fit a model using all of them:</p>\n\n<pre><code>df1 &lt;- within(df1, {\n       qbad &lt;- ifelse(q==\"bad\", 1, 0)\n       qmed &lt;- ifelse(q==\"med\", 1, 0)\n       qhigh &lt;- ifelse(q==\"high\", 1, 0)\n       })    \nlm(y ~ x + qbad + qmed + qhigh, data=df1, singular.ok=FALSE)\n</code></pre>\n\n<p>We get the expected error: <code>singular fit encountered</code></p>\n"
    ],
    "createdAt": "2025-07-12T08:02:13Z",
    "updatedAt": "2025-07-12T08:02:13Z",
    "similar_questions": [
      94521,
      185583,
      104978,
      156619,
      57790
    ]
  },
  {
    "id": 77543,
    "title": "How do I get the amplitude and phase for sine wave from lm() summary?",
    "body": "<p>A simple sine curve could be written as $\\text{amplitude}\\cdot\\sin(x+\\text{phase})$. It can be also written in linear form as $a \\cdot \\sin(x) + b \\cdot \\cos(x)$.</p>\n\n<p>I run my analysis with R as:</p>\n\n<pre><code> fit.lm2 &lt;- lm(temperature~sin(2*pi*Time/366) + cos(2*pi*Time/366))\n summary(fit.lm2)\n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)             26.9188     0.1005  267.87  &lt; 2e-16\nsin(2 * pi * Time/366)   1.7468     0.1390   12.56  &lt; 2e-16\ncos(2 * pi * Time/366)   1.2077     0.1485    8.13 6.94e-11\n</code></pre>\n\n<p>The general form of the equation is $y = b_0 + b_1x_1 + b_2x_2$, thus, in my case, it can be written as $y = 26.9188x_0 + 1.7468x_1 + 1.2077x_2$.</p>\n\n<p>If I were to write it back to the simple sine form, $\\text{amplitude}\\cdot\\sin(x+\\text{phase})$, is it correct to say that:</p>\n\n<p>$\\text{amplitude} = b_0 = 26.9188$</p>\n\n<p>$\\text{phase} = \\arctan(\\frac{b_1}{b_2})$</p>\n\n<p>Is this the correct way how to do it?</p>\n\n<p>Thanks in advance.</p>\n",
    "tags": "r,regression",
    "answers": [
      "<p>The fit is</p>\n\n<p>$$y = 26.9188 + 1.7468\\sin(x) + 1.2077\\cos(x).$$</p>\n\n<p>Consider a general (non-zero) linear combination $\\alpha \\sin(x) + \\beta\\cos(x).$ Viewing $(\\alpha, \\beta)$ as a vector and writing it in polar coordinates $(r, \\phi)$ yields</p>\n\n<p>$$\\alpha = r \\cos(\\phi),\\quad \\beta = r \\sin(\\phi), \\quad r = \\sqrt{\\alpha^2+\\beta^2}$$</p>\n\n<p>whence</p>\n\n<p>$$\\alpha\\sin(x) + \\beta\\cos(x) = r\\cos(\\phi)\\sin(x) + r\\sin(\\phi)\\cos(x) = r\\sin(x+\\phi).$$</p>\n\n<p>$r$ is the <em>amplitude</em> and $\\phi$ is the <em>phase</em>.  In the present case $\\alpha=1.7468$ and $ \\beta=1.2077$  entailing</p>\n\n<p>$$r = \\sqrt{ 1.7468^2+1.2077^2 } = 2.123641$$</p>\n\n<p>and</p>\n\n<p>$$\\phi = \\arctan(\\beta, \\alpha) =  0.6049163.$$</p>\n\n<p>Consequently</p>\n\n<p>$$y = 26.9188 + 2.123641 \\sin(x + 0.6049163).$$</p>\n\n<p>This can be checked by plotting.  Here is <code>R</code> code to do it:</p>\n\n\n\n<pre><code>b0 &lt;- coef(fit.lm2)[1]\nalpha &lt;- coef(fit.lm2)[2]\nbeta &lt;- coef(fit.lm2)[3]\n\nr &lt;- sqrt(alpha^2 + beta^2)\nphi &lt;- atan2(beta, alpha)\n\npar(mfrow=c(1,2))\ncurve(b0 + r * sin(x + phi), 0, 2*pi, lwd=3, col=\"Gray\",\n      main=\"Overplotted Graphs\", xlab=\"x\", ylab=\"y\")\ncurve(b0 + alpha * sin(x) + beta * cos(x), lwd=3, lty=3, col=\"Red\", add=TRUE)\n\ncurve(b0 + r * sin(x + phi) - (b0 + alpha * sin(x) + beta * cos(x)), \n      0, 2*pi, n=257, lwd=3, col=\"Gray\", main=\"Difference\", xlab=\"x\", y=\"\")\n</code></pre>\n\n<p><img src=\"http://i.stack.imgur.com/Okfl9.png\" alt=\"Plots\"></p>\n\n<p><em>The two formulas agree to sixteen significant figures in double-precision arithmetic. The difference reflects pseudo-random floating point errors. (Because my data are not exactly the same as the original data, the \"difference\" plot will differ in its details but will still exhibit only tiny variations.)</em></p>\n"
    ],
    "createdAt": "2025-07-12T08:02:13Z",
    "updatedAt": "2025-07-12T08:02:13Z",
    "similar_questions": [
      133488,
      185583,
      145657,
      156619,
      135490
    ]
  },
  {
    "id": 135490,
    "title": "How to derive the conjugate prior of an exponential family distribution",
    "body": "<p>I am trying to derive the conjugate prior of the univariate Gaussian distribution over both the mean and the precision. I know that the prior I'm looking for is the normal-gamma distribution, but the idea is to derive this result.</p>\n\n<p>It seems that you can write down the conjugate prior of an exponential family distribution immediately if you represent it in the canonical exponential family form. <a href=\"http://en.wikipedia.org/wiki/Exponential_family#Bayesian_estimation:_conjugate_distributions\">This link</a> suggests a way to do just that. So far I have written the Gaussian probability function in this form and equated the following, but I don't see how the form of the conjugate prior is immediately obvious.</p>\n\n<p>$$p(\\mu, \\lambda | D) \\propto p(\\mu,\\lambda)p(D | \\mu, \\lambda)$$</p>\n",
    "tags": "probability,distributions",
    "answers": [
      "<p>The intuitive approach to conjugate priors is to try to deduce a family of distributions from the likelihood function. In the normal case, the likelihood is\n\\begin{align*}\\ell(\\mu,\\Sigma|x_1,\\ldots,x_n)&amp;\\propto|\\Sigma|^{-n/2}\\,\\exp\\left\\{\n-\\frac{1}{2}\\sum_{i=1}^n (x_i-\\mu)^\\text{T}\\Sigma^{-1}(x_i-\\mu)\\right\\}\\\\\n&amp;\\propto|\\Sigma|^{-n/2}\\,\\exp\\left\\{-\\frac{1}{2}\\sum_{i=1}^n (x_i-\\bar{x})^\\text{T}\\Sigma^{-1}(x_i-\\bar{x})\\right\\}\\\\&amp;\\qquad \\times \\exp\\left\\{-\\frac{n}{2}(\\bar{x}-\\mu)^\\text{T}\\Sigma^{-1}(\\bar{x}-\\mu)\\right\\}\\\\\n&amp;\\propto |\\Sigma|^{-n/2}\\,\\exp\\left\\{-\\frac{1}{2}\\text{tr}\\left(\\Sigma^{-1}S_n \\right)-\\frac{n}{2}(\\bar{x}-\\mu)^\\text{T}\\Sigma^{-1}(\\bar{x}-\\mu)\\right\\}\\\\\n\\end{align*}where $$S_n=\\sum_{i=1}^n (x_i-\\bar{x})(x_i-\\bar{x})^\\text{T}$$So we have three items in this likelihood:</p>\n\n<ol>\n<li>a power of $|\\Sigma|$;</li>\n<li>an exponential of a trace of $\\Sigma^{-1}$ times another matrix;</li>\n<li>an exponential of a quadratic in $\\mu$ with matrix $\\Sigma^{-1}$.</li>\n</ol>\n\n<p>And all three terms are stable by multiplication, i.e.</p>\n\n<ol>\n<li>$|\\Sigma|^a\\times |\\Sigma|^b = |\\Sigma|^{a+b}$;</li>\n<li>$\\exp\\left\\{-\\text{tr}\\left(\\Sigma^{-1}A\\right)\\right\\}\\times\\exp\\left\\{-\\text{tr}\\left(\\Sigma^{-1}B\\right)\\right\\}=\\exp\\left\\{-\\text{tr}\\left(\\Sigma^{-1}[A+B]\\right)\\right\\}$;</li>\n<li>$\\exp\\left\\{-(a-\\mu)^\\text{T}\\alpha\\Sigma^{-1}(a-\\mu)\\right\\}\\times\\exp\\left\\{-(b-\\mu)^\\text{T}\\beta\\Sigma^{-1}(b-\\mu)\\right\\}$ remains an exponential of a quadratic in $\\mu$ with matrix $\\Sigma^{-1}$ (with an extra term of the form $\\exp\\left\\{-\\text{tr}\\left(\\Sigma^{-1}A\\right)\\right\\}$ as this is not a perfect quadratic term).</li>\n</ol>\n\n<p>This means that the likelihood induces a shape of prior that remains stable by multiplication with another term with this shape. Which is a way of defining conjugacy. So, if I take my prior to be\n$$\\pi(\\mu,\\Sigma)\\propto|\\Sigma|^{-\\gamma/2}\\,\\exp\\left\\{-\\frac{1}{2}\\text{tr}\\left(\\Sigma^{-1}\\Xi \\right)-\\frac{\\nu}{2}(\\xi-\\mu)^\\text{T}\\Sigma^{-1}(\\xi-\\mu)\\right\\}$$the posterior will look the same, except that $\\gamma,\\Xi,\\nu,\\xi$ will change.</p>\n"
    ],
    "createdAt": "2025-07-12T08:02:14Z",
    "updatedAt": "2025-07-12T08:02:14Z",
    "similar_questions": [
      209413,
      57801,
      26748,
      200827,
      185624
    ]
  },
  {
    "id": 190067,
    "title": "Matching algorithm using probabilities and counts",
    "body": "<p>I have a data  set of id\u00e2\u0080\u0099s with location and time information. I want to determine which location is the home address of the id. For example, </p>\n\n<p>Id 1 could have the following data where percentage represents the percentage of visits per location for the (day,hour,min) tuple, i.e.,  A1 had 57% of the visits for the (Sunday,8,15) tuple and A3 had 43% of the visits for the  (Sunday,8,15) tuple.</p>\n\n<p>Id 1</p>\n\n<pre><code>           Location  day    hour  min Total Visits_Percentage   \n             A1      Sunday   8    15    4        .57                            \n             A1      Sunday   8    30    4        .57\n             A2      Monday  15     0    2        .66\n             A2      Tuesday 15     0    1        1.0\n             A2      Tuesday 11    15    1         .33\n             A3      Saturday 1    30    2        1.0\n             A3      Saturday 1    45    1        1.0\n             A3      Saturday 2     0    1        1.0\n             A3      Saturday 8    15    3         .43\n             A3      Saturday 8    30    3         .43\n</code></pre>\n\n<p>I have training data that has specific probabilities for home on any given (day, hour, min) tuple.</p>\n\n<p>For example, for the (Sunday,8,15) tuple, the home location appears with a probability of 41.2%. For the (Sunday,8,30) tuple the home location has a probability of 41.9%, the (Monday,15,0) tuple has probability  32.3%, (Tuesday,15,0) has probability 34.1%,\u00e2\u0080\u00a6</p>\n\n<p>I want to set up a model that either returns the probability that each of the id\u00e2\u0080\u0099s location is the home, i.e.,</p>\n\n<pre><code>   A1= .52\n   A2= .19\n   A3= .49\n</code></pre>\n\n<p>Or explicitly classifies each location as \u00e2\u0080\u009chome\u00e2\u0080\u009d or \u00e2\u0080\u009cnot home\u00e2\u0080\u009d. I welcome any ideas on an existing model that may fit this problem or strategies to build a custom algorithm.</p>\n",
    "tags": "machine learning,probability,classification",
    "answers": [
      "No answers available."
    ],
    "createdAt": "2025-07-12T08:02:14Z",
    "updatedAt": "2025-07-12T08:02:14Z",
    "similar_questions": [
      133441,
      94638,
      145657,
      81901,
      81840
    ]
  },
  {
    "id": 179336,
    "title": "Stacking ensembles to improve prediction",
    "body": "<p>I recently read this <a href=\"http://mlwave.com/kaggle-ensembling-guide/\" rel=\"nofollow\">blog</a> and it has many ideas for ensembling various models. I created three models for my training data, random forest model, SVM model and a KNN model. However when I use linear method to stack these ensemble methods together the error I get is lower than the RF model only about 40% of the time.</p>\n\n<p>The linear method that I employ for stacking is a plain one: determining coefficients of each model and minimizing their least squares.</p>\n\n<p>Am I missing something? This results in a \"best case\" scenario less than half the time, so I would be better off just using the randomforest model.</p>\n\n<p>Secondly, What exactly are meta features as described in this <a href=\"http://arxiv.org/pdf/0911.0460.pdf\" rel=\"nofollow\">paper</a>? How do I extract them for any data and successfully use them to build a lower error ensemble that individual models themselves?</p>\n",
    "tags": "r,machine learning",
    "answers": [
      "No answers available."
    ],
    "createdAt": "2025-07-12T08:02:14Z",
    "updatedAt": "2025-07-12T08:02:14Z",
    "similar_questions": [
      26790,
      135573,
      211677,
      185624,
      77851
    ]
  },
  {
    "id": 156680,
    "title": "Statistical Independence of Continuous variables",
    "body": "<p>I recently had a quiz where I had to find out whether two variables were dependent or independent. I know that given $f(x,y)$ if $h(x)*g(y) = f(x,y)$ then they are independent, else dependent.</p>\n\n<p>I am confused about how they come up with the upper and lower limit of integration and expected value of $h(x)$ or $g(y)$.</p>\n\n<p>An example was</p>\n\n<p>$$f(x,y) = \\frac{2x+3y}{5} \\ \\ if \\ \\ 0&lt;x&lt;1, \\ 0&lt;y&lt;2$$</p>\n\n<p>any other interval $f(x,y) = 0$</p>\n\n<p>Find if $x,y$ are independent and find $E(x)$.</p>\n\n<p>So how do you choose what the limits of integration are and how do you get $E(x)$?</p>\n",
    "tags": "probability,self study",
    "answers": [
      "No answers available."
    ],
    "createdAt": "2025-07-12T08:02:14Z",
    "updatedAt": "2025-07-12T08:02:14Z",
    "similar_questions": [
      200822,
      211696,
      298,
      25387,
      209413
    ]
  },
  {
    "id": 135514,
    "title": "How to know if my data is balanced or imbalanced for an ROC curve analysis?",
    "body": "<p>I am doing a research on the reliability of different models in detecting hidden defects in a test specimen. I have made a test specimen with defect prevalence about 25% (12 positive out of 49 total points). Is this data considered as a balanced or imbalanced data for this purpose? What do you recommend to use with this data set, ROC or PR curve? if it is imbalanced, what is the minimum percentile that satisfies the balanced data condition? </p>\n",
    "tags": "machine learning,hypothesis testing,logistic",
    "answers": [
      "<p>There is no hard threshold, but this is considered balanced. If you have 1% or less positives the situation is very different. </p>\n\n<p>The question, however, depends not so much on class balance as on what performance measures you care about. ROC and PR curves, though related, visualize different aspects of a model. </p>\n"
    ],
    "createdAt": "2025-07-12T08:02:14Z",
    "updatedAt": "2025-07-12T08:02:14Z",
    "similar_questions": [
      135573,
      77851,
      133552,
      185625,
      57807
    ]
  },
  {
    "id": 211636,
    "title": "R - Pearson correlation in assessing multivariate regression performances",
    "body": "<p>How to assess the performance of the multivariate regression outputs in different scenarios using Pearson's correlation in R ? Is it a straight process to compare the correlation of expected values and observed values ?</p>\n",
    "tags": "r,regression",
    "answers": [
      "No answers available."
    ],
    "createdAt": "2025-07-12T08:02:15Z",
    "updatedAt": "2025-07-12T08:02:15Z",
    "similar_questions": [
      133488,
      25316,
      200822,
      185425,
      121465
    ]
  },
  {
    "id": 133488,
    "title": "Interpreting high p value and low correlation value",
    "body": "<p>I am trying to run regression on financial data in R. I am new to regression analysis so I am finding it to difficult to interpret certain scenarios. I have the code as follows:</p>\n\n<pre><code>#regression analysis\nfit &lt;- lm(fiveMinReturns~RegressionData, data=maindata)\nsummary(fit) # show results\n#correlation\ncor(maindata$fiveMinReturns,maindata$RegressionData,use=\"everything\")\n</code></pre>\n\n<p>My output is: </p>\n\n<pre><code>Call:\nlm(formula = fiveMinReturns ~ RegressionData, data = maindata)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.205790 -0.001144 -0.000062  0.001117  0.156418 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    6.346e-05  8.785e-06   7.223 5.09e-13 ***\nRegressionData 1.597e-07  1.432e-08  11.155  &lt; 2e-16 ***\n---\nSignif. codes:  0 \u00e2\u0080\u0098***\u00e2\u0080\u0099 0.001 \u00e2\u0080\u0098**\u00e2\u0080\u0099 0.01 \u00e2\u0080\u0098*\u00e2\u0080\u0099 0.05 \u00e2\u0080\u0098.\u00e2\u0080\u0099 0.1 \u00e2\u0080\u0098 \u00e2\u0080\u0099 1\n\nResidual standard error: 0.004035 on 210912 degrees of freedom\nMultiple R-squared:  0.0005896, Adjusted R-squared:  0.0005849 \nF-statistic: 124.4 on 1 and 210912 DF,  p-value: &lt; 2.2e-16\n\ncor(maindata$fiveMinReturns,maindata$RegressionData,use=\"everything\")\n[1] 0.02428219\n</code></pre>\n\n<p>p-value is very small that means two variables are tightly coupled, but correlation is small too.\nMy question is how do I evaluate this situation?\nCan we say that this equation will give correct results almost every time?\nWhich scenario suggests both p-value and correlation both to be really small?\nWhat measures should i take to improve the result? </p>\n",
    "tags": "r,regression",
    "answers": [
      "<p>One scenario would be a huge sample size (I'm guessing you have a very big data set), and some small effect that is significant statistically in that it explains some portion of variance (less than 1% in your case), but not particularly useful from a predictive standpoint if your goal is to predict the FiveMinReturns variable.</p>\n",
      "<p>Significance addresses whether or not the data are similar to the null hypothesis.  Specifically, the p-value indicates the probability of observing a correlation as strong as the one you just observed  (or stronger) , if the null hypothesis (i.e. 'no correlation') really were true. </p>\n\n<p>With a huge sample size (210912 DF), you have great power to detect even a small divergence from the null), which is what you are observing.  </p>\n\n<p>I often find that people confuse the concepts of meaningful (is the correlation big enough to care about) versus significance (is the data consistent with the null hypothesis).</p>\n",
      "<blockquote>\n  <p>My question is how do I evaluate this situation?</p>\n</blockquote>\n\n<p>It means that the relationship between your two variables is very small, and only detected because your analyses had massive power (df of 200k+).</p>\n\n<blockquote>\n  <p>Can we say that this equation will give correct results almost every time? </p>\n</blockquote>\n\n<p>No, again, because the relationship between the two variables is very small, and your data do not explain a lot of the underlying variability in your dependent measure. Specifically, your model has an R^2 of 0.0005849, which means that your independent variable explains .05% (not 5%) of all the variability in the dependent variable.</p>\n\n<blockquote>\n  <p>Which scenario suggests both p-value and correlation both to be really small?</p>\n</blockquote>\n\n<p>If there is a relationship between the two variables, regardless of how large or small (i.e., H1 is true), the p-value will become smaller as your sample grows larger. So, it is entirely possible that the relationship between your variables is tiny, and yet you've still managed to detect it because your sample is so huge.</p>\n\n<blockquote>\n  <p>What measures should i take to improve the result?</p>\n</blockquote>\n\n<p>I'm not sure what you mean. The result is fine as it is. You tested a hypothesis and the data do not really support it.</p>\n",
      "<p>You can see what that means visually by checking this paper in PeerJ. They show an example of a significant relationship with low correlation.</p>\n\n<p><a href=\"https://peerj.com/articles/589/\" rel=\"nofollow\">https://peerj.com/articles/589/</a></p>\n\n<p>Interpreting your results is more than just R2 and p-values. </p>\n\n<p>Your regression coefficient is 0.00000016, meaning that for a one unit change in X, you only get a 0.00000016 unit change in y. This is a very tiny effect size. However, as others have pointed out, you have a giant sample size, so your standard error of that estimate is so low that you're virtually guaranteed to reject the hypothesis that there is no effect. Next, looking at the R2 value, it suggests that your X data have little predictive or explanatory power for you response. So your point estimates of mean responses (basically the regression line) provide very little information about the individual responses. The higher the R2, the closer the scatter of points is to the regression line, the better the regression line approximates the individual response. You need to consider all three of these things separately to get a solid interpretation.</p>\n"
    ],
    "createdAt": "2025-07-12T08:02:15Z",
    "updatedAt": "2025-07-12T08:02:15Z",
    "similar_questions": [
      25381,
      109234,
      94521,
      185583,
      57807
    ]
  },
  {
    "id": 133552,
    "title": "Name and methods for classification with 'unknown' as acceptable result",
    "body": "<p>What is it called when, in a classification task, it is acceptable that some data-points do not receive a label? And what classifiers are suitable?</p>\n\n<p>I have a dataset with a two valued target variable. After manually tweaking the results of the CN2 rule-induction algoritm i get:</p>\n\n<ul>\n<li>A few short rules that select sizable parts of the data (ie do not overfit)</li>\n<li>These rules are meaningful to a domain expert</li>\n</ul>\n\n<p>These rules accurately label data-points belonging in each group, but about 35% of the data-points are not labelled at all. The distribution of classes in the unlabelled data is the same as in the orginal dataset. In other words, the probability of falling in either group, for the unlabeled data-points, is the same as the a-priori probability.</p>\n\n<p>These are acceptable results, because the number of false positives is low for both labels. In real life this kind of classification is common, think of a doctor who runs tests to exclude certain conditions.<br>\nIt also is the best classification i have been able to come up with for this data-set.</p>\n\n<p>My questions are:</p>\n\n<ul>\n<li>Is there a name for this kind of classification?</li>\n<li>What classifiers can do this out of the box? Here i manually tweaked rules generated bij CN2.</li>\n</ul>\n",
    "tags": "machine learning,classification",
    "answers": [
      "<p>Any classification which returns probabilities (or at least some numeric score), can transformed in a classification with unknown. Suppose you have a binary classifier which returns the probability of an instance to belong to one of the classes. A probability closer to $1$ means a high probability to belong to class $A$, and a probability value closer to $0$ means a high probability to belong to class $B$ (to not belong to class $A$). You cand design your decision function such as that if the probability is belong a low threshold value (let's say 0.40) will be classified as $B$, if is greater than a high threshold value (let's say 0.60) will be classifier as $A$, and is undecided if the value is in between the threshold values. </p>\n\n<p>Obviously, you can extend this idea to scores instead of probabilities, and also to multi-class algorithms. For the later case you can use a min threshold value for a discriminant function.</p>\n"
    ],
    "createdAt": "2025-07-12T08:02:15Z",
    "updatedAt": "2025-07-12T08:02:15Z",
    "similar_questions": [
      156619,
      211709,
      121408,
      222278,
      156633
    ]
  },
  {
    "id": 133439,
    "title": "Threshold selection by intersection of Sensitivity and Specificity",
    "body": "<p>Some days ago, I learned in a lecture that the intersection of Sensitivity and Specificity provides an optimal compromise for choosing a classification threshold for logit or probit models. However, no one told me in which sense it is optimal. Is there some criterion which is minimized or maximized by doing it? I think there are several different approaches for choosing a threshold. So, I doubt the intersection of Sensitivity and Specificity is the only one.</p>\n",
    "tags": "self study,classification",
    "answers": [
      "<p>This overall approach is inconsistent with the theory of optimum decision making.  The goal of a logit or probit model is to accurately estimate the probability of an event - nothing more, nothing less.  Risk estimation nicely avoids the multitude of problems that arise from seeking thresholds, and avoids the use of improper accuracy scoring rules such as sensitivity and specificity.</p>\n"
    ],
    "createdAt": "2025-07-12T08:02:15Z",
    "updatedAt": "2025-07-12T08:02:15Z",
    "similar_questions": [
      77851,
      200822,
      211677,
      109249,
      26790
    ]
  },
  {
    "id": 211653,
    "title": "Statistics 60 duels found between two people",
    "body": "<p>It's been a long time since I took stats, and I'm forgetting a good way to explain this.  Friend and I fought 60 duels.   I won 40, he won 20.    He's upset.</p>\n\n<p>I'm trying to explain to him that our skill level is pretty close to the same and that its probable that it was just a bad outcome, and within a standard range of chance. </p>\n\n<p>How can I show this more academically?</p>\n",
    "tags": "probability,distributions",
    "answers": [
      "<p>Use the binomial distribution.</p>\n\n<p>Suppose the probability of winning is $p=\\frac12$, which it is if you indeed have the same skill level. </p>\n\n<p>Call the number of wins for you $X$.</p>\n\n<p>Then $P(X \\geq 40) = 1-P(X \\leq 39) = 1-F(39;60,\\frac12)\\approx0.0067$.</p>\n\n<p>I'm sorry, but stats aren't really on your side.</p>\n\n<p>However, suppose you are a little better, and your probability of winning is $p=0.6$. Then $P(X \\geq 40) = 1-P(X \\leq 39) = 1-F(39;60,0.6)\\approx0.179$. Maybe you can tell him this. You can use <a href=\"http://stattrek.com/online-calculator/binomial.aspx\" rel=\"nofollow\">this online calculator</a> to calculate with other values for $p$. </p>\n"
    ],
    "createdAt": "2025-07-12T08:02:16Z",
    "updatedAt": "2025-07-12T08:02:16Z",
    "similar_questions": [
      70668,
      25385,
      94581,
      109234,
      211661
    ]
  },
  {
    "id": 211661,
    "title": "Is there a clustering method that can deal with levels/grouping",
    "body": "<p>I have a matrix of pearson correlations that I would like to cluster on similarity and identify correlated networks. However the variables are part of groups and I'm not interested correlations within groups; but between groups. </p>\n\n<p>I hypothesize that there shouldn't be any correlation within groups (or at least random) and that there is a cluster of variables among the groups  (hope I'm clear). Specifically I would like to identify a cluster of correlated variables that consists of  one variable from group1, one of group2, one of group3, etc. </p>\n\n<p>Are there clustering methods that can take into account leveled data (preferably in R or Python), would it be possible?</p>\n\n<pre><code>Group   Var A   B   C   D   E   F   \u00e2\u0080\u00a6\n1   A   1.00    -0.85   -0.85   -0.78   0.68    -0.87   \n1   B   0.20    1.00    -0.10   0.20    -0.40   0.78    \n1   C   0.10    0.30    1.00    0.40    -0.71   0.89    \n2   D   0.30    0.10    0.79    1.00    -0.45   0.66    \n2   E   0.90    -0.70   -0.71   -0.45   1.00    -0.71   \n2   F   0.40    0.78    0.89    0.66    -0.71   1.00    \n4   G   0.20    -0.59   -0.43   -0.71   0.09    -0.17   \n4   H   0.40    -0.81   -0.71   -0.72   0.44    -0.55   \n4   I   0.90    -0.52   -0.59   -0.24   0.71    -0.69   \n5   J   0.48    -0.49   -0.56   -0.13   0.80    -0.58   \n</code></pre>\n",
    "tags": "r,machine learning",
    "answers": [
      "No answers available."
    ],
    "createdAt": "2025-07-12T08:02:16Z",
    "updatedAt": "2025-07-12T08:02:16Z",
    "similar_questions": [
      133441,
      145657,
      211677,
      135565,
      104978
    ]
  },
  {
    "id": 133502,
    "title": "How to find the distribution of a function of multiple, not necessarily independent, random variables?",
    "body": "<p>If $Y$ is a random variable defined as $Y=g(X_1,X_2)$, where $X_1$ and $X_2$ are two different random variables whose distributions are known (say with pdf's $f_{X_1}$ and $f_{X_2}$), how do we find the distribution of $Y$ (i.e. the pdf $f_Y$)?</p>\n\n<p>Is there a general method to solve this problem? If there isn't, let us consider two specific cases which I am interested in.</p>\n\n<p>a) $Y=X_1X_2$; $X_1$ and $X_2$ being independent random variables.</p>\n\n<p>b) $Y=X_1X_2$; $X_1$ and $X_2$ being correlated random variables.</p>\n",
    "tags": "probability,distributions,self study",
    "answers": [
      "<p>For $a &gt; 0$, \n$\\displaystyle P\\{XY &gt; a\\} = \\int_0^\\infty \\int_{\\frac ax}^\\infty f_{X,Y}(x,y)\n\\,\\mathrm dy\\,\\mathrm dx + \\int_{-\\infty}^0 \\int_{-\\infty}^{\\frac ax} f_{X,Y}(x,y)\n\\,\\mathrm dy\\,\\mathrm dx$.<br>\nFrom this, you can deduce $P\\{XY \\leq a\\} = F_{XY}(a)$ for $a &gt;0$ and hence the\ndensity function for $a &gt;0$ by differentiating $F_{XY}(a)$ with respect\nto $a$. A similar calculation can be done for $a &lt; 0$; setting up\nthe integrals and their limits is left to you as an exercise.</p>\n\n<p>If you understand\nthe Fundamental Theorem of Calculus well enough to be able to differentiate\nan integral with respect to a parameter that appears only in the limits,\nthen you can even avoid calculating the inner integrals in both cases.\nSome textbook writers even exhibit the pdf of $XY$ as a mystical magical\nintegral involving absolute values etc that gives the answer for all\n$a \\in (-\\infty,\\infty)$, but using this formula in any actual calculation \nusually results\nin disaster for beginners. I am not a trained professional by any means,\nbut I still advise you not to try it at home.</p>\n\n<p>Note that this answers both questions asked as long as you know the \n<em>joint</em> pdf\nof $X$ and $Y$ (e.g. $X$ and $Y$ are independent with known pdfs) but\nnot if you know only that $X$ and $Y$ are correlated without knowing\nthe joint pdf. As Xi'an says in a comment, your second question\nhas no answer in general</p>\n"
    ],
    "createdAt": "2025-07-12T08:02:16Z",
    "updatedAt": "2025-07-12T08:02:16Z",
    "similar_questions": [
      109304,
      77922,
      200822,
      104978,
      185625
    ]
  },
  {
    "id": 211668,
    "title": "How to calculate the mean of a certain urn experiment",
    "body": "<p>I'm sorry if my style of formulation is a bit sloppy</p>\n\n<p>Let us assume that we have an abstract urn with balls having $S$ different colors.<br>\nThe probbility that the color $i$ ranging from $1..S$ has $n$ balls is $P(N=n)$, where $P(N=n)\\tilde{}Log(p)$ in my case. Thus, in the mean, the number of colors having $n$ balls is simply $S*P(N=n)$.</p>\n\n<p>The experiment:<br>\nA urn is created by making a concrete realization of the previous distribution. We then count the number of balls of each color and sort the colors by frequency. </p>\n\n<p>The question:\nWhat is the mean (normalized) frequency of the most frequent (second most frequent and so on) color?</p>\n\n<p>Any help is appreciated :)</p>\n",
    "tags": "probability,distributions",
    "answers": [
      "No answers available."
    ],
    "createdAt": "2025-07-12T08:02:16Z",
    "updatedAt": "2025-07-12T08:02:16Z",
    "similar_questions": [
      57807,
      190106,
      57850,
      880,
      94581
    ]
  },
  {
    "id": 211670,
    "title": "How to fit a short time series model using ARIMA?",
    "body": "<p>I want to make predictions use ARIMA in forecast package. I find that basically the prediction is just a lag of the actuals. Is there any way that I can better fit the model or any other approach available? (I find the ARIMA parameters through function <code>auto.arima</code> in \"forecast\" package in R.)</p>\n\n<pre><code>Model and Plot:\n        fit &lt;- arima(dataf$actuals, xreg=dataf$regressor,order=c(0,1,0))\n        labDates &lt;- seq(as.Date(\"2016-01-01\", format = \"%Y-%m-%d\"),as.Date(\"2016-01-16\", format = \"%Y-%m-%d\"),by = \"day\")\n        plot(labDates, dataf$actuals,col=\"red\",type='l')\n        lines(labDates,fitted(fit),col=\"blue\")\n        legend('topleft',c(\"Actual Number\",\"Predicted Number\"),pch=c(20,20,20),col=c(\"red\",\"blue\"))\n\nData:\n    actuals&lt;-c(26952, 38178, 36377, 45718, 40393, 43111, 39947, 40853, 38792, 41816, 46091, 41866, 35701, 52738,73834, 82813) \n    actuals_next&lt;-c(38178, 36377, 45718, 40393, 43111, 39947, 40853, 38792, 41816, 46091, 41866, 35701, 52738, 73834,82813,NA)  \n    regressor&lt;-c(519020, 671049 ,501083 ,288259 ,290899 ,260817, 276166, 274859 ,279405, 286689, 234050,95562,15138,  16401,  27145,  53717)    \n    dataf&lt;-as.data.frame(cbind(actuals, actuals_next, regressor))\n</code></pre>\n",
    "tags": "r,time series",
    "answers": [
      "No answers available."
    ],
    "createdAt": "2025-07-12T08:02:16Z",
    "updatedAt": "2025-07-12T08:02:16Z",
    "similar_questions": [
      25387,
      57807,
      77915,
      185583,
      135565
    ]
  },
  {
    "id": 211677,
    "title": "Linear Mixed Model with few and ranked subjects",
    "body": "<p>I am studying linear mixed model recently, and my data have only 6 subjects and those are ranked groups of observations (Tier 1 customers > Tier 2 customers > Tier 3 > ... > Tier 6)</p>\n\n<p>The formula looks like this:</p>\n\n<pre><code>lmerfit &lt;- lmer(data=data, y ~ ( (1 + x1) | Tier ) + x2 )\n</code></pre>\n\n<p>In this case, intercepts of random effects are random, but the slopes (sensitivity to x1) are ranked corresponding to the groups:</p>\n\n<p><a href=\"http://i.stack.imgur.com/kVWzh.gif\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/kVWzh.gif\" alt=\"enter image description here\"></a></p>\n\n<p>I do worry about this: usually the random effects should be randomly distributed, but apparently the observations are too few and they are not random in my model. </p>\n\n<p>If I already know the subjects are going to have ranked sensitivity to x1, but not sure the exact values of betas, can I use linear mixed model as an unsupervised (or \"lazy\") model to cluster the data?</p>\n",
    "tags": "r,regression",
    "answers": [
      "No answers available."
    ],
    "createdAt": "2025-07-12T08:02:16Z",
    "updatedAt": "2025-07-12T08:02:16Z",
    "similar_questions": [
      94619,
      94519,
      55788,
      179336,
      179329
    ]
  },
  {
    "id": 133493,
    "title": "Should image classifier be trained using colormap pixels or the actual value?",
    "body": "<p>For example, I have a population density map of a 100 x 100 km square region. </p>\n\n<p>Each part of the rectangular region represents the population density i.e. (1,1) -> 128 people, (100,100) -> 50 people</p>\n\n<p>I plug this square matrix into MATLAB using the \"imagesc\" command and it generates an image presenting the population density.</p>\n\n<p><img src=\"http://i.stack.imgur.com/xyzNo.png\" alt=\"enter image description here\"></p>\n\n<p>Now the question is that I wish to train a set of these 100 x 100 images using an ANN classifier.</p>\n\n<p>Should I use the <strong>pixel value of the colormap generated by imagesc</strong> (RBG colors), or should I use the <strong>values of the population density matrix</strong> (number people per region)? </p>\n\n<p><img src=\"http://i.stack.imgur.com/tc0Pn.png\" alt=\"enter image description here\"></p>\n",
    "tags": "machine learning,classification",
    "answers": [
      "<p>Using RGB images is to expensive. 100x100 matrix of data means that you must design network which contains 10000 inputs. For RGB format of data must be three times more inputs, because you use 3 parameters for every point in matrix, so you need 3x100x100 or 30000 inputs and in first layer your computations will be processed longer if you will use RGB images as input parameters.</p>\n",
      "<p>Use the raw data! </p>\n\n<p>If I understand your problem correctly, you've got a matrix of data points where the (x,y) coordinates indicate a location and the value is the population density at that location. In other words $\\textrm{data}(50,75)=100$ indicates that the area at position (50,75) has a population density of 100 people/square mile.</p>\n\n<p>You have also generated a raster plot of this data with imagesc.</p>\n\n<p>If this is true, there are several reasons to use the raw data</p>\n\n<ul>\n<li><p>The raw data is univariate, but the colormap pixels are not. By that, I mean that the (x,y) coordinates are associated with a single number (people/square mile). On the other hand, if you use the colormapped values, you'll now have three outputs (the r,g and b values) for each (x,y) coordinate. Predicting multiple outputs is typically much harder.</p></li>\n<li><p>Worse, the color map is totally arbitrary. Your response variable presumably changes smoothly with population density--the areas with 10 people/square mile should be relatively similar to those with 5 and 15. This ordering, which you get for free when using the numeric values, can be distorted by applying a color map: is (0,255,0) more like (0, 0, 255) or (255, 0, 0)? This transformation also varies depending on which color map you choose.</p></li>\n<li><p>Finally, converting to a color map may discard information. In theory, the colormap space is pretty big--there are $256^3$ possible colors. If your data points are distributed evenly across their range, you could potentially end up one color/entry. However outliers can eat into that dynamic range and you may find that a lot of the data is \"compressed\" into the middle of that range (the bluish green in your example).</p></li>\n</ul>\n\n<p>In contrast, I can't think of a single good reason to use the colormap values instead. Perhaps it would make sense if you expect to receive future data that <em>only</em> comes in this form (e.g., scans from older documents), but otherwise...it just seems odd. It makes your classification problem harder in several different ways for no real reason. </p>\n"
    ],
    "createdAt": "2025-07-12T08:02:17Z",
    "updatedAt": "2025-07-12T08:02:17Z",
    "similar_questions": [
      94519,
      55788,
      94572,
      185425,
      179329
    ]
  },
  {
    "id": 211696,
    "title": "What is the best approach to transform independent variables that have a bimodal relationship with the dependent variable?",
    "body": "<p>I am building a logistic regression model with a binary rating (High and Low) as the dependent variable and 40+ independent variables. One of the independent variable (Age) has a non-linear relationship (bimodal shape) with the dependent.</p>\n\n<p>What is the best approach / transformation to deal with this? Are splines the best method? I feel splines make the model too complicated and hard to interpret especially if interactions are also included. </p>\n",
    "tags": "regression,logistic",
    "answers": [
      "<p>IMO,Two approaches can be followed:\n1)You can visualize how the target varies with the different age buckets\n2) The most popular transformations used are log transformations or quadratic transformation. For each transformation, you can use cross-validation to check which one performs better  </p>\n"
    ],
    "createdAt": "2025-07-12T08:02:17Z",
    "updatedAt": "2025-07-12T08:02:17Z",
    "similar_questions": [
      200822,
      156680,
      57790,
      185625,
      70699
    ]
  },
  {
    "id": 211709,
    "title": "Plogit: Lasso logistic regression with Stata",
    "body": "<p>I want to execute a Lasso logistic regression with Stata. </p>\n\n<p>Why? Because...\n 1.) I have a dummy dependant variable (=> Investment success (1) and failure (0)); samples(1/0)(28/23).\n2.) I have a set of 63 possible predictors (all continuous).</p>\n\n<p>Unfortunately, I have to do it with Stata and there is only one user written program called plogit for that.</p>\n\n<p>The documentation of plogit is not very detailed and there are no examples online on how to execute it properly.</p>\n\n<p>My questions are:</p>\n\n<ul>\n<li><p>When searching via plsearch for optimal lambda, what exactly do I have specify in lambda(#)?</p></li>\n<li><p>I was not able to stick more than 15 predictors into the plogit log. regression. Is it possible to put in all 63? </p></li>\n<li><p>Why do I often get error messages like:<br>\n=> \u00e2\u0080\u009cobservation numbers out of  range\u00e2\u0080\u009d \n=> \u00e2\u0080\u009cconvergence not achieved\u00e2\u0080\u009d</p></li>\n</ul>\n\n<p>I know most of my questions aren't very detailed, so just ask if you need some further information.</p>\n\n<p>Thx in advance for any kind of information.\nTobias</p>\n",
    "tags": "regression,logistic",
    "answers": [
      "No answers available."
    ],
    "createdAt": "2025-07-12T08:02:17Z",
    "updatedAt": "2025-07-12T08:02:17Z",
    "similar_questions": [
      133552,
      156619,
      121408,
      222278,
      156633
    ]
  },
  {
    "id": 81901,
    "title": "Probability of game type lottery outcomes",
    "body": "<p>This is about a lottery type football. It consists of 14 games. 13 of them have 3 possible outcomes: home team wins, tie, or away team wins. The 14th game is about 1 of the 13 games in which one chooses which team scores first (or no score), and in what quarter. So it can be home team scores in 1q, 2q, 3q or 4q, and the same for the away team, or no score at all. So in this game there are 9 possible outcomes.</p>\n\n<p>I already calculated the probability of winning 1 game, 2 games, 3 games, etc. using the binomial distribution.</p>\n\n<p>I want to calculate the same but with an alternative way of play. You can play the lottery by choosing doubles and triples (this option is only valid for the first 13 games). Doubles allow to choose 2 of the 3 outcomes, and triples allow you to choose all three, securing that result.</p>\n\n<p>Questions. \nIf I use, let's say, 3 doubles and 1 triple: How many combinations would I be using? and, what is the probability of choosing correctly 0 games, 1 game, 2 games, etc.? (the 2 questions include choosing 1 outcome of the 14th game).</p>\n\n<p>Another part of the problem. I have the data of the past 500 lotteries (only for the first 13 games) and did a frecuency table of each outcome, calculated means and standard deviations. I want to know the probability of the outcomes based on their frecuencies. I did a normal distribution but I don't know if it's the correct way, because it doesn't add up exactly. For example, it shows a very small probability of 15 home wins, and this should be zero because there are only 13 games. And also the probabilities of each outcome don't add up to 1, it's very close, but it should be exactly 1.</p>\n\n<p>Based on the frecuencies, the most common results are 5 home wins, 5 ties, and 3 away wins. \nFinal question. How many combinations can be made with that array? Also including 1 outcome of the 14th game.</p>\n",
    "tags": "probability,distributions",
    "answers": [
      "No answers available."
    ],
    "createdAt": "2025-07-12T08:02:17Z",
    "updatedAt": "2025-07-12T08:02:17Z",
    "similar_questions": [
      109249,
      190067,
      200827,
      133441,
      209369
    ]
  },
  {
    "id": 26728,
    "title": "Kernel logistic regression",
    "body": "<p>I heard Kernel Logistic Regression is a classical combination of kernel methods and Logistic regression, but I cannot find any major reference (book, or paper) on this topic. Can you give me any suggestions? Thanks. </p>\n",
    "tags": "machine learning,logistic",
    "answers": [
      "<p>This is the only reference I know of</p>\n\n<blockquote>\n  <p>Fr\u00c3\u00b6lich, M. (2006), Non-parametric regression for binary dependent\n  variables. The Econometrics Journal, 9: 511\u00e2\u0080\u0093540. doi:\n  10.1111/j.1368-423X.2006.00196.x</p>\n</blockquote>\n",
      "<p>I've written a couple ;o)</p>\n\n<p>G. C. Cawley and N. L. C. Talbot, Efficient approximate leave-one-out cross-validation for kernel logistic regression, Machine Learning, vol, 71, no. 2-3, pp. 243--264, June 2008. </p>\n\n<p>Which gives a reasonable method for choosing kernel and regularisation parameters and an empirical evaluation</p>\n\n<p>G. C. Cawley, G. J. Janacek and N. L. C. Talbot, Generalised kernel machines, in Proceedings of the IEEE/INNS International Joint Conference on Neural Networks (IJCNN-2007), pages 1732-1737, Orlando, Florida, USA, August 12-17, 2007.</p>\n\n<p>Which basically documents a MATLAB <a href=\"http://theoval.cmp.uea.ac.uk/~gcc/projects/gkm/\">toolbox</a> for making kernel versions of generalised linear models with kernel logistic regression as one of the examples.  The library includes code for model selection (but sadly no manual yet, just some demos)</p>\n\n<p>However the earliest paper I know of that uses that particular name is\n\"Kernel logistic regression and the import vector machine\" by Zhu and Hastie, Advances in Neural Information Processing Systems (2001) (available via google scholar)</p>\n"
    ],
    "createdAt": "2025-07-12T08:02:17Z",
    "updatedAt": "2025-07-12T08:02:17Z",
    "similar_questions": [
      135442,
      94521,
      155546,
      26790,
      57850
    ]
  },
  {
    "id": 94619,
    "title": "Stata's xtlogit (fe, re) equivalent in R?",
    "body": "<p>Stata allows for fixed effects and random effects specification of the logistic regression through the <code>xtlogit fe</code> and <code>xtlogit re</code> commands accordingly. I was wondering what are the equivalent commands for these specifications in R.</p>\n\n<p>The only similar specification I am aware of is the mixed effects logistic regression </p>\n\n<pre><code>&gt; mymixedlogit &lt;- glmer(y ~ x1 + x2 +  x3 + (1 | x4), data = d, family = binomial)\n</code></pre>\n\n<p>but I am not sure whether this maps to any of the aforementioned commands.</p>\n",
    "tags": "r,logistic",
    "answers": [
      "No answers available."
    ],
    "createdAt": "2025-07-12T08:02:17Z",
    "updatedAt": "2025-07-12T08:02:17Z",
    "similar_questions": [
      94521,
      70699,
      185583,
      211677,
      104978
    ]
  },
  {
    "id": 109272,
    "title": "R Sales Modeling with Non-Replenishing Inventory",
    "body": "<p>I'm working on modeling sales for tickets for a specific event over time. The issue that I'm having is that the inventory does not replenish, so I can't have my model predicting over what I actually have available. So for example, if I have 100 tickets available 30 days out and 20 sell, the remaining pool I have to sell from is only 80 tickets, so I can't predict to sell 90 tickets the next day because the original 100 tickets aren't all available any more.</p>\n\n<p>Is there a way to take this into consideration when building a model?</p>\n",
    "tags": "r,regression",
    "answers": [
      "<p>if you have history from previous events, could you look at sell through as a comparison?\nI forecast high fashion, which is often doesn't replenish either, but I've found that sales after a couple week usually account X% of my total sales for the season, and this gives me a reasonably good expectation of what sales I'll finish with. I realize this doesn't give you a specific model, but it might help how you approach the issue.</p>\n"
    ],
    "createdAt": "2025-07-12T08:02:18Z",
    "updatedAt": "2025-07-12T08:02:18Z",
    "similar_questions": [
      133493,
      104978,
      211670,
      57807,
      222278
    ]
  },
  {
    "id": 26748,
    "title": "How to represent statistical power graphically for a given hypothesis test?",
    "body": "<p>Disclosure: I'm preparing my oral exam for statistics (so I'm tagging as homework, hope it's appropriate).</p>\n\n<p>I think that I'm failing to understand the concept of power of a test (under a graphical point of view). I'll try to explain what I have in mind:</p>\n\n<p>I have a test for which I have</p>\n\n<pre><code>H0: Mu = 52\nH1: Mu &gt; 52\nConfidence: alpha\n</code></pre>\n\n<p>And I'm asked to find the power of the test when the true mean is 50.</p>\n\n<p>What I know is that power = 1 - beta = P(rej H0 | H0 is false); and that beta = P(not rej H0 | H0 is false) aka type II error.</p>\n\n<p>Is this figure a correct representation of my reasoning?</p>\n\n<p><img src=\"http://i.stack.imgur.com/v3den.png\" alt=\"power of a test\"></p>\n\n<p>How would the picture look like if my test was something like:</p>\n\n<pre><code>H0: Mu = 50\nH1: Mu &lt; 50\nConfidence: alpha\n</code></pre>\n\n<p>And I was given the true mean = 52 for checking the power of the test? Would it be the same figure but with alpha and beta swapped?</p>\n",
    "tags": "hypothesis testing,self study",
    "answers": [
      "<p>Forget the question about power for now, delete all thoughts about\n$\\mu = 50$ from your mind, and concentrate on\ngetting the test set up correctly.</p>\n\n<p>Your figure is incorrect.  With your corrected version of the hypotheses,\nthe red curve is the density of the statistic when $H_0$ is true, and \n$H_0$ is <strong>not</strong> $\\mu \\geq 52$ as marked in your figure. Furthermore,\nthe threshold $\\bar{X}_c$ for the test is larger than $52$, not smaller,\nand the test should be rejecting $H_0$ if the statistic exceeds the\nthreshold.  Thus, the area to the <strong>right</strong> of the threshold should be \n$\\alpha$, not the area to the left as you have it.</p>\n\n<p><strong>Added</strong> in response to OP's query as to what is\nthe test illustrated by the figure.</p>\n\n<p>So,your figure (did you draw it yourself or copy it from \nsomewhere?) corresponds to a null hypothesis $H_0 : \\mu \\geq 52$\nand an alternative hypothesis $H_a : \\mu &lt; 52$. The red curve\nis the density of the statistic $\\bar{X}$ when $H_0$ is\ntrue, and the boundary \n$\\bar{X}_c$ of the decision region is determined by the \nrequirement that the area\nunder the red curve to the left of $\\bar{X}_c$ is $\\alpha$\n(or less, but let's not nitpick for now).  Your decision\nis correctly indicated: the null hypothesis is rejected\nif the test statistic $\\bar{X}$ is smaller than \n$\\bar{X}_c$, and not rejected if $\\bar{X}$ is larger than\n$\\bar{X}_c$.</p>\n\n<p>The blue curve is one of many possible distributions of\n$\\bar{X}$ when the null hypothesis is not true. Let us\nassume that it is drawn for the case of $\\mu = 50$. Then,\nthe shaded region under the blur curve is the probability\nof falsely failing to reject the null when the null is\nin fact not true (because $\\mu = 50$).  The complementary\nprobability is the power of the test in this instance.</p>\n\n<p><strong>The figure DOES NOT match the description of the test that you have\n(as of now) in the sentence beginning \"I have a test for...\"</strong> and\nthe answers that I have given above are not an answer to the question \nyou have asked in words.  But, then, your question has less than\na thousand words...</p>\n"
    ],
    "createdAt": "2025-07-12T08:02:18Z",
    "updatedAt": "2025-07-12T08:02:18Z",
    "similar_questions": [
      133441,
      94519,
      57807,
      179329,
      94521
    ]
  },
  {
    "id": 121465,
    "title": "How to check linearity in binary logistic regression with many covariates having 0 as a value",
    "body": "<p>I'm trying to check linearity in my binary logistic regression. According to my handbook (<em>Discovering Statistics Using SPSS</em>, by Andy Fields: ch.19.8.1) this should be done by adding var*log(var) to the model and check for significance. </p>\n\n<p>Many of my covariates however are binary variables which can be 0 or 1. Doing log(0) doesn't give an outcome which results in a lot of missing values when recoding the data. Now only 8% of my variables is used in the regression and I don't get the desired results. How do I do this check on my data where lots of values are 0?</p>\n",
    "tags": "regression,logistic",
    "answers": [
      "<p>This problem is no problem. You are quite correct that this method can't be applied given zeros as data points, but there is a deeper issue. </p>\n\n<p>Setting aside logistic regression, imagine that you are plotting any response on the $y$ axis against a covariate with just two distinct values on the $x$ axis. \nIn your case the values are 0 and 1, but that doesn't affect the explanation I am going to give. Any transformation that I can apply that changes those two distinct values to two distinct values (whether the same or different) will not affect the linearity of the relation on the graph. At most, I might flip the values around (exchange smaller and larger) and then my relation will change sign (positive to negative, or <em>vice versa</em>). But there's no scope to play with a non-linear version of the covariate as an alternative. Using a logit link doesn't affect this: any transformation of a binary covariate just changes coefficients given different measurements, but it can't affect goodness of fit. </p>\n\n<p>So, not being able to apply this device to binary covariates scored $(0, 1)$ is not a problem. It doesn't apply even in principle. </p>\n\n<p>In general, it certainly can't work with variables that are ever negative or zero. In general also, you need at least three distinct values of a covariate for working on a transformed scale ever to be different. </p>\n\n<p>I don't have access to the book by Field [N.B.] to check his rationale and whether he mentions any qualifications. </p>\n\n<p>N.B.: What I call \"distinct\" values are sometimes called \"unique\" values. I don't follow that usage. Although language is always shifting, the term \"unique\" is best reserved in my view for values that occur precisely once. </p>\n"
    ],
    "createdAt": "2025-07-12T08:02:18Z",
    "updatedAt": "2025-07-12T08:02:18Z",
    "similar_questions": [
      57790,
      200822,
      185625,
      185583,
      25513
    ]
  },
  {
    "id": 55718,
    "title": "PCA and the train/test split",
    "body": "<p>I have a dataset for which I have multiple sets of binary labels. For each set of labels, I train a classifier, evaluating it by cross-validation. I want to reduce dimensionality using principal component analysis (PCA). My question is:</p>\n\n<p>Is it possible to do the PCA <strong>once for the whole dataset</strong> and then use the new dataset of lower dimensionality for cross-validation as described above?\nOr do I need to do a <strong>separate PCA for every training set</strong> (which would mean doing a separate PCA for every classifier and for every cross-validation fold)?</p>\n\n<p>On one hand, the PCA does not make any use of the labels. On the other hand, it does use the test data to do the transformation, so I am afraid it could bias the results.</p>\n\n<p>I should mention that in addition to saving me some work, doing the PCA once on the whole dataset would allow me to visualize the dataset for all label sets at once. If I have a different PCA for each label set, I would need to visualize each label set separately.</p>\n",
    "tags": "machine learning,classification",
    "answers": [
      "<p><strong>For measuring the generalization error, you need to do the latter: a separate PCA for every training set</strong> (which would mean doing a separate PCA for every classifier and for every CV fold). You then apply the same transformation to the test set (i.e. you do <strong>not</strong> do a separate PCA on the test set! see also: <a href=\"http://stats.stackexchange.com/questions/142216\">Zero-centering the testing set after PCA on the training set</a>).</p>\n\n<ul>\n<li><p>You'll need to define an automatic criterium for the number of PCs to use.<br>\nAs it is just a first data reduction step before the \"actual\" classification, using a few too many PCs will likely not hurt the performance. If you have an expectation how many PCs would be good from experience, you can maybe just use that.</p></li>\n<li><p>You can also test afterwards whether redoing the PCA for every surrogate model was necessary (repeating the analysis with only one PCA model). I think the result of this test is worth reporting.</p></li>\n<li><p>I once measured the bias of not repeating the PCA, and found that with my  spectroscopic classification data, I detected only half of the generalization error rate when not redoing the PCA for every surrogate model. </p></li>\n</ul>\n\n<p>That being said, you can build an <em>additional</em> PCA model of the whole data set for descriptive (e.g. visualization) purposes. Just make sure you keep the two approaches separate from each other. </p>\n\n<hr>\n\n<blockquote>\n  <p>I am still finding it difficult to get a feeling of how an initial PCA on the whole dataset would bias the results without seeing the class labels.</p>\n</blockquote>\n\n<p>But it does see the data. And if the between-class variance is large compared to the within-class variance, between-class variance  will influence the PCA projection. Usually the PCA step is done because you need to stabilize the classification. That is, in a situation where additional cases <em>do</em> influence the model.</p>\n\n<p>If between-class variance is small, this bias won't be much, but in that case neither would PCA help for the classification: the PCA projection then cannot help emphasizing the separation between the classes.</p>\n"
    ],
    "createdAt": "2025-07-12T08:02:18Z",
    "updatedAt": "2025-07-12T08:02:18Z",
    "similar_questions": [
      880,
      222241,
      81840,
      166892,
      133552
    ]
  },
  {
    "id": 104978,
    "title": "Can AUC decrease with additional variables?",
    "body": "<p>I'm fitting a logistic regression model to predict probabilities from a set of variables. I'm comparing two such models, say <code>M1</code> and <code>M2</code>. The only difference is that <code>M2</code> includes all the variables of <code>M1</code> plus a few more variables. The idea is to see which variables are useful in predicting my dependent variable. </p>\n\n<p>I expected that AUCs should be non-decreasing with the addition of new variables. If the new variables have predictive power, they should increase the AUC, if they don't, then the AUC should be unaffected. But I find that AUC actually decreases as I add a particular set of new variables. What could be the issue here?</p>\n\n<p>I'm using <code>predict()</code> to get the predicted probabilities. Does it automatically drop all the statistically insignificant variables when calculating the predicted value? Could this be the cause of the drop in AUC? </p>\n",
    "tags": "regression,logistic",
    "answers": [
      "<p>Check if you have not missings values in the new variables. Logistic regression reject the cases with missing data, and only adjust the model for full cases. You must sure that you are comparing the discrimination in the same cohorts.</p>\n",
      "<p>The effect of uninformative features depends largely on your modeling strategy. For some approaches they are irrelevant while for others they can dramatically decrease overall performance.</p>\n\n<p>Your intuition that using more features should necessarily yield a better model is wrong.</p>\n"
    ],
    "createdAt": "2025-07-12T08:02:18Z",
    "updatedAt": "2025-07-12T08:02:18Z",
    "similar_questions": [
      57790,
      94521,
      70699,
      185583,
      200822
    ]
  },
  {
    "id": 185450,
    "title": "Compare classification performance of two heuristics",
    "body": "<p>I want to compare the classification performance of two heuristics: $h_0$ and $h_1$. I have around 700 samples for which I know the output class $c$ and the results of the two heuristics $c_{\\textit{h_0}}, c_{\\textit{h_1}}$.  That is, for each sample I have $\\text{sample}_{\\textit{id}}, c, c_{\\textit{h_0}}, c_{\\textit{h_1}}$. </p>\n\n<p>$h_0$ has around ~71% correct predictions of the output class and $h_1$ has around ~77% correct. What I want to calculate now is whether this performance difference is just by chance or if the prediction performance of $h_1$ is statistical significant better than that of $h_0$.</p>\n\n<p>I have found <a href=\"https://en.wikipedia.org/wiki/Fisher%27s_exact_test\" rel=\"nofollow\">Fisher's exact test</a>, but I don't know whether I can just put my data in there like the following matrix to calculate a p-value.</p>\n\n<p>$$\n  \\begin{matrix} \n                              &amp; h_0 &amp; h_1 \\\\\n    \\text{correct prediction} &amp; 497 &amp; 539 \\\\\n    \\text{wrong prediction}   &amp; 203 &amp; 161\n  \\end{matrix}\n$$</p>\n\n<p>I don't know if I can calculate the p-value like that or if I'm completely on the wrong track. I would really appreciate it if someone could point me in the right direction.</p>\n",
    "tags": "hypothesis testing,classification",
    "answers": [
      "<p>You have matched pairs.  That is, you (ultimately) have a correct / wrong classification for each sample.  (You will need to get this status for each sample to validly assess the results.)  As a result of this fact (<a href=\"http://stats.stackexchange.com/a/14230/7290\">among others</a>), Fisher's exact test is not appropriate for your data.  </p>\n\n<p>Instead, you should format your table so that each count indexes the status of a given matched pair.  In other words, you would enumerate the number of <em>pairs</em> for which both classifications were correct, only $h_0$ was correct, only $h_1$ was correct, and where both classifications were wrong.  In the end, the sum of the counts in the table will be half of your current table.  For example, your table might end up looking like this (totally made up):  </p>\n\n  \n\n<pre class=\"lang-r prettyprint-override\"><code>tab = as.table(matrix(c(400, 150,\n                         50, 100), nrow=2, byrow=2))\nrownames(tab) &lt;- colnames(tab) &lt;- c(\"correct\", \"wrong\")\nnames(dimnames(tab)) = c(\"h0\", \"h1\")\ntab\n#          h1\n# h0        correct wrong\n#   correct     400   150\n#   wrong        50   100\n</code></pre>\n\n<p>You can assess if the heuristics differ in predictive accuracy using McNemar's test.  (I have explained McNemar's test <a href=\"http://stats.stackexchange.com/a/89415/7290\">here</a> and <a href=\"http://stats.stackexchange.com/a/141450/7290\">here</a>.)  </p>\n\n<pre class=\"lang-r prettyprint-override\"><code>mcnemar.test(tab)\n#  McNemar's Chi-squared test with continuity correction\n# \n# data:  tab\n# McNemar's chi-squared = 49.005, df = 1, p-value = 2.553e-12\n</code></pre>\n\n<hr>\n\n<p>Let me note that there may be a better way to evaluate these heuristics.  Many models (e.g., logistic regression) will output a continuous value, which people then dichotomize to generate the predicted classifications.  If your case is like this, you would do better to compare the outputted continuous values, rather than the classifications.  Two possibilities would be to compare the area under the Receiver Operating Characteristic (ROC) curve (see <a href=\"http://stats.stackexchange.com/q/68893/7290\">here</a> and <a href=\"http://stats.stackexchange.com/a/132832/7290\">here</a>, e.g.), or to compare a proper score (such as the <a href=\"https://en.wikipedia.org/wiki/Brier_score\" rel=\"nofollow\">Brier score</a>).  </p>\n"
    ],
    "createdAt": "2025-07-12T08:02:18Z",
    "updatedAt": "2025-07-12T08:02:18Z",
    "similar_questions": [
      133441,
      185624,
      77543,
      94642,
      25316
    ]
  },
  {
    "id": 185425,
    "title": "How to determine the critical values of ACF?",
    "body": "<p>I have a sample of 1000 data points and I used it as the training sample to forecast with Timeseries. My lecture suggested me comparing the ACF with its critical values (upper and lower) numerically rather than looking at the graph. </p>\n\n<p>Here are my ACF values: </p>\n\n<p><a href=\"http://i.stack.imgur.com/RJiYb.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/RJiYb.png\" alt=\"enter image description here\"></a></p>\n\n<p><strong>Question:</strong> How do I come up with the upper and the lower critical values for the ACF? Is there any function in R to yield these values? </p>\n",
    "tags": "r,time series",
    "answers": [
      "<p>Based on <a href=\"http://sfb649.wiwi.hu-berlin.de/fedc_homepage/xplore/tutorials/xegbohtmlnode39.html\" rel=\"nofollow\">this</a> source, it looks like under the null the autocorrelation is asymptoticaly standard normal. The 5% critical values of the autocorrelation at any given lag $d$ ($d \\neq 0$) are </p>\n\n<p>$$\\pm \\frac{1.96}{\\sqrt{T-d}}$$</p>\n\n<p>where $T$ is the sample size. </p>\n\n<p>In your case, $T=1000$, so the critical values for lag 1 are $\\pm \\frac{1.96}{\\sqrt{1000-1}} \\approx 0.06201$, for lag 2 are $\\pm \\frac{1.96}{\\sqrt{1000-2}} \\approx 0.06204$, and so on.</p>\n\n<p>Mind also a note from <a href=\"http://sccn.ucsd.edu/wiki/Chapter_3.6._Model_Validation\" rel=\"nofollow\">another</a> source: </p>\n\n<blockquote>\n  <p>Additionally, in small sample conditions ... this test may be overly conservative such that the null hypothesis is rejected (residuals indicated as non-white) less often than indicated by the chosen significance level (Lutkepohl, 2006).</p>\n</blockquote>\n\n<p>However, it is not likely to be relevant for a sample as large as 1000.</p>\n",
      "<p>Since the standard deviation of the acf is approximately = 1/SQRT(NOB) it is so approximate that it is practically useless for large sample sizes . If your \"reason\" for obtaining critical values is to automatically identify the form of the ARIMA model , you can stop right now ! . Identification of a reasonable starting model for the ARIMA structure is better conducted via approaches like the Inverse Autocorrelation Function <a href=\"http://www.jstor.org/stable/2982488?seq=1#page_scan_tab_contents\" rel=\"nofollow\">http://www.jstor.org/stable/2982488?seq=1#page_scan_tab_contents</a> which is the basis of how AUTOBOX (a piece of software that I have helped develop) effectively solves the riddle.</p>\n",
      "<p>According to <a href=\"http://www.ltrr.arizona.edu/~dmeko/notes_3.pdf\" rel=\"nofollow\">this source</a>, there are two kinds of critical value for ACF. 2/SQRT(N), where N is the sample size, is a simple approximate confidence interval to judge whether the series is significantly random under the null hypothesis. But, if you want to determine cut-off or not, you can use large-lag standard error that calculate the standard error of ACF at the shorter lags. For more details, please visit the link I provide above.</p>\n"
    ],
    "createdAt": "2025-07-12T08:02:18Z",
    "updatedAt": "2025-07-12T08:02:18Z",
    "similar_questions": [
      55788,
      179329,
      94519,
      94572,
      133493
    ]
  },
  {
    "id": 166894,
    "title": "How to build \"supervised clustering\" for neural networks?",
    "body": "<p>I'm confused as to what the output would be. </p>\n\n<p>Consider the \"blind source separation\" problem. Let's say I have a ton of training examples where the input is the final cacophony of sounds as a sound file, and the \"answer\" is the multiple individual audio files which comprised the final sound file. Maybe some of these are labeled, like \"piano\" or \"man's voice\", but others might be unlabeled, like a weird machine or animal. The system should learn what the labeled ones sound like (so it should identify \"piano\" and \"man's voice\" in future test data), but it should also be able to separate unknowns. But order doesn't matter on the unknowns -- for example if there is an alien sound and a tree falling sound, it doesn't matter if it puts alien sound as \"unknown #1\" and tree falling as \"unknown #2\" or vice versa. </p>\n\n<p>Traditionally, if all source files were labeled, then I could maybe have n*k output neurons where n is the number of frequency bins needed to make a good-enough sound file, and k is the number of all possible sources in the whole world (piano, man's voice). Then when it's being fed the input data, it could be an RNN which fires 0 or more output neurons at each time step (e.g. 1024 per second), to recreate the source sounds.</p>\n\n<p>But with the \"unknowns\", this turns into more like \"sort of classification sort of clustering\" problem and I am completely in the dark as to how to use the output neurons. For example if I reserve n*5 output neurons for \"unknowns\", how would it learn that it's supposed to put unknowns in there, but the order doesn't matter? How would it avoid erroneously \"learning\" that an \"unknown\" is supposed to sound like whatever the training data happened to tell it? </p>\n",
    "tags": "machine learning,classification",
    "answers": [
      "No answers available."
    ],
    "createdAt": "2025-07-12T08:02:19Z",
    "updatedAt": "2025-07-12T08:02:19Z",
    "similar_questions": [
      173,
      190106,
      135555,
      109249,
      222278
    ]
  },
  {
    "id": 185583,
    "title": "How to read the Interaction effect in multiple linear regression with continuous regressors?",
    "body": "<p>If the interaction happens between a <code>continuous</code> and a <code>discrete</code> variable it is (if I'm not mistaken) relatively straightforward. The mathematical expression is:</p>\n\n<p>$\\hat Y=\\hat\u00ce\u00b2_0+\\hat\u00ce\u00b2_1X_1+\\hat\u00ce\u00b2_2X_2+\\hat\u00ce\u00b2_3X_1\u00e2\u0088\u0097X_2+\\epsilon$</p>\n\n<p>So if we take my favorite dataset <code>mtcars{datasets}</code> in R, and we carry out the following regression:</p>\n\n<pre><code>(fit &lt;- lm(mpg ~ wt * am, mtcars))\n\nCall:\nlm(formula = mpg ~ wt * am, data = mtcars)\n\nCoefficients:\n(Intercept)           wt           am        wt:am  \n     31.416       -3.786       14.878       -5.298  \n</code></pre>\n\n<p><code>am</code>, which dummy-codes for the type of transmission in the car <code>am Transmission (0 = automatic, 1 = manual)</code> will give us an intercept of <code>31.416</code> for <code>manual</code> (<code>0</code>), and <code>31.416 + 14.878 = 46.294</code> for <code>automatic</code> (<code>1</code>). The slope for weight is <code>-3.786</code>. And for the interaction, when <code>am</code> is <code>1</code> (automatic), the regression expression will have the added term, $-5.298*1*\\text {weight}$, which will add to $-3.786*\\text {weight}$, resulting in a slope of $-9.084*\\text {weight}$. So <strong><em>we are changing the slope with the interaction.</em></strong></p>\n\n<p>But when it is two <code>continuous</code> variables that are interacting, <strong><em>are we really creating an infinite number of slopes? How do express the output without corny  sentences like \"the slope we would get with cars that weight $0\\,\\text{lbs.}$, or $1\\,\\text{lb.}$?</em></strong> For example, take the explanatory variables <code>wt</code> (weight) and <code>hp</code> (horsepower) and the regressand <code>mpg</code> (miles per gallon):</p>\n\n<pre><code>(fit &lt;- lm(mpg ~ wt * hp, mtcars))\n\nCall:\nlm(formula = mpg ~ wt * hp, data = mtcars)\n\nCoefficients:\n(Intercept)           wt           hp        wt:hp  \n   49.80842     -8.21662     -0.12010      0.02785\n</code></pre>\n\n<p><strong><em>How do we read the output?</em></strong> There seems to be one single intercept <code>49.80842</code>, whereas it would make sense to have two different intercepts to give flexibility to the fit, as in the prior scenario (<strong><em>what am I missing?</em></strong>). We have a slope for <code>wt</code> and a slope for hp (<code>-8.21662 -0.12010 = -8.33672</code>, <strong><em>is that right?</em></strong>). And finally the more intriguing <code>0.02785</code>. So, yes, are we constrained to expressing this with absurd scenarios, such as <strong><em>if we had cars with $1\\text{hp}$ we would have a modified slope for the weight equal to $(-8.21662 + 0.02785)*1*\\text{weight}$?</em></strong> Or is there a more sensible way to look at this term?</p>\n\n<p><strong>SOLUTION:</strong></p>\n\n<p>[Quick note, safe to skip: I really appreciate the answers and help provided, and will accept - it is rather difficult with such outstanding Answers, though. So please don't take this edit as anything more than a way of sharing what I've been doing for a little while this morning: basically hacking away at the R coefficients until I got what I wanted because despite the generous help provided I still couldn't \"see\" how one of the coeff's worked. Also, all this pre-emption will be erased shortly.]</p>\n\n<p>We can \"prove\" how these coefficients \"work\" by simply taking the first values of <code>mpg</code>, <code>wt</code> and <code>hp</code>, which happen to be for the glamorous Mazda RX4:</p>\n\n<p><img src=\"http://i.stack.imgur.com/Gunpa.jpg\" width=\"300\" height=\"150\"></p>\n\n<p><a href=\"http://www.classicandperformancecar.com\" rel=\"nofollow\">credit here</a></p>\n\n<p>These are:</p>\n\n<pre><code>          mpg cyl disp  hp drat   wt  qsec vs am gear carb\nMazda RX4  21   6  160 110  3.9 2.62 16.46  0  1    4    4\n</code></pre>\n\n<p>And simply run <code>predict(fit)[1] Mazda RX4</code>, which returns a $\\hat y$ value of  $23.09547$. No matter what, I have to rearrange the coefficient to get this number - all possible permutations if necessary! No just kidding. Here it is:</p>\n\n<p><code>coef(fit)[1] + (coef(fit)[2] * mtcars$wt[1]) + (coef(fit)[3] * mtcars$hp[1]) \n+ (coef(fit)[4] * mtcars$wt[1] * mtcars$hp[1])</code> $= 23.09547$. </p>\n\n<p>The math expression is:</p>\n\n<p>$\\small \\hat Y=\\hat \u00ce\u00b2_0 (=1^{st}\\,\\text{coef})\\,+\\,\\hat\u00ce\u00b2_1 (=2^{nd}\\,\\text{coef})\\,*wt \\,+\\, \\hat\u00ce\u00b2_2 (=3^{rd}\\,\\text{coef})\\,*hp \\,+\\, [\\hat\u00ce\u00b2_3(=4^{th}\\,\\text{coef})\\, *wt\\,\u00e2\u0088\u0097\\,hp]$</p>\n\n<p>So, as pointed out in the answers, there is only one intercept (the first coefficient), but there are two <em>\"private\"</em> slopes: one for each explanatory variable, <em>plus</em> one <em>\"shared\"</em> slope. This shared slope allows obtaining uncountably infinite slopes if we \"zip\" through $\\mathbb{R}$ for all the theoretically possible realizations of one of the variables, and at any point we combine ($+$) the <em>\"shared\"</em> coefficient <em>times</em> the remaining random variable (e.g. for <code>hp = 100</code>, it would be <code>0.02785 * 100 * wt</code>) with its <em>\"private</em>\" slope (<code>-8.21662 * wt</code>). I wonder if I can call it a <em>convolution</em>...</p>\n\n<p>We can also see that this is the right interpretation running: </p>\n\n<pre><code>y &lt;- coef(fit)[1] + (coef(fit)[2] * mtcars$wt[1]) + (coef(fit)[3] * mtcars$hp[1]) + (coef(fit)[4] * mtcars$wt[1] * mtcars$hp[1])\nidentical(as.numeric(predict(fit)[1]), as.numeric(y)) TRUE\n</code></pre>\n\n<p>Having rediscovered the wheel we see that the \"shared\" coefficient is positive (0.02785), leaving one loose end, now, which is the explanation as to why the weight of the vehicle as a predictor for \"gas-guzzliness\" is buffered for higher horse-powered cars... We can see this effect (thank you @Glen_b for the tip) with the $3\\,D$ plot of the predicted values in this regression model, which conforms to the following <em>parabolic hyperboloid</em>:</p>\n\n<p><a href=\"http://i.stack.imgur.com/VeVpK.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/VeVpK.png\" alt=\"enter image description here\"></a></p>\n",
    "tags": "r,regression",
    "answers": [
      "<p>The easiest way is to have a look at different quantiles of one of your variables of interest, depending on your research question. Assuming that you want to know the effect of one additional unit of horse power on the miles per gallon a car uses, you look at the distribution of the weight and use e.g. the percentiles 1, 10, 25, 50, 75, 90 and 99. For each of these percentiles you compute the effect of hp on mpg.</p>\n\n<p>Concerning interaction terms and parent variables (variables which are interacted) Chipman put forward the Heredity principle (<a href=\"http://arxiv.org/abs/bayes-an/9510001\" rel=\"nofollow\">http://arxiv.org/abs/bayes-an/9510001</a>). Basically he differentiates between strong and weak heredity, depending on how much parent variables are included in the regression. From a statistical point of view it would also be correct to just add the interaction term, without the variables themselves. But this makes the results hard (/impossible) to interpret.</p>\n",
      "<blockquote>\n  <p>There seems to be one single intercept 49.80842, whereas it would make sense to have two different intercepts </p>\n</blockquote>\n\n<p>No, it usually wouldn't make sense to have <em>two</em> intercepts; that only makes sense when you have a factor with two levels (and even then only if you regard the relationship holding factor levels constant).</p>\n\n<p>The population intercept, strictly speaking, is $E(Y)$ for the population model when all the predictors are 0, and the estimate of it is whatever our fitted value is when all the predictors are zero.</p>\n\n<p>In that sense - whether we have factor variables or numerical variables - there's only one intercept for the whole equation.</p>\n\n<p><em>Unless, that is, you're considering different parts of the model as separate equations</em>.</p>\n\n<p>Imagine that we had one factor with three levels, and one continuous variable - for now without the interaction:</p>\n\n<p><a href=\"http://i.stack.imgur.com/egpxU.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/egpxU.png\" alt=\"enter image description here\"></a></p>\n\n<p>For the equation as a whole, there's one intercept, but if you think of it as a different relationship within each subgroup (level of the factor), there's three, one for each level of the factor ($a$) -- by considering a specific value of $a$, we get a specific straight line that is shifted by the effect of $a$, giving a different intercept for each group.</p>\n\n<p>But now let's consider the relationship with $a$. Now for each level of $a$, if $x$ had no impact, there'd be a very simple relationship $E(Y|a=j)=\\mu_j$. There's one intercept, the baseline mean  (or if you conceive it that way, three, one for each subgroup  -- where the intercept would be the average value in that subgroup).</p>\n\n<p><a href=\"http://i.stack.imgur.com/lUW3d.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/lUW3d.png\" alt=\"enter image description here\"></a><br>\n(nb It may be hard to see here but the means are not equally spaced; don't be tempted by this plot to think of $y$ as linear in $a$ considered as a numeric variable.)</p>\n\n<p>But now if we consider $x$ does have an impact and look at the relationship at a <em>specific</em> value of $x$ ($x=x_0$), as a function of $a$, $E(Y|a=j)=\\mu_j(x_0)$ -- each group has a different mean, but those means are shifted by the effect of $x$ at $x_0$. </p>\n\n<p><a href=\"http://i.stack.imgur.com/LAbF3.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/LAbF3.png\" alt=\"enter image description here\"></a></p>\n\n<p>So that would be one intercept (the black dot if it's the baseline group) ... <em>at each value of</em> $x$.</p>\n\n<p>For each of infinite number of different values that $x$ might take, there's a new intercept.</p>\n\n<p>So depending on how we look at it, there's one intercept, or three, or an infinite number... but not two.</p>\n\n<p>Now if we introduce an $x:a$ interaction, nothing changes but the slopes! We still can conceive of this as having one intercept, or perhaps three, or perhaps an infinite number.</p>\n\n<hr>\n\n<p>So how does this all relate to two numeric variables?</p>\n\n<p>Even though we didn't have it in this case, imagine that the levels of $a$ were numeric and that the fitted model was linear in $a$ (perhaps $a$ is discrete, like the number of phones owned collectively by a household). [i.e. we're now doing what I said earlier not to do, taking $a$ to be numeric and (conditionally) linearly related to $y$]</p>\n\n<p>Then we'd still have one intercept in the strict sense, the value taken by the model when $x=a=0$ (even though neither variable is 0 in our sample), or one for each possible value taken by $a$ (in our sample, three different values occurred, but maybe 0, 4, 5 ... are also possible), or one for each value taken by $x$ (an infinity of possible values since $x$ is discrete). It doesn't matter if our model has an interaction, it doesn't change that consideration about how we count intercepts.</p>\n\n<hr>\n\n<p>So how do we interpret the interaction term when both variables are numeric? </p>\n\n<p>You can consider it as providing for a different <em>slope</em> in the relationship between $y$ and $x$, at each $a$ (three different slopes in all, one for the baseline and two more via interaction), or you can consider it as providing for a different slope between $y$ and (the now-numeric) $a$ <em>at each value of</em> $x$.</p>\n\n<p>Now if we replace this now numeric but discrete $a$ with a continuous variate, you'd have an infinite number of slopes for both one-on-one relationships, one at each value of the third variable.</p>\n\n<p>You effectively say as much in your question of course.</p>\n\n<blockquote>\n  <p>are we constrained to expressing this with absurd scenarios, such as if we had cars with 1hp we would have a modified slope for the weight equal to (\u00e2\u0088\u00928.21662+0.02785)\u00e2\u0088\u00971\u00e2\u0088\u0097weight? Or is there a more sensible way to look at this term?</p>\n</blockquote>\n\n<p>Sure there is, consider values more like the mean. So for a typical relationship between mpg and wt, hold horsepower at some value near the mean. To see how much the slope changes, consider two values of horsepower, one below the mean and one above it.</p>\n\n<p>Where the variable-values aren't especially meaningful in themselves (like score on some Likert-scale-based instrument say) you might go up or down by a standard deviation on the third variable, or pick the lower and upper quartile. </p>\n\n<p>Where they are meaningful (like hp) you can pick two more or less typical values (100 and 200 seem like sensible choices for hp for the mtcars data, and if you also want to look at something near the mean, 150 will serve quite well, but you might choose a typical value for a particular <em>kind</em> of car for each choice instead)</p>\n\n<p>So you could draw a fitted mpg-vs-wt line for a 100hp car and a 150hp car and a 200 hp car. You could <em>also</em> draw a mpg-vs-hp line for a car that weighs 2.0 (that's 2.0 thousand-pounds) and 4.0 or (or 2.5 &amp; 3.5 if you want something nearer to quartiles).</p>\n",
      "<p>Output</p>\n\n<pre><code>Coefficients:\n(Intercept)           wt           hp        wt:hp  \n   49.80842     -8.21662     -0.12010      0.02785  \n</code></pre>\n\n<blockquote>\n  <p>How do we read this output? ... We have a slope for wt and a slope for hp (-8.21662 -0.12010 = -8.33672, is that right?). </p>\n</blockquote>\n\n<p>Nope. Some calculus should confirm that the derivative of <code>mpg</code> with respect to <code>wt</code>, which is the only kind of slope you should be interested in, is  </p>\n\n<pre><code>-8.21662 + 0.02785 x hp\n</code></pre>\n\n<p>That whole expression is the expected increase in <code>mpg</code> from increasing <code>wt</code> by one.  Similarly the derivative of <code>mpg</code> with respect to <code>hp</code> is</p>\n\n<pre><code>-8.21662 + 0.02785 x wt\n</code></pre>\n\n<p>because interactions are symmetric.</p>\n\n<blockquote>\n  <p>So, yes, are we constrained to expressing this with absurd scenarios, such as if we had cars with 1hp we would have a modified slope for the weight equal to (\u00e2\u0088\u00928.21662+0.02785) * 1 * weight? Or is there a more sensible way to look at this term?</p>\n</blockquote>\n\n<p>I'm not sure what you mean here.  If you're worried about talking about cars with zero <code>hp</code> or <code>wt</code> or whatnot, then it's probably just easier to redefine those variables to have a zero that's more meaningful.  </p>\n\n<p>For example, subtract the Mazda RX4 from each row. Now you have a rotary-engined early seventies zero point to compare to.  That is, you are no longer considering cars with an <code>hp</code> of one, but rather cars an <code>hp</code> one higher than that of a Mazda RX4.</p>\n"
    ],
    "createdAt": "2025-07-12T08:02:19Z",
    "updatedAt": "2025-07-12T08:02:19Z",
    "similar_questions": [
      94521,
      70699,
      156619,
      94519,
      104978
    ]
  },
  {
    "id": 44091,
    "title": "Enforcing sparsity on probability",
    "body": "<p>I am trying to induce a probability distribution $Q$ by optimizing an objective function and am wondering how can one encourage sparsity for $Q$ while keeping the optimization convex.</p>\n\n<p>In particular, there $m$ distributions: $q_i$ for $i=1... m$, where each $q_i$ lies, say, in a 10-dimensional simplex. Given certain observations, we learn $Q=\\{q_1,...,q_m\\}$ as</p>\n\n<p>$F(Q) =  \\min_{q_i, i=1...m} \\sum_i f(q_i)$ </p>\n\n<p>where $f$ is convex. This is all well and good. Now, we also want to encourage sparsity for $q_i$ i.e. we want most of the probabilities within $q_i$ to be zero. </p>\n\n<p>I am wondering if it is possible to device a regularization for $q_i$ to achieve this while keeping the optimization convex.</p>\n",
    "tags": "machine learning,distributions",
    "answers": [
      "No answers available."
    ],
    "createdAt": "2025-07-12T08:02:19Z",
    "updatedAt": "2025-07-12T08:02:19Z",
    "similar_questions": [
      209369,
      81840,
      411,
      156703,
      166892
    ]
  },
  {
    "id": 57790,
    "title": "spss: working with two binary/dummy variables",
    "body": "<p>Am trying to set a few binary/dummy variables against each other, i.e. <code>propensity_to_dance</code> and <code>gender</code>.</p>\n\n<p>I assume that it' ok to be using a binary logistic regression in SPSS (or otherwise) to investigate the interaction on these two variables; I've previously been using that kind of regression to investigate the relationship between binary and linear variables. Is it correct to use this methodology for two binary variables as in the above?</p>\n",
    "tags": "regression,logistic",
    "answers": [
      "<p>First, why is \"propensity to dance\" binary? That seems like a mistake. I think it would vary along from people with no propensity to dance (e.g. me) to those who will dance at every opportunity, or even make opportunities). </p>\n\n<p>But, if it has to be binary, then .... logistic regression is OK here; like other forms of regression, it assumes that there is a dependent and an independent variables. Here, I would assume that propensity is the DV and gender the IV; although I can imagine situations in which they would be reversed!</p>\n\n<p>If you just want to look at association between two binary variables, chi-square can be a good method. </p>\n"
    ],
    "createdAt": "2025-07-12T08:02:19Z",
    "updatedAt": "2025-07-12T08:02:19Z",
    "similar_questions": [
      104978,
      94521,
      185625,
      70699,
      121465
    ]
  },
  {
    "id": 185624,
    "title": "Feature scaling (normalization) in multiple regression analysis with normal equation method?",
    "body": "<p>I am doing <a href=\"https://en.wikipedia.org/wiki/Linear_regression\" rel=\"nofollow\"><strong>linear regression</strong></a> with multiple features/variables. I decided to use <strong>normal equation</strong> method to find coefficients of linear model. If we use <a href=\"https://en.wikipedia.org/wiki/Gradient_descent\" rel=\"nofollow\"><strong>gradient descent</strong></a> for linear regression with multiple variables we typically do <a href=\"https://en.wikipedia.org/wiki/Feature_scaling\" rel=\"nofollow\"><strong>feature scaling</strong></a> in order to quicken gradient descent convergence. For now, I am going to use normal equation method with formula:</p>\n\n<blockquote>\n  <p>$$\\hat{\\beta} = (X^TX)^{-1}X^Ty = X^+y$$\n  <em>Source: <a href=\"http://cs229.stanford.edu/notes/cs229-notes1.pdf\" rel=\"nofollow\">The normal equations (Andrew Ng lecture notes, p. 11)</a></em></p>\n</blockquote>\n\n<p>I have two contradictory information sources. In first it is stated that no feature scaling required for normal equations. In another I can see that feature normalization has to be done.</p>\n\n<p>Sources:</p>\n\n<blockquote>\n  <ol>\n  <li><a href=\"http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&amp;doc=exercises/ex3/ex3.html\" rel=\"nofollow\">http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&amp;doc=exercises/ex3/ex3.html</a></li>\n  <li><a href=\"http://puriney.github.io/numb/2013/07/06/normal-equations-gradient-descent-and-linear-regression/\" rel=\"nofollow\">http://puriney.github.io/numb/2013/07/06/normal-equations-gradient-descent-and-linear-regression/</a></li>\n  </ol>\n</blockquote>\n\n<p>At the end of these two articles information concerning feature scaling in normal equations presented.</p>\n\n<hr>\n\n<p><strong>The question is:</strong> do we need to do feature scaling (normalization) before normal equation analysis?</p>\n",
    "tags": "regression,machine learning",
    "answers": [
      "<p>Well, in the second article there is a sentence: </p>\n\n<blockquote>\n  <p>Note that before conducting linear regression, you should normalize\n  the data. One way is $\\frac{x_i\u00e2\u0088\u0092mean(x)}{Range(x)}$, and some use\n  $sd(x)$ as the denominator. Both work.</p>\n</blockquote>\n\n<p>But is not said that it applies specifically to normal equations. And in gradient descent section there is nothing said about normalization. So I suppose it was a small mistake to include that sentence in Normal Equation section instead of Gradient Descent.</p>\n\n<p>Anyway Andrew Ng is pretty authoritative on machine learning topic, so you can rely on his words:</p>\n\n<blockquote>\n  <p>Using this formula does not require any feature scaling, and you will\n  get an exact solution in one calculation: there is no 'loop until\n  convergence' like in gradient descent.</p>\n</blockquote>\n"
    ],
    "createdAt": "2025-07-12T08:02:19Z",
    "updatedAt": "2025-07-12T08:02:19Z",
    "similar_questions": [
      156619,
      880,
      179336,
      156633,
      185425
    ]
  },
  {
    "id": 156703,
    "title": "Estimating probability of observing greater than X events based on a current population and historical rates",
    "body": "<p>Let's say I have a population that varies from month to month, and per month, there are X number of failures. Based on historical rates, I am trying to find the probability of observing Y or greater failures for the population of the current month, based on historical rates. </p>\n\n<p>For example, we have the following data:   </p>\n\n<pre><code>Month   Failures    Population  \n1       20          11969\n2       14          15607\n3       37          19900\n4       17          25038\n5       25          29992\n6       30          35645\n</code></pre>\n\n<p>I am trying to figure out the probability of observing 30 failures out of 35,645 items, given the previous months' observations. </p>\n\n<p>I have several questions about distributions: </p>\n\n<p>To find the distribution of my data, do I look at the number of failures? or failures per population unit?  Plotting failures, it seems to follow a negative binomial distribution. Plotting failures per population unit, it seems to follow a lognormal/gamma/Weibull distribution. (This was found using the <code>fitdist</code> function in R.)</p>\n\n<p>What I've tried already is averaging the rate (failures divided by population) over the previous months, and then using that value times the current population to get the lambda value for a Poisson probability calculation. Then I used the ppois function in R to calculate the poisson probability of observing 30 failures in month 6, based on the calculated lambda. However, the more I look at it, the more I'm starting to think I can't use poisson because of the changing population over time, and because my mean and variance are very different. </p>\n\n<p>I believe my data is overdispersed, so I looked into gamma-poisson mixture, but am not sure how to algorithmically implement it, nor if it's the right approach. I read up on negative binomial distribution and how it arises from a continuous mixture of Poisson distributions, but am unsure if this is the right fit for my data as well.</p>\n\n<p>What is the best course of action to find the probability of observing 30 failures in month 6, based on the observations in months 1-5? I have about 40 total months of data. How do I find the best distribution to fit my data to? </p>\n",
    "tags": "time series,probability",
    "answers": [
      "No answers available."
    ],
    "createdAt": "2025-07-12T08:02:19Z",
    "updatedAt": "2025-07-12T08:02:19Z",
    "similar_questions": [
      133441,
      133493,
      94521,
      200827,
      94581
    ]
  },
  {
    "id": 26762,
    "title": "How to do logistic regression in R when outcome is fractional?",
    "body": "<p>I'm reviewing a paper which has the following biological experiment. A device is used to expose cells to varying amounts of fluid shear stress. As greater shear stress is applied to the cells, more of them start to detach from the substrate. At each level of shear stress, they count the cells that remain attached, and since they know the total number of cells that were attached at the beginning, they can calculate a fractional attachment (or detachment).</p>\n\n<p>If you plot the adherent fraction vs. shear stress, the result is a logistic curve. In theory, each individual cell is a single observation, but obviously there are thousands or tens of thousand of cells, so the data set would be gigantic, if it was set up in the usual way (with each row being an observation).</p>\n\n<p>So, naturally, my question (as stated in the title) should make sense now. How do we do a logistic regression using the fractional outcome as the D.V.? Is there some automatic transform that can be done in glm?</p>\n\n<p>Along the same lines, if there were potentially 3 or more (fractional) measurements, how would one do this for a multinomial logistic regression?</p>\n",
    "tags": "r,logistic",
    "answers": [
      "<p>As a start, if you have a dependent variable that is a proportion, you can use Beta Regression. This doesn't extend (with my limited knowledge) to multiple proportions.</p>\n\n<p>For Beta Regression overview and an R implementation check out <a href=\"https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0CCoQFjAA&amp;url=http://cran.r-project.org/web/packages/betareg/vignettes/betareg.pdf&amp;ei=8iaQT5u1BondggfymuXiBA&amp;usg=AFQjCNGcZvouO6by9GeLT3XGkLLfEMQNlg\" rel=\"nofollow\">betareg</a>.</p>\n",
      "<p>I'v been using <code>nnet::multinom</code> (package nnet is part of MASS) for a similar purpose, it accepts continuous input in [0, 1]. </p>\n\n<p>If you need a reference: <a href=\"http://www.springerlink.com/content/5020361378260270/?MUD=MP\" rel=\"nofollow\">C. Beleites et.al.:\nRaman spectroscopic grading of astrocytoma tissues: using soft reference information.\nAnal Bioanal Chem, 2011, Vol. 400(9), pp. 2801-2816</a></p>\n",
      "<p>The <code>glm</code> function in <code>R</code> allows 3 ways to specify the formula for a logistic regression model.  </p>\n\n<p>The most common is that each row of the data frame represents a single observation and the response variable is either 0 or 1 (or a factor with 2 levels, or other varibale with only 2 unique values).</p>\n\n<p>Another option is to use a 2 column matrix as the response variable with the first column being the counts of 'successes' and the second column being the counts of 'failures'.</p>\n\n<p>You can also specify the response as a proportion between 0 and 1, then specify another column as the 'weight' that gives the total number that the proportion is from (so a response of 0.3 and a weight of 10 is the same as 3 'successes' and 7 'failures').</p>\n\n<p>Either of the last 2 ways would fit what you are trying to do, the last seems the most direct for how you describe your data. </p>\n"
    ],
    "createdAt": "2025-07-12T08:02:19Z",
    "updatedAt": "2025-07-12T08:02:19Z",
    "similar_questions": [
      26728,
      135442,
      200827,
      94581,
      156633
    ]
  },
  {
    "id": 26790,
    "title": "Ensembling regression models",
    "body": "<p>I'm working on a securities pricing project and have a bunch of models I'd like to stack/ensemble together.  I've been using simple linear regression in R (the <code>lm()</code> function) so far but the results are over fitting pretty badly.  </p>\n\n<p>Does anyone have any suggestions for whether some other stacking method might be better or any papers/articles that describe how to stack regression models (as opposed to classification models).</p>\n",
    "tags": "r,regression,machine learning",
    "answers": [
      "<p>If you are experiencing over fitting you could look into <a href=\"http://en.wikipedia.org/wiki/Regularization_%28mathematics%29\">regularized regression</a> which in R can be fit using many packages such as (<a href=\"http://cran.r-project.org/web/packages/glmnet/index.html\">glmnet</a>). There are many good tutorials for this - one is <a href=\"http://www.jstatsoft.org/v33/i01/paper\">Regularization Paths for Generalized Linear Models\nvia Coordinate Descent</a> </p>\n\n<p>You might also look at <code>randomForest</code> or <code>gbm</code> in R depending on your data.</p>\n\n<p>You can try fitting many models and averaging their prediction as well (your reference to ensembles).</p>\n"
    ],
    "createdAt": "2025-07-12T08:02:20Z",
    "updatedAt": "2025-07-12T08:02:20Z",
    "similar_questions": [
      179336,
      200822,
      94521,
      77851,
      70699
    ]
  },
  {
    "id": 57801,
    "title": "Standardized generalized hyperbolic distribution",
    "body": "<p>I am interested in the standardized version (mean zero, variance one) of the generalized hyperbolic and the hyperbolic distribution. I want to include this in my analysis and therefore I need the dervations and the implementations. In R, there is an <a href=\"http://help.rmetrics.org/fBasics/dist-sgh.html\" rel=\"nofollow\">implementation of the standardized generalized hyperbolic distribution</a>. </p>\n\n<ol>\n<li><p>I need the used formula and how to derive it for this implementation, where can I get it?</p></li>\n<li><p>Where can I see, what the code is doing? So where can I get a 'look behind the scenes' to see what is implemented?</p></li>\n<li><p>I searched via google but I could not find any derviations of how to get a standardized version of the ghyp or hyp. Where can I find this?</p></li>\n<li><p>I could use the <a href=\"http://help.rmetrics.org/fBasics/dist-sgh.html\" rel=\"nofollow\">R command</a> to also fit a standardized hyperbolic distribution if I do the following: Inser for lambda=1 (this gives hyperbolic) and then use the <code>dsgh</code> command along together with the <code>optim</code> command to get a good fit of the standardized hyperbolic distribution to my data, is this correct?</p></li>\n</ol>\n\n<p>edit: OK, the answer to 2) is, that I just can load the package and enter the command without any specifications. I then get the code behind it.</p>\n\n<p>edit: I also noticed the sghFIT command, where can I find the theory behind this? Because they have to use a standardized version, so they need the formula and the derivation, this is what I need. </p>\n\n<p>edit: I just saw, that they also only use the dsgh command for fitting.</p>\n\n<p>Ok, my last edit, all comes back to the implementation of the dsgh command, the code for this command is:</p>\n\n<pre><code>dsgh &lt;-  \nfunction(x, zeta = 1, rho = 0, lambda = 1, log = FALSE) \n{\n    # A function implemented by Diethelm Wuertz\n\n    # Description:\n    #   Returns density of the sgh distribution\n\n    # FUNCTION:    \n\n    # Parameters:\n    if (length(zeta) == 3) {\n       lambda = zeta[3]\n       rho = zeta[2]\n       zeta = zeta[1]\n    } \n\n    # Compute Density:\n    param = .paramGH(zeta, rho, lambda)\n    ans = dgh(x, param[1], param[2], param[3], param[4], lambda, log)\n\n    # Return Value:\n    ans\n}\n</code></pre>\n\n<p>He computes the density with</p>\n\n<pre><code># Compute Density:\n        param = .paramGH(zeta, rho, lambda)\n        ans = dgh(x, param[1], param[2], param[3], param[4], lambda, log)\n</code></pre>\n\n<p>but I am not getting the idea behind the code. dgh is just giving the moments of the generalized hyperbolic distribution, see <a href=\"http://help.rmetrics.org/fBasics/dist-gh.html\" rel=\"nofollow\">here</a>? So where is the density? How does he do the modification to get a standardized version? Where is the theory behind this? So there must be a puplished paper or so, where it is described how to get the standardized version? I cannot find this?</p>\n",
    "tags": "r,distributions",
    "answers": [
      "<p>First of all, \"Standardised\" in the location-scale family also means: location $0$ and scale $1$, not always mean $0$ and variance $1$. They coincide in the Normal distribution, but not in general. This applies to several of your questions. So, be careful about this.</p>\n\n<ul>\n<li>dgh is just giving the moments of the generalized hyperbolic distribution, see here?</li>\n</ul>\n\n<p>No, the standardised density is implemented in the command <code>dgsh()</code> as stated in your first link which is more reliable. </p>\n\n<ul>\n<li>So where is the density?</li>\n</ul>\n\n<p>See the link in the last point. See also the manual of the <code>fBasics</code> package.</p>\n\n<ul>\n<li>How does he do the modification to get a standardized version? Where is the theory behind this? </li>\n</ul>\n\n<p>See the link in the last point. </p>\n\n<ul>\n<li>So there must be a puplished paper or so, where it is described how to get the standardized version? I cannot find this?</li>\n</ul>\n\n<p>Section 2.3.5 of the following manual</p>\n\n<p><a href=\"http://cran.r-project.org/web/packages/rugarch/vignettes/Introduction_to_the_rugarch_package.pdf\" rel=\"nofollow\">http://cran.r-project.org/web/packages/rugarch/vignettes/Introduction_to_the_rugarch_package.pdf</a></p>\n\n<p>I found it by googling \"standardized generalized hyperbolic distribution\". Remember: google is your friend.</p>\n"
    ],
    "createdAt": "2025-07-12T08:02:20Z",
    "updatedAt": "2025-07-12T08:02:20Z",
    "similar_questions": [
      156619,
      209413,
      94521,
      156633,
      211709
    ]
  },
  {
    "id": 238627,
    "title": "Testing for difference in effect between two regressors",
    "body": "<p>So I have been given two regressors in a multivariate regression and their standard ERRORS:\nBeta1 = -0.0028\nBeta2 = -0.0010\nSE1 = 0.0008\nSE2 = 0.0007</p>\n\n<p>furthermore I can assume that the test-statistic I construct is normally distributed, so the critical value for a test at the 5% level of signicance is 1.96.</p>\n\n<p>I need to know whether whether these Betas have different effects on y. So I figured:</p>\n\n<p>Null-hypothesis = Beta1 - Beta2 = 0\nAlternative hypothesis = Beta 1 - Beta2 is not zero</p>\n\n<p>I tried to calculate the t-statistic, but I'm confused. So far I've only learned how to do this if I know the standard deviations and N. But here I only know the standard error???</p>\n\n<p>*edit: There are NO datasets. Beta1 is the effect of drinking of male on wage, and Beta2 is the effect of drinking of females on wage</p>\n",
    "tags": "hypothesis testing,self study",
    "answers": [
      "No answers available."
    ],
    "createdAt": "2025-07-12T08:02:20Z",
    "updatedAt": "2025-07-12T08:02:20Z",
    "similar_questions": [
      109234,
      133441,
      57627,
      222278,
      26748
    ]
  },
  {
    "id": 57807,
    "title": "Confusion with Augmented Dickey Fuller test",
    "body": "<p>I am working on the data set <code>electricity</code> available in R package <code>tsa</code>. My aim is to find out if an <code>arima</code> model will be appropriate for this data and eventually fit it. So I proceeded as follows: <br/> <br/>1st: Plot the time series which resulted if the following graph: <img src=\"http://i.stack.imgur.com/2t3Qk.png\" alt=\"ts plot1\"> <br/> <br/> 2nd: I wanted to take log of <code>electricity</code> to stabilize variance and afterward differenced the series as appropriate, but just before doing so, I tested for stationarity on the original data set using the <code>adf</code> (Augmented Dickey Fuller) test and surprisingly, it resulted as follows: <br/> <br/></p>\n\n<h3>Code and Results:</h3>\n\n<pre><code>adf.test(electricity)\n\n             Augmented Dickey-Fuller Test\ndata:  electricity \nDickey-Fuller = -9.6336, Lag order = 7, p-value = 0.01 \nalternative hypothesis: stationary\nWarning message: In adf.test(electricity) : p-value smaller than printed p-value\n</code></pre>\n\n<p>Well, as per my beginner's notion of time series, I suppose it means that the data is stationary (small p-value, reject null hypothesis of non-stationarity). But looking at the ts plot, I find no way that this can be stationary. Does anyone has a valid explanation for this?</p>\n",
    "tags": "r,time series",
    "answers": [
      "<p>Assuming that \"adf.test\" really comes from the \"tseries\" package (directly or indirectly), the reason would be that it automatically includes a linear time trend. From the tseries doc (version 0.10-35): \"The general regression equation which incorporates a constant and a linear trend is used [...]\" So the test result indeed indicates trend stationarity (which despite the name is not stationary). </p>\n\n<p>I also agree with Pantera that the seasonal effects could distort the result. The series could in reality be a time trend + deterministic seasonals + stochastic unit root process, but the ADF test might mis-interpret the seasonal fluctuations as stochastic reversions to the deterministic trend, which would imply roots smaller than unity. (On the other hand, given that you have included enough lags, this should rather show up as (spurious) unit roots at seasonal frequencies, not the zero/long-run frequency that the ADF test looks at. In any case, given the seasonal pattern it's better to include the seasonals.)</p>\n",
      "<p>Since you take the default value of k in <code>adf.test</code>, which in this case is 7, you're basically testing if the information set of the past 7 months helps explain $x_t - x_{t-1}$. Electricity usage has strong seasonality, as your plot shows, and is likely to be cyclical beyond a 7-month period. If you set k=12 and retest, the null of unit root cannot be rejected, </p>\n\n<pre><code>&gt; adf.test(electricity, k=12)\n\nAugmented Dickey-Fuller Test\ndata:  electricity\nDickey-Fuller = -1.9414, Lag order = 12, p-value = 0.602\nalternative hypothesis: stationary\n</code></pre>\n"
    ],
    "createdAt": "2025-07-12T08:02:20Z",
    "updatedAt": "2025-07-12T08:02:20Z",
    "similar_questions": [
      94521,
      94519,
      185583,
      70699,
      133441
    ]
  },
  {
    "id": 55788,
    "title": "Interpreting probability conditions from question",
    "body": "<p>I've encountered this question:</p>\n\n<p><img src=\"http://i.stack.imgur.com/LmgEf.jpg\" alt=\"http://i.stack.imgur.com/zlE1w.jpg\"></p>\n\n<p>And got the answer here:</p>\n\n<p><img src=\"http://i.stack.imgur.com/W2jMY.jpg\" alt=\"enter image description here\"></p>\n\n<p>However, what I don't quite understand is how are the two conditions derived from the question in the first place.</p>\n",
    "tags": "probability,self study",
    "answers": [
      "<p>The conditions are not derived from that question. The conditions are imposed on any function which can be considered as a probability distribution. </p>\n"
    ],
    "createdAt": "2025-07-12T08:02:20Z",
    "updatedAt": "2025-07-12T08:02:20Z",
    "similar_questions": [
      94572,
      94519,
      179329,
      185425,
      133493
    ]
  },
  {
    "id": 25513,
    "title": "Binary classification problem",
    "body": "<p>I have a binary classification problem. \nMy inputs consist of a time-series of values plus some binary values.\nFor real-valued inputs I would usually use a neural network, while for binary-values inputs I would use either a neural network with a step function or some other method fit for binary inputs (like a maximum entropy classifier)</p>\n\n<p>In the case where input types are mixed what architecture should be used?</p>\n",
    "tags": "machine learning,classification",
    "answers": [
      "<p>You could continue to use NN with the binary values as an additional input feature. The activation function would be a sigmoid. You will end up with an output vector where the values  where the class values with either be high or low. You could then classify the high values as 1 and the low values a 0</p>\n"
    ],
    "createdAt": "2025-07-12T08:02:20Z",
    "updatedAt": "2025-07-12T08:02:20Z",
    "similar_questions": [
      57790,
      135442,
      185625,
      166892,
      121465
    ]
  },
  {
    "id": 209369,
    "title": "Estimate the probability of the distribution of a sample",
    "body": "<p>Suppose I have the sample of a continuous variable and I want to estimate the probability of that variable taking certain values. Suppose in addition that I estimate the density of that distribution with the density() function of R and the shape doesn't resemble any known distribution.</p>\n\n<p>In that case, how can I estimate the probability of that variable taking some value? Can I just add up the values returned by the density() function on the range that I'm interested?</p>\n\n<p>Thanks in advance.</p>\n",
    "tags": "r,distributions",
    "answers": [
      "<p>I would use the ecdf() function. Let's try this with i.i.d draws from a N(500, 75).</p>\n\n<pre><code>set.seed(1)\ntemp&lt;-rnorm(10000, 500, 75)\n</code></pre>\n\n<p>Now we calculate the empirical CDF and plot it...just to validate that it has the shape we would expect:</p>\n\n<pre><code>temp_cdf&lt;-ecdf(temp) # generate empirical CDF for temp\nplot(temp_cdf(seq(275, 725, length.out = 1000)),type='l') # plot cdf +/- 3 SDs from mean\n</code></pre>\n\n<p>Note that temp_cdf() is a function that will give you the CDF value at any value of temp. You can use this to calculate the probability that temp is between any given pair of values. Below we confirm that ~99.7% of the data are within 3 standard deviations of the mean.</p>\n\n<pre><code>&gt; temp_cdf(725)-temp_cdf(275)\n[1] 0.9972\n</code></pre>\n"
    ],
    "createdAt": "2025-07-12T08:02:20Z",
    "updatedAt": "2025-07-12T08:02:20Z",
    "similar_questions": [
      133493,
      104928,
      57801,
      94642,
      185425
    ]
  },
  {
    "id": 77915,
    "title": "How to describe the failure of this linear modelling?",
    "body": "<p>I have a time series $X_t$, which is shown in the first plot. In the second plot, I am doing a linear regression on $X_t\\sim X_{t-1}$. The regression line is very close to $y=x$. But this is tricky since if if we look at the bottom left or the top right part of the data, they are almost random. From the diagnosis of residuals, it is not a good regression either. But the model passes all the $t$ tests and $F$ tests. How can I say it's not a good model then? Is there a statistic  to describe (not visually) the failure of this modelling?</p>\n\n<p>Here are the <code>R</code> codes I used to generate the plots:</p>\n\n<pre><code># Generating X_t\nx=c(arima.sim(list(order = c(1,0,0),ar=0.1),n=64,sd=1),3,5,7,11,14,17,rep(20,64)+arima.sim(list(order = c(1,0,0),ar=0.1),n=64,sd=1))\n# Regression X_t~X_{t-1}\nreg=lm(x[2:length(x)]~x[1:(length(x)-1)])\n# Plotting\npar(mfrow=c(3,2))\nplot(x,xlab='',ylab=expression(X[t]),ty='l')\nplot(x[1:(length(x)-1)],x[2:length(x)],xlab=expression(x[t-1]),ylab=expression(x[t]) ,main=paste('coeff= ',round(reg$coefficients[2],2)))\n# Plotting the regression line\nabline(reg,col=2)\n# Plotting the residual diagnose\nplot(reg)\n</code></pre>\n\n<p><img src=\"http://i.stack.imgur.com/2xJ65.png\" alt=\"enter image description here\"></p>\n",
    "tags": "r,regression,hypothesis testing",
    "answers": [
      "<p>The regression does what you ask, and quite well in regression terms. The real question to me is why you think it interesting or useful. </p>\n\n<p>You simulated a time series which is one regime with small fluctuations followed by another regime with small fluctuations, with a short transition between the two. More precisely, if I decipher your code correctly, it is one autoregressive scheme with mean 0 followed by another with mean 20 with a steep ramp in between. (As like myself many people here don't use R routinely, a verbal explanation would have been helpful.) </p>\n\n<p>That being so, $x_t \\approx x_{t-1}$ is a good overall approximation and that's essentially what your regression reports. But you've discovered, indirectly, what you invented in the first place. </p>\n\n<p>Otherwise put, it is inevitable that your data are two big blobs plus transitional values, so a straight line is a fair descriptor, but that just follows from the simulation rule. </p>\n"
    ],
    "createdAt": "2025-07-12T08:02:21Z",
    "updatedAt": "2025-07-12T08:02:21Z",
    "similar_questions": [
      156619,
      94519,
      179329,
      57807,
      94521
    ]
  },
  {
    "id": 200822,
    "title": "binary logistic regression p values problem",
    "body": "<p>i made different models . in first I took a dependent variable and four independent variables .\nin second model I took different dependent variable and similar independent variable \nlike wise I made four models \nbut when I ran binary logistic regression I found similar p values in all models despite of different dependent variables could this happen or am I making any mistake \nactually I code  the dependent variable as 0 and 1 in all models\nand independent variables in all models are same as bmi, whr, age and % body fat then p values in binary logistic regression become similar I am confused here rather the dependent variables are also linked with each other but in original test results they have different values </p>\n",
    "tags": "regression,logistic",
    "answers": [
      "<p>I don't think you're necessarily making a mistake. One possibility is that the different sets of independent variables that you're choosing are heavily correlated to each other (across different sets).</p>\n",
      "<p>\"Similar\" p values can certainly happen, especially if the dependent variables are related to each other. </p>\n\n<p>However, without seeing your code it's not possible to say for sure what you did or whether it was a mistake.  </p>\n\n<p>E.g. suppose one DV was \"Voted for McCain\" and another was \"Voted for Romney\" and another was \"Voted for Obama in 2008\".  Those would give very similar p values. </p>\n"
    ],
    "createdAt": "2025-07-12T08:02:21Z",
    "updatedAt": "2025-07-12T08:02:21Z",
    "similar_questions": [
      211696,
      156680,
      104978,
      57790,
      26790
    ]
  },
  {
    "id": 57830,
    "title": "How to correctly model noise?",
    "body": "<p>Assume a linear mixing model $x = As$, where $x = (x_{0}, ..., x_{n})^T$ are linear mixtures of $s = (s_{0}, ..., s_{n})^T$, and $A$ is the mixing matrix.</p>\n\n<p>Now, if I introduce additive noise to this system, that is,\n$x = As + e$, where $e = (e_{0}, ..., e_{n})$, what are the relations between $e_{0}, ..., e_{n}$?</p>\n\n<p>For example, assume I have three microphones ($x_{0}, x_{1}, x_{2}$) in a room. There are also three speakers ($s_0, s_1, s_2$) and some background noise ($e$). If we assume the noise is white gaussian (or laplacian, or uniform), and that the noise level is static over the room, do I get the same noise into each of the microphones, that is $e_0 = e_1 = e_2$? If not, why?</p>\n",
    "tags": "time series,self study",
    "answers": [
      "<p>The error terms (e0, e1, e2) are independent.  They could have the same mean and variance (e.g., mean=0, sd=1, \"white\"), but are still independently generated.</p>\n\n<p>The noise reaching the microphones could be the result of a variety of unknown processes --- each microphone could be at a different point in the room, some could be of higher quality than others, etc.  The background noise, \"e\", could be one of several factors influencing the departure of x from the model's process.  The line between observation error and process error can be fuzzy at times, but either way there are reasons why the series of error terms should be independent between your state (x) variables.</p>\n\n<p>When I've done similar simulations, my error terms have been generated independently.</p>\n"
    ],
    "createdAt": "2025-07-12T08:02:21Z",
    "updatedAt": "2025-07-12T08:02:21Z",
    "similar_questions": [
      77915,
      57790,
      94581,
      70699,
      179336
    ]
  },
  {
    "id": 222278,
    "title": "Is autocorrelation not worth addressing with small N?",
    "body": "<p>Consider a simple regression context in which there is a small set of response values, $Y$, and corresponding dates, $X$.  (For simplicity, we can assume the dates are equally spaced.)  We would like to regress $Y$ onto the time variable, but there may be autocorrelations that would make the standard errors / $p$-values, etc., invalid.  One approach would be to use an autocorrelation-consistent variance-covariance matrix for the betas and construct SEs and tests manually from it.  </p>\n\n<p>My questions are:  </p>\n\n<ol>\n<li>Can it be the case that this just isn't worth it?  How does the value of this vary as a function of $N$ and the magnitude of the autocorrelations?  </li>\n<li>Does the distribution of $Y$ matter (e.g., if the data are conditionally normal, vs. binomial, vs. Poisson)?  </li>\n<li>Would it make a difference if the true data generating process had an MA term but no actual AR terms?  </li>\n</ol>\n\n<p><sub> For context, this thread comes from the discussion in comments here: <a href=\"http://stats.stackexchange.com/a/221972/7290\">Is there a nonparametric test available in R to test for a trend in a binary variable?</a>  </sub></p>\n",
    "tags": "regression,time series",
    "answers": [
      "No answers available."
    ],
    "createdAt": "2025-07-12T08:02:21Z",
    "updatedAt": "2025-07-12T08:02:21Z",
    "similar_questions": [
      121408,
      133552,
      156619,
      156633,
      211709
    ]
  },
  {
    "id": 200827,
    "title": "Using environmental evidence to improve control estimations",
    "body": "<p>I have a question with regards to incorporating evidence into an estimation, which feels like it should be relatively straight forward, but for some reason I'm struggling with. </p>\n\n<p>I have a controller, modelled as an MDP, which based on its current state can choose an action most likely to achieve a desired result. Each of these actions are stochastic, and depending on what happens during an execution, can transition the system into one of many states. There is full observability, such that while the actions are stochastic, the system always knows the resulting state it has entered into. There is a probability distribution over these states representing which one the system is most likely to end up in, however the actual result is determined during execution.</p>\n\n<p><a href=\"http://i.stack.imgur.com/jBmMq.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/jBmMq.png\" alt=\"Example action with three possible outcomes.\"></a></p>\n\n<p>My problem is I would like to update these distributions based on the evidence seen during an execution. If for example we have the situation in the diagram above, the system selects the action with the expectation that 70 percent of the time it will transition into state 2. If due to some external factor it consistently finds itself ending in state 3 I would like to update this function in order to reflect that.</p>\n\n<p>I initially thought a simple update using bayes rule would be the answer, but there is no uncertainty about the observation. I know the prior, for example 0.7 above, but the observation is a given, we know exactly what state we end up in. Reading through a lot of the posts here I then started looking into using dirichlet priors, as we have a multinomial distribution over our states. This seems the most sensible answer I've found, and I implemented it but it has a fundamental flaw for this work. This method seems to be trying to learn an underlying distribution based on a series of evidence, however in my case the environment can constantly change. If we observe s1 15 times in a row we should be more confident about that being the future result, however it creates too strong a prior such that if we then observe s2 3 or 4 times in a row it has little impact on the estimations. </p>\n\n<p>This isn't my area so I feel there is something I'm missing, but I want to continually update an action's estimation based on observing its outcomes. I've been looking through statistical books/blogs and lots of papers on reinforcement learning, however I'm coming up short. If anyone has any ideas or could even point me in a direction it would be appreciated!</p>\n",
    "tags": "machine learning,probability",
    "answers": [
      "No answers available."
    ],
    "createdAt": "2025-07-12T08:02:21Z",
    "updatedAt": "2025-07-12T08:02:21Z",
    "similar_questions": [
      55788,
      135573,
      94572,
      94519,
      185425
    ]
  },
  {
    "id": 104988,
    "title": "What is the difference between a loss function and decision function?",
    "body": "<p>I see that both functions are part of data mining methods such as <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html\">Gradient</a> Boosting Regressors. I see that those are separate objects too.</p>\n\n<p>How is the relationship between both in general?</p>\n",
    "tags": "regression,classification",
    "answers": [
      "<p>The loss function is what is minimized to obtain a model which is optimal in some sense. The model itself has a decision function which is used to predict.</p>\n\n<p>For example, in SVM classifiers:</p>\n\n<ul>\n<li>loss function: minimizes error and squared norm of the separating hyperplane $\\mathcal{L}(\\mathbf{w}, \\xi) =\\frac{1}{2}\\|\\mathbf{w}\\|^2 + C\\sum_i \\xi_i$</li>\n<li>decision function: signed distance to the separating hyperplane: $f(\\mathbf{x}) = \\mathbf{w}^T\\mathbf{x} + b$</li>\n</ul>\n",
      "<p>A <strong>decision function</strong> is a function which takes a dataset as input and gives a decision as output. What the decision can be depends on the problem at hand. Examples include:</p>\n\n<ul>\n<li><em>Estimation problems:</em> the \"decision\" is the estimate.</li>\n<li><em>Hypothesis testing problems:</em> the decision is to reject or not reject the null hypothesis.</li>\n<li><em>Classification problems:</em> the decision is to classify a new observation (or observations) into a category.</li>\n<li><em>Model selection problems:</em> the decision is to chose one of the candidate models.</li>\n</ul>\n\n<p>Typically, there are an infinite number of decision functions available for a problem. If we for instance are interested in estimating the height of Swedish males based on ten observations $\\mathbf{x}=(x_1,x_2,\\ldots,x_{10})$, we can use any of the following decision functions $d(\\mathbf{x})$:</p>\n\n<ul>\n<li>The sample mean: $d(\\mathbf{x})=\\frac{1}{10}\\sum_{i=1}^{10}x_i$.</li>\n<li>The median of the sample: $d(\\mathbf{x})=\\mbox{median}(\\mathbf{x})$</li>\n<li>The geometric mean of the sample: $d(\\mathbf{x})=\\sqrt[10]{x_1\\cdots x_{10}}$</li>\n<li>The function that always returns 1: $d(\\mathbf{x})=1$, regardless of the value of $\\mathbf{x}$. Silly, yes, but it is nevertheless a valid decision function.</li>\n</ul>\n\n<p>How then can we determine which of these decision functions to use? One way is to use a <strong>loss function</strong>, which describes the loss (or cost) associated with all possible decisions. Different decision functions will tend to lead to different types of mistakes. The loss function tells us which type of mistakes we should be more concerned about. The best decision function is the function that yields the lowest <strong>expected loss</strong>. What is meant by expected loss depends on the setting (in particular, whether we are talking about <a href=\"http://en.wikipedia.org/wiki/Frequentist_inference\" rel=\"nofollow\">frequentist</a> or <a href=\"http://en.wikipedia.org/wiki/Bayesian_statistics\" rel=\"nofollow\">Bayesian</a> statistics).</p>\n\n<p><strong>In summary:</strong> </p>\n\n<ul>\n<li>Decision functions are used to make decisions based on data.</li>\n<li>Loss functions are used to determine which decision function to use.</li>\n</ul>\n"
    ],
    "createdAt": "2025-07-12T08:02:22Z",
    "updatedAt": "2025-07-12T08:02:22Z",
    "similar_questions": [
      179336,
      185624,
      411,
      94642,
      57850
    ]
  },
  {
    "id": 109304,
    "title": "Good machine learning algo for partial derivatives?",
    "body": "<p>Does anyone know of good robust algos to estimate partial derivatives of a regression model? I am talking about a general regression model like this:</p>\n\n<p>$\\mathbb{E}(y|x_1, x_2, ... x_n) = f(x_1, x_2, ... x_n)$, where I want estimates of $\\frac{\\partial f}{\\partial x_k}$.</p>\n\n<p>These are of great importance in medicine, economics, social sciences, etc. Now a linear model would give an approximation of these partial derivatives (its parameters) around the mean point. </p>\n\n<p>This approximation can be made valid further around the mean by the inclusion of polynomial terms, but the exponential increase in numbers of parameters as the polynomial degree rises makes this infeasible with a linear model (except perhaps with Lasso?).</p>\n\n<p>For better local estimates over the whole input space, natural candidates would be kernel regression and feedforward neural networks, which have smooth forms (unlike tree-based methods) and from which partial derivatives are easy to recover. But I don't know about the robustness of these estimates, as the main aim of these models is to reduce prediction error, not find the right derivatives...</p>\n\n<p>I know gradient boosting (and really all regression models) give a \"partial dependance\", but they are not partial derivatives: they are a function $f_k(x_k)$ that gives the mean value of the predicted regression function $\\widehat{f}$ for a given value of $x_k$, averaging over the realisations of the other variables $(x_1, x_2 ... x_n)$ in the training dataset.</p>\n\n<p>Any help would be greatly appreciated!</p>\n",
    "tags": "regression,machine learning",
    "answers": [
      "<p>I am not sure if this is what you are searching for, but I'll give it a try.</p>\n\n<p>I think you can use Gaussian process regression and also get a nice robustness measure. </p>\n\n<p>Let's say you have $m$ observations $x_1, ..., x_m\\in \\mathbb R^n$ and a target variable $y_i \\in \\mathbb R$ for each $x_i$. For a Gaussian process with covariance function $k$, the joint density of a function value $f$ at an new point $x^*$ (the idea is that $y=f+\\varepsilon$ where $\\varepsilon$ is Gaussian noise) is given by</p>\n\n<p>$$\\left(\\begin{array}{c}\n\\mathbf{f}\\\\\nf^{*}\n\\end{array}\\right)=\\mathcal{N}\\left(\\mathbf{0},\\left(\\begin{array}{cc}\nK+\\sigma^{2}I &amp; \\mathbf{k}^{*}\\\\\n\\mathbf{k}^{*} &amp; k\\left(x^{*},x^{*}\\right)\n\\end{array}\\right)\\right)$$\nwhere $\\mathcal N$ denotes the Gaussian distribution, $\\mathbf K$ is the matrix of pairwise covariance function values between the $x_i$ and $\\mathbf k^*$ is the vector of covariance function values between $x^*$ and all $x_i$ from the training set. </p>\n\n<p>From that it is easy to compute the posterior mean (i.e. the prediction at $x^*$) and covariance for $f^*$ (i.e. the measure of certainty which I think you mean with robustness). For all the formulae see the <a href=\"http://www.gaussianprocess.org/gpml/chapters/\" rel=\"nofollow\">book by Rasmussen and Williams</a>.</p>\n\n<p>The interesting fact is that if $k(x_i,x_j)=\\mbox{cov}(f_i,f_j)$, then $$\\frac{\\partial}{\\partial x_{i\\ell}}k(x_i,x_j)=\\mbox{cov}\\left(\\frac{\\partial}{\\partial x_{i\\ell}} f_i,f_j\\right)$$ and $$\\frac{\\partial^2}{\\partial x_{i\\ell}\\partial x_{jt}}k(x_i,x_j)=\\mbox{cov}\\left(\\frac{\\partial}{\\partial x_{i\\ell}} f_i,\\frac{\\partial}{\\partial x_{jt}}f_j\\right)$$ (again, check chapter 9.4 of Rasmussen and Williams). This means, you can just treat the partial derivatives as \"unobserved function value $f^*$\" and adapt the covariance functions in the above equation to the respective partial derivatives of the original covariance function. Then you simply predict the derivative like you would predict $f^*$. As a measure of certainty, you can look at the posterior covariance which is easily computes as well. </p>\n\n<p>There is a nice  <a href=\"http://mloss.org/software/view/263/\" rel=\"nofollow\">Gaussian process toolbox for matlab</a> written by Hannes Nickisch. It can probably help you play around with GPs.</p>\n"
    ],
    "createdAt": "2025-07-12T08:02:22Z",
    "updatedAt": "2025-07-12T08:02:22Z",
    "similar_questions": [
      133502,
      26790,
      135442,
      179336,
      156619
    ]
  },
  {
    "id": 185625,
    "title": "Classification problem with statistically insignificant variables",
    "body": "<p>I am working on a binary classification problem taking one categorical and four numeric variables. I started with t-test and logistic regression, which resulted in high p-values for all the variables I considered. </p>\n\n<p>As, all the variables I considered are statistically insignificant, what should be my next approach for the classification task?</p>\n",
    "tags": "regression,logistic,classification",
    "answers": [
      "<p>If your predictors are not significant, that generally means they are not useful for predicting your variable of interest. </p>\n\n<p>So you need to find new predictors, preferably some that are strongly related to your variable of interest. </p>\n",
      "<p>This only means that there are no linear relationships between predictors and the decision; there is still a chance some more complex method would find them (obviously at a greater risk of overfitting).</p>\n\n<p>You may give it a try with random forest, it finds much more complex iterations than regression and still is pretty hard to overfit.</p>\n",
      "<p>To summarize previous two posts: </p>\n\n<ul>\n<li><p>You can either try to find new predictors which have higher predicting power. This is called <em>feature engineering</em> (<a href=\"http://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/\" rel=\"nofollow\">http://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/</a>). </p></li>\n<li><p>You can try and use a more complicated (non-linear) classifier. An overview in which you can try to select the right one specific to your demands, is given in the book the Elements of Statistical Learning or in this presentation: <a href=\"http://web.engr.oregonstate.edu/~tgd/classes/534/slides/part2.pdf\" rel=\"nofollow\">http://web.engr.oregonstate.edu/~tgd/classes/534/slides/part2.pdf</a>. </p></li>\n</ul>\n"
    ],
    "createdAt": "2025-07-12T08:02:22Z",
    "updatedAt": "2025-07-12T08:02:22Z",
    "similar_questions": [
      57790,
      104978,
      200822,
      880,
      121465
    ]
  },
  {
    "id": 135555,
    "title": "Decision Tree English Rules and Dependency Network in MS SSAS",
    "body": "<p>I created a Decision Tree model in Microsoft Analysis Services (SSAS, Visual Studio 2010). There are two tabs in the Mining Model Viewer tab: (1) Decision Tree that shows a tree itself, and (2) Dependency Network that shows the chart of most important variables.</p>\n\n<p><img src=\"http://i.stack.imgur.com/pJjbi.png\" alt=\"enter image description here\"></p>\n\n<p>On the Decision Tree tab I can click on each individual leaf and see the English Rule for that leaf. Is there a way to get ALL the rules at once as a list with the case numbers?</p>\n\n<p>The Dependency Network tab has a slider that you can move to see which variables influence the decision tree most. Is it possible to get ALL important variables as a list with their \"importance\" number?</p>\n",
    "tags": "machine learning,classification",
    "answers": [
      "<p>I have found a way to get the <strong>list of ALL English Rules</strong> from the Mining Structure.</p>\n\n<pre><code>SELECT * FROM [MyModel].CONTENT WHERE [CHILDREN_CARDINALITY] = 0\n</code></pre>\n\n<p><strong>[CHILDREN_CARDINALITY] = 0</strong> defines the DT leaves only.</p>\n\n<p>The following links may be helpful: <a href=\"https://msdn.microsoft.com/en-us/library/cc645903.aspx\" rel=\"nofollow\">Decision Trees Model Query Examples</a>, <a href=\"https://msdn.microsoft.com/en-us/library/cc645758.aspx\" rel=\"nofollow\">Mining Model Content for Decision Tree Models</a></p>\n\n<p><strong>\"FLATTENED\"</strong> keyword will split [NODE_DISTRIBUTION] outcomes into separate rows.</p>\n\n<pre><code>SELECT FLATTENED [MODEL_CATALOG],[MODEL_NAME],[NODE_CAPTION],[CHILDREN_CARDINALITY],[NODE_DESCRIPTION],[NODE_PROBABILITY],[NODE_DISTRIBUTION],[NODE_SUPPORT] FROM [MyModel].CONTENT WHERE [CHILDREN_CARDINALITY] = 0 ORDER BY [NODE_SUPPORT] DESC\n</code></pre>\n"
    ],
    "createdAt": "2025-07-12T08:02:23Z",
    "updatedAt": "2025-07-12T08:02:23Z",
    "similar_questions": [
      94572,
      55788,
      94519,
      104928,
      179329
    ]
  },
  {
    "id": 135565,
    "title": "Optimizing a time-series with multiple predictors",
    "body": "<p>I have a few questions about turing a univariate time series into a multivariate time series and optimizing the predictors. Here is the univariate data:</p>\n\n<pre><code>index\n22\n26\n34\n33\n40\n39\n39\n45\n50\n58\n64\n78\n51\n60\n80\n80\n93\n100\n96\n108\n111\n119\n140\n164\n103\n112\n154\n135\n156\n170\n146\n156\n166\n176\n193\n204\n</code></pre>\n\n<p>My first step here was to of course create a ts object in R and visualize the data:</p>\n\n<pre><code>tsData &lt;- ts(data = dummyData, start = c(2012,1), end = c(2014,12), frequency = 12)\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n2012  22  26  34  33  40  39  39  45  50  58  64  78\n2013  51  60  80  80  93 100  96 108 111 119 140 164\n2014 103 112 154 135 156 170 146 156 166 176 193 204\n\nplot(tsData)\n</code></pre>\n\n<p>I interpreted this plot as a deterministic time series with a trend and perhaps a bit of seasonality</p>\n\n<p><img src=\"http://i.stack.imgur.com/YD0vW.png\" alt=\"enter image description here\">\nExamining the acf and pacf plot confirms the trend component of the time series</p>\n\n<p><img src=\"http://i.stack.imgur.com/c8eu3.png\" alt=\"enter image description here\"></p>\n\n<p>My first question has to do with creating trend &amp; seasonal variables for the time series using the decompose() function in R which yields the following plots:</p>\n\n<p><img src=\"http://i.stack.imgur.com/9xXYx.png\" alt=\"enter image description here\"></p>\n\n<p>I understand that the decompose() function in R has created a list of vectors for the trend, seasonal and random components of the original time series but what am I suppose to do with them? Should I cbind() them to my univariate data and model:</p>\n\n<pre><code>lm(index ~ trend + seasonal + random)\n\n         index     trend    seasonal       random\nJan 2012    22        NA -23.8940972           NA\nFeb 2012    26        NA -19.4357639           NA\nMar 2012    34        NA   6.8350694           NA\nApr 2012    33        NA  -7.5399306           NA\nMay 2012    40        NA   4.3142361           NA\nJun 2012    39        NA   9.5017361           NA\nJul 2012    39  45.20833  -6.3524306   0.14409722\nAug 2012    45  47.83333  -0.8315972  -2.00173611\nSep 2012    50  51.16667  -1.1232639  -0.04340278\nOct 2012    58  55.04167   2.2517361   0.70659722\nNov 2012    64  59.20833  11.2100694  -6.41840278\nDec 2012    78  63.95833  25.0642361 -11.02256944\nJan 2013    51  68.87500 -23.8940972   6.01909722\nFeb 2013    60  73.87500 -19.4357639   5.56076389\nMar 2013    80  79.04167   6.8350694  -5.87673611\nApr 2013    80  84.12500  -7.5399306   3.41493056\nMay 2013    93  89.83333   4.3142361  -1.14756944\nJun 2013   100  96.58333   9.5017361  -6.08506944\nJul 2013    96 102.33333  -6.3524306   0.01909722\nAug 2013   108 106.66667  -0.8315972   2.16493056\nSep 2013   111 111.91667  -1.1232639   0.20659722\nOct 2013   119 117.29167   2.2517361  -0.54340278\nNov 2013   140 122.20833  11.2100694   6.58159722\nDec 2013   164 127.75000  25.0642361  11.18576389\nJan 2014   103 132.75000 -23.8940972  -5.85590278\nFeb 2014   112 136.83333 -19.4357639  -5.39756944\nMar 2014   154 141.12500   6.8350694   6.03993056\nApr 2014   135 145.79167  -7.5399306  -3.25173611\nMay 2014   156 150.37500   4.3142361   1.31076389\nJun 2014   170 154.25000   9.5017361   6.24826389\nJul 2014   146        NA  -6.3524306           NA\nAug 2014   156        NA  -0.8315972           NA\nSep 2014   166        NA  -1.1232639           NA\nOct 2014   176        NA   2.2517361           NA\nNov 2014   193        NA  11.2100694           NA\nDec 2014   204        NA  25.0642361           NA\n</code></pre>\n\n<p>When I use the auto.arima function in the forecast package is this all happening under the hood? It seems to me that the auto.arima() selected a MA(1) term and a differencing term to deal with the trend? Is my interpretation correct? What is drift?</p>\n\n<pre><code>plot(forecast(auto.arima(tsData, stepwise=FALSE)))\n\nForecast method: ARIMA(0,0,1)(0,1,0)[12] with drift        \n\nModel Information:\nSeries: tsData \nARIMA(0,0,1)(0,1,0)[12] with drift         \n\nCoefficients:\n         ma1   drift\n      0.9622  4.5780\ns.e.  0.4698  0.4352\n\nsigma^2 estimated as 176.6:  log likelihood=-44.52\nAIC=95.05   AICc=96.25   BIC=98.58\n\nError measures:\n                    ME     RMSE      MAE        MPE     MAPE       MASE\nTraining set 0.2459764 7.673429 4.967187 -0.7272714 4.661455 0.08876581\n                   ACF1\nTraining set -0.0791942\n</code></pre>\n\n<p><img src=\"http://i.stack.imgur.com/3TWL6.png\" alt=\"enter image description here\"></p>\n\n<p>What happens if I'm interested in expanding the model to include other time series variables such as spend_1 and spend_2? do I need to create trend and seasonal and random variables for each of these spend variables or do I just plug them into the auto.arima as external variables:</p>\n\n<pre><code>auto.ariam(tsData, xreg=spendData, stepwise= FALSE)\n\nspend_1 spend_2\n0   0\n0   0\n0   0\n0   0\n0   0\n0   209\n0   0\n0   0\n0   239\n0   0\n0   553\n0   216\n0   0\n0   161\n0   449\n107 0\n53  0\n120 81\n242 0\n100 80\n482 0\n708 81\n54  240\n688 0\n80  0\n254 108\n183 84\n104 191\n183 84\n243 167\n0   108\n0   0\n0   191\n0   191\n0   167\n0   0\n</code></pre>\n\n<p>Once I build this multivariate time series model how do I interpret the coefficients for spend_1 and spend_2? How do to optimize them in order to maximize the index variable where the model was something like:</p>\n\n<pre><code>lm(index ~ spend_1 + spend_2 + trend + seasonal + random)\n</code></pre>\n\n<p>Thanks all for the advice please let me know if I can clarify anything further.</p>\n",
    "tags": "r,time series",
    "answers": [
      "No answers available."
    ],
    "createdAt": "2025-07-12T08:02:23Z",
    "updatedAt": "2025-07-12T08:02:23Z",
    "similar_questions": [
      94519,
      25381,
      57807,
      77915,
      179329
    ]
  },
  {
    "id": 57850,
    "title": "Link Anomaly Detection in Temporal Network",
    "body": "<p>I came across this paper that uses link anomaly detection to predict trending topics, and I found it incredibly intriguing: The paper is <em><a href=\"http://arxiv.org/abs/1110.2899\">\"Discovering Emerging Topics in Social Streams via Link Anomaly Detection\"</a></em>.</p>\n\n<p>I would love to replicate it on a different data set, but I'm not familiar enough with the methods to know how to use them. Let's say I have a series of snapshots of network of nodes across a period of six months. The nodes have a long-tailed degree distribution, with most having only a few connections, but some having a great many. New nodes appear within this time period.</p>\n\n<p>How could I implement sequentially discounted normalized maximum likelihood calculations used in the paper to detect anomalous links that I think might be precursors to a burst? Are there other methods that would be more appropriate?</p>\n\n<p>I ask both theoretically and practically. If someone could point me to a way to implement this in python or R, that would be very helpful.</p>\n\n<p>Anyone? I know you smart folks out there have some starting thoughts for an answer,</p>\n",
    "tags": "time series,machine learning",
    "answers": [
      "No answers available."
    ],
    "createdAt": "2025-07-12T08:02:23Z",
    "updatedAt": "2025-07-12T08:02:23Z",
    "similar_questions": [
      121408,
      185583,
      179336,
      26728,
      121465
    ]
  },
  {
    "id": 94638,
    "title": "What do the coefficients of the crossproduct of regression mean?",
    "body": "<p>How can I interpret the coefficients of the crossproduct of each of the following codes? What do they mean? How can I deduce that they correspond to our expectation? Also which crossproduct is correct? (1) or (2)? What's the difference? The data are at the bottom:</p>\n\n<pre><code>fit.model = lm(formula = CO ~ ., data = cigarettes)\nCall:\nlm(formula = CO ~ ., data = cigarettes)\n\nCoefficients:\n(Intercept)          Tar     Nicotine       Weight  \n     3.2022       0.9626      -2.6317      -0.1305  \nX = model.matrix(fit.model)\n&gt; X\n                 (Intercept)  Tar Nicotine Weight\nAlpine                     1 14.1     0.86 0.9853\nBenson&amp;Hedges              1 16.0     1.06 1.0938\nBullDurham                 1 29.8     2.03 1.1650\nCamelLights                1  8.0     0.67 0.9280\nCarlton                    1  4.1     0.40 0.9462\nChesterfield               1 15.0     1.04 0.8885\nGoldenLights               1  8.8     0.76 1.0267\nKent                       1 12.4     0.95 0.9225\nKool                       1 16.6     1.12 0.9372\nL&amp;M                        1 14.9     1.02 0.8858\nLarkLights                 1 13.7     1.01 0.9643\nMarlboro                   1 15.1     0.90 0.9316\nMerit                      1  7.8     0.57 0.9705\nMultiFilter                1 11.4     0.78 1.1240\nNewportLights              1  9.0     0.74 0.8517\nNow                        1  1.0     0.13 0.7851\nOldGold                    1 17.0     1.26 0.9186\nPallMallLight              1 12.8     1.08 1.0395\nRaleigh                    1 15.8     0.96 0.9573\nSalemUltra                 1  4.5     0.42 0.9106\nTareyton                   1 14.5     1.01 1.0070\nTrue                       1  7.3     0.61 0.9806\nViceroyRichLight           1  8.6     0.69 0.9693\nVirginiaSlims              1 15.2     1.02 0.9496\nWinstonLights              1 12.0     0.82 1.1184\nattr(,\"assign\")\n[1] 0 1 2 3\n\n(1) result=t(X) %*% X\n\n            (Intercept)       Tar  Nicotine    Weight\n(Intercept)     25.0000  305.4000  21.91000  24.25710\nTar            305.4000 4501.2000 314.67100 302.17874\nNicotine        21.9100  314.6710  22.21050  21.63176\nWeight          24.2571  302.1787  21.63176  23.72096\n(2) XbyX &lt;- crossprod(X)\n            (Intercept)       Tar  Nicotine    Weight\n(Intercept)     25.0000  305.4000  21.91000  24.25710\nTar            305.4000 4501.2000 314.67100 302.17874\nNicotine        21.9100  314.6710  22.21050  21.63176\nWeight          24.2571  302.1787  21.63176  23.72096\n\n&gt; dput(cigarettes)\nstructure(list(Tar = c(14.1, 16, 29.8, 8, 4.1, 15, 8.8, 12.4, \n16.6, 14.9, 13.7, 15.1, 7.8, 11.4, 9, 1, 17, 12.8, 15.8, 4.5, \n14.5, 7.3, 8.6, 15.2, 12), Nicotine = c(0.86, 1.06, 2.03, 0.67, \n0.4, 1.04, 0.76, 0.95, 1.12, 1.02, 1.01, 0.9, 0.57, 0.78, 0.74, \n0.13, 1.26, 1.08, 0.96, 0.42, 1.01, 0.61, 0.69, 1.02, 0.82), \n    Weight = c(0.9853, 1.0938, 1.165, 0.928, 0.9462, 0.8885, \n    1.0267, 0.9225, 0.9372, 0.8858, 0.9643, 0.9316, 0.9705, 1.124, \n    0.8517, 0.7851, 0.9186, 1.0395, 0.9573, 0.9106, 1.007, 0.9806, \n    0.9693, 0.9496, 1.1184), CO = c(13.6, 16.6, 23.5, 10.2, 5.4, \n    15, 9, 12.3, 16.3, 15.4, 13, 14.4, 10, 10.2, 9.5, 1.5, 18.5, \n    12.6, 17.5, 4.9, 15.9, 8.5, 10.6, 13.9, 14.9)), .Names = c(\"Tar\", \n\"Nicotine\", \"Weight\", \"CO\"), class = \"data.frame\", row.names = c(\"Alpine\", \n\"Benson&amp;Hedges\", \"BullDurham\", \"CamelLights\", \"Carlton\", \"Chesterfield\", \n\"GoldenLights\", \"Kent\", \"Kool\", \"L&amp;M\", \"LarkLights\", \"Marlboro\", \n\"Merit\", \"MultiFilter\", \"NewportLights\", \"Now\", \"OldGold\", \"PallMallLight\", \n\"Raleigh\", \"SalemUltra\", \"Tareyton\", \"True\", \"ViceroyRichLight\", \n\"VirginiaSlims\", \"WinstonLights\"))\n</code></pre>\n",
    "tags": "r,regression",
    "answers": [
      "<p>This seems rather confused to me.  It is probably best for you to simply think of $X'X$ as being a computational step in the process of calculating your beta estimates that does not have any independent meaning.  Here are some answers to your specific questions:  </p>\n\n<ul>\n<li>There are no coefficients in your model fit for the sum of squares and crossproducts of your design matrix (i.e., <code>crossprod(X)</code>).  You are fitting a model with an intercept and slopes for <code>Tar</code>, <code>Nicotine</code>, and <code>Weight</code>.  </li>\n<li>Since there are no such coefficients, there can be no meaningful answer to what they mean.  </li>\n<li>I do not understand what is meant by whether \"they correspond to our expectation\".  </li>\n<li><p>There is no difference between (1) and (2).  Both are correct (for what they are).  </p>\n\n<pre><code>&gt; identical(crossprod(X), t(X) %*% X)\n[1] TRUE\n</code></pre></li>\n</ul>\n\n<hr>\n\n<p>I wonder if you are trying to think through the nature of <em>interactions</em> in multiple regression.  To form an interaction term, you would multiply two variables and enter their product as a new variable in the model.  For example:  </p>\n\n<pre><code>within(cigarettes, new.var = Tar*Nicotine)\nfit.model2 = lm(CO~Weight+  Tar + Nicotine + new.var, data=cigarettes)\n # or:\nfit.model2 = lm(CO~Weight + Tar*Nicotine,             data=cigarettes)\nsummary(fit.model2)\n# ...\n# Coefficients:\n#              Estimate Std. Error t value Pr(&gt;|t|)    \n# ... \n# Tar:Nicotine -0.20997    0.06211  -3.380  0.00297 ** \n# ...\n</code></pre>\n\n<p>The meaning of the coefficient on the interaction term (<code>Tar:Nicotine -0.20997</code>) is difficult for people to interpret in isolation, and it may be best for you not to try.  The fact that the interaction is significant implies that the effect of <code>Tar</code>, e.g., depends on the level of <code>Nicotine</code>.  Beyond that, to see how this plays out in a specific case, it is best to hold one of those constant at a specific level that seems meaningful in your situation and examine (e.g., plot) the relationship between the other interacting variable and the response.  To understand interactions further, some of the information in my answers below may be helpful for you:  </p>\n\n<ul>\n<li><a href=\"http://stats.stackexchange.com/a/49701/7290\">Interaction in generalized linear model</a></li>\n<li><a href=\"http://stats.stackexchange.com/a/84316/7290\">What does \u00e2\u0080\u009call else equal\u00e2\u0080\u009d mean in multiple regression?</a></li>\n</ul>\n"
    ],
    "createdAt": "2025-07-12T08:02:24Z",
    "updatedAt": "2025-07-12T08:02:24Z",
    "similar_questions": [
      145657,
      25381,
      190067,
      133441,
      185583
    ]
  },
  {
    "id": 135573,
    "title": "Hidden Markov Model vs Markov Transition Model vs State-Space Model...?",
    "body": "<p>For my master's thesis, I am working on developing a statistical model for the transitions between different states, defined by serological status. For now, I won't give too many details into this context, as my question is more general/theoretical. Anyway, my intuition is that I should be using a Hidden Markov Model (HMM); the trouble I am coming across as I go through the literature and other background research necessary to formulate my model is confusion over terminology and the exact differences between different types of hidden process models. I am only very vaguely aware of what distinguishes them (examples to come). Further, it seems to me that, at least from what I have seen in the literature, there is a very non-standard vocabulary built up around this type of modeling, and on occasion I see terms used interchangeably in one context but contrasted in another. </p>\n\n<p>So, I was hoping people can help me disambiguate some of these terms for me. I have a number of questions, but I am guessing that as one or two get answered satisfactory the rest will become disentangled as a result. I hope this isn't too long-winded; if a moderator wants me to split this up into multiple posts I will. In any case, I've put my questions in bold, followed by the details of the question that I've uncovered during my literature search. </p>\n\n<p>So, in no particular order:</p>\n\n<p>1) <strong>What exactly is a \"hidden process model\"?</strong></p>\n\n<p>I have been operating under the impression that \"hidden process model\" is sort of an umbrella term that can be used to describe a number of different types of statistical models, all essentially probabilistic descriptions of time series data generated by \"a system of overlapping, potentially hidden, linearly additive processes\" ([1]). Indeed, [2] defines a \"hidden process model\" as \"a general term referring to to either a state-space model or a hidden Markov model.\" [1] seems to infer that a hidden Markov model is a subtype of hidden process models specifically geared towards inference on binary states; the basic implication seems to me that a hidden process model is a generalization of a hidden Markov model. I sometimes see \"hidden process model\" AND the phrase \"hidden process dynamic model\", but it is not clear to me that these are distinct concepts.</p>\n\n<p>Is this intuition on my part correct? If not, does anybody have a reference that more clearly delineates these methods?  </p>\n\n<p>2) <strong>What is the difference between a Hidden Markov Model and a state-space model?</strong></p>\n\n<p>Again returning to [2] (if only because the paper comes with a clear glossary of terms, not because the paper itself seems to be particularly authoritative; it is just a convenient source of one-sentence definitions), the difference seems to be that a Hidden Markov Model is a specific type of state-space model in which the states are Markovian (there doesn't seem to be a definite restriction on the order of the Markov process; i.e. first order,...,kth order). Here, a state-space model is defined as \"A model that runs two time series in parallel, one captures the dynamic of the true states (latent) and the other consists of observations that are made from these underlying but possibly unknown states.\" If those states also exhibit the Markov property, then it is a Hidden Markov Model.</p>\n\n<p>However, [3] defines the difference between state-space models and Hidden Markov Models as being related to the characteristics of the latent state. Here, a Hidden Markov Model deals with discrete states while state-space models deal with continuous states; otherwise, they are conceptually identical. </p>\n\n<p>These seem to me to be two very different definitions. Under one, a Hidden Markov Model is a subtype of state-space model, while under the other they are both just different instantiations of a broader class of hidden process models. Which of these is correct? My intuition points me to follow [3] as opposed to [2], but I can't find an authoritative source that supports this. </p>\n\n<p>3) <strong>What is a \"Markov transition model\"?</strong></p>\n\n<p>Another term that has come up in a lot of sources is \"Markov transition model\". I have not been able to find this phrase in any textbooks, but it appears a lot in journal articles (simply plug it into Google to confirm). I haven't been able to find a rigorous definition of the term (every paper I find cites another paper, which cites another, etc., sending me down a PubMed rabbit hole that leads nowhere sane). My impression from context is that it is a very general term to refer to any model in which the object of inference is the transitions between states that follow a Markov process, and that a Hidden Markov Model may be considered a specific type of Markov transition model. [4], however, seems to use transition model, Hidden Markov Model, and several similar terms interchangeably. </p>\n\n<p>On the other hand, [5] talks about Markov transition models and Hidden Markov Models a bit differently. The authors state: \"Transition models provide a method for summarising\nrespondent dynamics that are helpful for interpreting results from more complex hidden Markov models\". I don't entirely understand what they mean by this phrase, and can't find a justification for it elsewhere in the paper. However, they seem to imply that Markov transition models use time as a continuous variable, while hidden Markov models use time as a discrete variable (they don't directly say this; they say they use the R package 'msm' to fit Markov transition models, and later describe 'msm' as treating time continuously in contrast to the R package for HMMs). </p>\n\n<p>4) <strong>Where do other concepts, for example Dynamic Bayesian Networks, fit in?</strong></p>\n\n<p>According to Wikipedia, a Dynamic Bayesian Network is a \"generalization of hidden Markov models and Kalman filters\". Elsewhere, I have seen hidden Markov models defined as a special case of a Dynamic Bayesian Network, \"which the entire state of the world is represented by a single hidden state variable\" (<a href=\"http://stats.stackexchange.com/questions/44702/definition-of-dynamic-bayesian-system-and-its-relation-to-hmm\">Definition of dynamic Bayesian system, and its relation to HMM?</a>). I generally understand this relationship, and it is well explained by [6]. </p>\n\n<p>However, I am having a hard time understanding how this relationship fits in the broader picture of things. That is, given this relationship between HMMs and DBNs, how are state-space models and hidden process models related to the two? How do all of these different types of methods interrelate, given that there seem to be multiple \"generalizations\" of hidden Markov models?</p>\n\n<hr>\n\n<p>References:</p>\n\n<p>[1] Tom M. Mitchell, Rebecca Hutchinson, Indrayana Rustandi. \"Hidden Process Models\". 2006. CMU-CALD-05-116. Carnegie Mellon University.</p>\n\n<p>[2] Oliver Giminez, Jean-Dominique Lebreton, Jean-Michel Gaillard, Remi Choquet, Roger Pradel. \"Estimating demographic parameters using hidden process dynamic models\". Theoretical Population Biology. 2012. 82(4):307-316. </p>\n\n<p>[3] Barbara Engelhardt. \"Hidden Markov Models and State Space Models\". STA561: Probabilistic machine learning. Duke University. <a href=\"http://www.genome.duke.edu/labs/engelhardt/courses/scribe/lec_09_25_2013.pdf\">http://www.genome.duke.edu/labs/engelhardt/courses/scribe/lec_09_25_2013.pdf</a></p>\n\n<p>[4] Jeroen K. Vermunt. \"Multilevel Latent Markov Modeling in Continuous Time with an Application to the Analysis of Ambulatory Mood Assessment Data\". Social Statistics Workshop. 2012. Tilburg University. <a href=\"http://www.lse.ac.uk/statistics/events/SpecialEventsandConferences/LSE2013-Vermunt.pdf\">http://www.lse.ac.uk/statistics/events/SpecialEventsandConferences/LSE2013-Vermunt.pdf</a></p>\n\n<p>[5] Ken Richardson, David Harte, Kristie Carter. \"Understanding health and labour force transitions: Applying Markov models to SoFIE longitudinal data\". Official Statistics Research Series. 2012. </p>\n\n<p>[6] Zoubin Ghahramani. \"An Introduction to Hidden Markov Models and Bayesian Networks\". Journal of Pattern Recognition and Artificial Intelligence. 2001. 15(1): 9-42.</p>\n",
    "tags": "machine learning,self study",
    "answers": [
      "<p>I and Alan Hawkes have written quite a lot about aggregated Markov processes with discrete states in continuous time. Our stuff has been about the problem of interpreting observations  of single ion channel molecules, and includes an exact treatment of missed short events. Similar theory works in reliability theory too.  It might well be adapted to other problems. \nSee <a href=\"http://www.onemol.org.uk/?page_id=175\" rel=\"nofollow\">http://www.onemol.org.uk/?page_id=175</a> for references.</p>\n",
      "<p>The following is  quoted from the <a href=\"http://www.scholarpedia.org/article/State_space_model\" rel=\"nofollow\">Scholarpedia website</a>:</p>\n\n<blockquote>\n  <p>State space model (SSM) refers to a class of probabilistic graphical model (Koller and Friedman, 2009) that describes the probabilistic dependence between the latent state variable and the observed measurement. The state or the measurement can be either continuous or discrete. The term \u00e2\u0080\u009cstate space\u00e2\u0080\u009d originated in 1960s in the area of control engineering (Kalman, 1960). SSM provides a general framework for analyzing deterministic and stochastic dynamical systems that are measured or observed through a stochastic process. The SSM framework has been successfully applied in engineering, statistics, computer science and economics to solve a broad range of dynamical systems problems. Other terms used to describe SSMs are hidden Markov models (HMMs) (Rabiner, 1989) and latent process models. The most well studied SSM is the Kalman filter, which defines an optimal algorithm for inferring linear Gaussian systems.</p>\n</blockquote>\n"
    ],
    "createdAt": "2025-07-12T08:02:24Z",
    "updatedAt": "2025-07-12T08:02:24Z",
    "similar_questions": [
      179336,
      200822,
      135514,
      200827,
      209413
    ]
  },
  {
    "id": 94642,
    "title": "What exactly does the 'boxcox' function in R do?",
    "body": "<p>I am familiar with the power transform family and I know how to estimate the MLE for $\\lambda$ for given samples of a random variable. I have been using the 'boxcox' function in R for a sample of a random variable that depends linearly on a latent variable (i.e. a linear model) and it works very well. The problem is that I don't quite know how it works on such linear models.</p>\n\n<p>The <a href=\"https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/boxcox.html\" rel=\"nofollow\">official reference</a> won't give clear clues on how to answer my question. Can you provide some information on the answer to my question?</p>\n",
    "tags": "r,regression",
    "answers": [
      "<p>This is not a complete answer, but the real official reference is arguably \"the code\". In this case it's a little harder to find (it's a method, and it's hidden behind a namespace), but this command shows the code for the default method.</p>\n\n<pre><code>library('MASS')\ngetAnywhere('boxcox.default')\n</code></pre>\n\n<p>The result is the following (see also <code>getAnywhere('boxcox.lm')</code>):</p>\n\n<pre><code>A single object matching \u00e2\u0080\u0098boxcox.default\u00e2\u0080\u0099 was found\nIt was found in the following places\n  registered S3 method for boxcox from namespace MASS\n  namespace:MASS\nwith value\n\nfunction (object, lambda = seq(-2, 2, 1/10), plotit = TRUE, interp = (plotit &amp;&amp; \n    (m &lt; 100)), eps = 1/50, xlab = expression(lambda), ylab = \"log-Likelihood\", \n    ...) \n{\n    if (is.null(y &lt;- object$y) || is.null(xqr &lt;- object$qr)) \n        stop(gettextf(\"%s does not have both 'qr' and 'y' components\", \n            sQuote(deparse(substitute(object)))), domain = NA)\n    if (any(y &lt;= 0)) \n        stop(\"response variable must be positive\")\n    n &lt;- length(y)\n    y &lt;- y/exp(mean(log(y)))\n    logy &lt;- log(y)\n    xl &lt;- loglik &lt;- as.vector(lambda)\n    m &lt;- length(xl)\n    for (i in 1L:m) {\n        if (abs(la &lt;- xl[i]) &gt; eps) \n            yt &lt;- (y^la - 1)/la\n        else yt &lt;- logy * (1 + (la * logy)/2 * (1 + (la * logy)/3 * \n            (1 + (la * logy)/4)))\n        loglik[i] &lt;- -n/2 * log(sum(qr.resid(xqr, yt)^2))\n    }\n    if (interp) {\n        sp &lt;- spline(xl, loglik, n = 100)\n        xl &lt;- sp$x\n            loglik &lt;- sp$y\n        m &lt;- length(xl)\n    }\n    if (plotit) {\n        mx &lt;- (1L:m)[loglik == max(loglik)][1L]\n        Lmax &lt;- loglik[mx]\n        lim &lt;- Lmax - qchisq(19/20, 1)/2\n        dev.hold()\n        on.exit(dev.flush())\n        plot(xl, loglik, xlab = xlab, ylab = ylab, type = \"l\", \n            ylim = range(loglik, lim))\n        plims &lt;- par(\"usr\")\n        abline(h = lim, lty = 3)\n        y0 &lt;- plims[3L]\n        scal &lt;- (1/10 * (plims[4L] - y0))/par(\"pin\")[2L]\n        scx &lt;- (1/10 * (plims[2L] - plims[1L]))/par(\"pin\")[1L]\n        text(xl[1L] + scx, lim + scal, \" 95%\", xpd = TRUE)\n        la &lt;- xl[mx]\n        if (mx &gt; 1 &amp;&amp; mx &lt; m) \n            segments(la, y0, la, Lmax, lty = 3)\n        ind &lt;- range((1L:m)[loglik &gt; lim])\n        if (loglik[1L] &lt; lim) {\n            i &lt;- ind[1L]\n            x &lt;- xl[i - 1] + ((lim - loglik[i - 1]) * (xl[i] - \n                xl[i - 1]))/(loglik[i] - loglik[i - 1])\n            segments(x, y0, x, lim, lty = 3)\n        }\n        if (loglik[m] &lt; lim) {\n            i &lt;- ind[2L] + 1\n            x &lt;- xl[i - 1] + ((lim - loglik[i - 1]) * (xl[i] - \n                xl[i - 1]))/(loglik[i] - loglik[i - 1])\n            segments(x, y0, x, lim, lty = 3)\n        }\n    }\n    list(x = xl, y = loglik)\n}\n&lt;bytecode: 0x7ff6e0973120&gt;\n&lt;environment: namespace:MASS&gt;\n</code></pre>\n"
    ],
    "createdAt": "2025-07-12T08:02:24Z",
    "updatedAt": "2025-07-12T08:02:24Z",
    "similar_questions": [
      185624,
      179336,
      209369,
      211677,
      77922
    ]
  },
  {
    "id": 156723,
    "title": "Formula to derive probabilities when using 'gbm' method through 'caret' package in R",
    "body": "<p>I am training a classification problem by 'gbm' algorithm through 'caret' package in r.The response variable is a yes/nope type. Here 'objmodel' is the model I trained through method='gbm' and package 'caret'. Everything went smoothly but the problem is I would like to know the exact way the probabilities are computed so that I can take that formula/logic and apply that logic in a spreadsheet for example where there are the same predictor variables instead of relying on the R environment.</p>\n\n<pre><code># probabilites \nlibrary(pROC)\npredictions &lt;- predict(object=objModel, testDF[,predictorsNames], type='prob')\nhead(predictions)\n##      nope    yes\n## 1 0.07292 0.9271\n## 2 0.76058 0.2394\n## 3 0.43309 0.5669\n## 4 0.67279 0.3272\n## 5 0.67279 0.3272\n## 6 0.54616 0.4538\n</code></pre>\n\n<p>Kindly let me know if that is possible. I used <a href=\"http://amunategui.github.io/binary-outcome-modeling/\" rel=\"nofollow\">http://amunategui.github.io/binary-outcome-modeling/</a> as my guide.</p>\n",
    "tags": "r,probability",
    "answers": [
      "No answers available."
    ],
    "createdAt": "2025-07-12T08:02:24Z",
    "updatedAt": "2025-07-12T08:02:24Z",
    "similar_questions": [
      145657,
      57790,
      70699,
      185624,
      104978
    ]
  },
  {
    "id": 209413,
    "title": "Why is it necessary to fix a matrix diagonal and after this calculate the exponential to assess transition probabilities?",
    "body": "<p>I'm learning markov chains in order to compute estimations of transition probabilities, and I found an example of the estimator construction for continuous time markov chains:</p>\n\n<p><a href=\"http://www.rinfinance.com/agenda/2015/talk/AlexanderMcNeil.pdf\" rel=\"nofollow\">http://www.rinfinance.com/agenda/2015/talk/AlexanderMcNeil.pdf</a></p>\n\n<p>(pages 10 -12)</p>\n\n<p>In its presentation, the author specifies that it's necessary to fix matrix diagonal and replace it by the negative of row sums without considering it's initial value:</p>\n\n<pre><code>    D &lt;- rep(0, dim(Lambda.hat)[2])\n    Lambda.hat &lt;-rbind(Lambda.hat,D)\n    diag(Lambda.hat) &lt;- D\n    rowsums &lt;- apply(Lambda.hat,1,sum)\n    diag(Lambda.hat) &lt;- -rowsums\n</code></pre>\n\n<p>After that, he computes estimated transition probabilities:</p>\n\n<pre><code>    P.hat &lt;- expm(Lambda.hat)\n</code></pre>\n\n<p>The output is consistent because each row adds up to one so it seems it works. Although, I don't understand why it was necessary to make these two steps and would really appreciate any help.</p>\n",
    "tags": "r,probability",
    "answers": [
      "<p>There is no \"initial\" value of the diagonal entries in the infinitesimal generator matrix, a.k.a. transition rate matrix, I'll call it Q, of a continuous time Markov Chain. The author shows how to estimate the off-diagonal transition entries of the infinitesimal generator matrix.  As is always the case with an infinitesimal generator matrix, each row must sum to zero, as described in <a href=\"https://en.wikipedia.org/wiki/Transition_rate_matrix\" rel=\"nofollow\">https://en.wikipedia.org/wiki/Transition_rate_matrix</a> .  Therefore, the diagonal entries of the infinitesimal generator matrix are set equal to the negative of the sum of the entries in the corresponding row.  </p>\n\n<p>As described at <a href=\"https://en.wikipedia.org/wiki/Continuous-time_Markov_chain#Transient_behaviour\" rel=\"nofollow\">https://en.wikipedia.org/wiki/Continuous-time_Markov_chain#Transient_behaviour</a> , the transition probability matrix is calculated as $e^{t Q}$, where t is the length of time, in units corresponding to the intensity parameter entries in Q.  The author in your link performed such calculation with annual intensity rates and t = 1, and therefore the transition probability matrix in his example corresponds to a one year period.  The mathematics works out that if the row sums of Q equal zero (as they must), then the row sums of the transition probability matrix equal one, as is the case in the example in the link.</p>\n"
    ],
    "createdAt": "2025-07-12T08:02:25Z",
    "updatedAt": "2025-07-12T08:02:25Z",
    "similar_questions": [
      57801,
      185583,
      25387,
      135573,
      135490
    ]
  },
  {
    "id": 133441,
    "title": "Computing the power of Fisher's exact test in R",
    "body": "<p>Suppose that I have the following 3 x 3 contingency table:</p>\n\n<pre><code>      |    T1   |    T2   |    T3   |\n------+---------+---------+---------+---\n  G1  |   18    |   15    |   65    | \n------+---------+---------+---------+---\n  G2  |   20    |   10    |   70    |\n------+---------+---------+---------+---\n  G3  |   15    |   55    |   30    |\n</code></pre>\n\n<p>Then I can run the Fisher's exact test (using the Monte Carlo simulation option) in R as follows:</p>\n\n<pre><code>table = matrix(c(18,20,15,15,10,55,65,70,30), 3, 3)\nfisher.test(table, simulate.p.value=TRUE)\n</code></pre>\n\n<p>, yielding the following result (p-value):</p>\n\n<pre><code>Fisher's Exact Test for Count Data with simulated p-value (based on\n    2000 replicates)\n\ndata:  table\np-value = 0.0004998\nalternative hypothesis: two.sided\n</code></pre>\n\n<p>In my hypothesis testing I would also be interested in observing the probability of a type-II error (false-negative rate, beta) as well as the probability of correctly rejecting the null hypothesis (H0) when it is false (statistical power, 1- beta). Does anybody know whether this is possible with R and Fisher's exact test on m x n (with m > 2 and n > 2) contingency tables? Do you maybe know some other tools I can use to calculate this? Thanks!</p>\n\n<p>P.S.: Please note that the above contingency table is just an example and the real obtained p-values can be > alpha (in my case 0.05) which is the more interesting case.</p>\n",
    "tags": "r,hypothesis testing",
    "answers": [
      "<p>What you are asking for here is a post-hoc power analysis.  (More specifically, \"the probability of correctly rejecting the null hypothesis\" is the power, and 1-power is beta, \"the probability of a type-II error\".  You ask for both, but we only need one to know the other.)  We take your existing dataset as the alternative hypothesis / model of the true data generating process.  I don't know of a specialized, pre-existing function (e.g., in the <a href=\"http://cran.r-project.org/web/packages/pwr/index.html\" rel=\"nofollow\">pwr</a> package) to do this, but, yes, this can be done in <code>R</code>.  You will just have to simulate it.  For (considerably) more information on power analyses, and simulating them in <code>R</code>, you should read my answer here: <a href=\"http://stats.stackexchange.com/a/35994/7290\">Simulation of logistic regression power analysis - designed experiments</a>.  In this case, I will just give a quick, adapted version for dealing with Fisher's exact test.  (I usually write code as close to pseudocode as possible so that it may be more widely understood, but because this has the potential of taking so long to run, I try to move as much as possible out of the <code>for</code> loop, and use some of <code>R</code>'s unique capacities.)  </p>\n\n<pre><code>table = matrix(c(18,20,15,15,10,55,65,70,30), 3, 3)\ntable\n#      [,1] [,2] [,3]\n# [1,]   18   15   65\n# [2,]   20   10   70\n# [3,]   15   55   30\nN = sum(table)                  # this is the total number of observations\nN\n# [1] 298\nprobs = prop.table(table)       # these are the probabilities of an observation\nprobs                           #  being in any given cell\n#            [,1]       [,2]      [,3]\n# [1,] 0.06040268 0.05033557 0.2181208\n# [2,] 0.06711409 0.03355705 0.2348993\n# [3,] 0.05033557 0.18456376 0.1006711\nprobs.v = as.vector(probs)      # notice that the probabilities read column-wise\nprobs.v\n# [1] 0.06040268 0.06711409 0.05033557 0.05033557 0.03355705 0.18456376 0.21812081\n# [8] 0.23489933 0.10067114\ncuts = c(0, cumsum(probs.v))    # notice that I add a 0 on the front\ncuts\n# [1] 0.00000000 0.06040268 0.12751678 0.17785235 0.22818792 0.26174497\n# [7] 0.44630872 0.66442953 0.89932886 1.00000000\n\nset.seed(4941)                  # this makes it exactly reproducible\nB      = 10000                  # number of iterations in simulation\nvals   = runif(N*B)             # generate random values / probabilities\ncats   = cut(vals,breaks=cuts, labels=c(\"11\",\"21\",\"31\",\"12\",\"22\",\"32\",\"13\",\"23\",\"33\"))\ncats   = matrix(cats, nrow=N, ncol=B, byrow=F)\ncounts = apply(cats, 2, function(x){ as.vector(table(x)) })\n\nrm(table, N, vals, probs, probs.v, cuts, cats) \np.vals = vector(length=B)       # this will store the outputs\nptm = proc.time()               # this lets me time the simulation\nfor(i in 1:B){\n  mat       = matrix(counts[,i], nrow=3, ncol=3, byrow=T)\n  p.vals[i] = fisher.test(mat, simulate.p.value=T)$p.value\n}\nproc.time() - ptm               # not too bad, really\n#  user  system elapsed \n# 28.66    0.32   29.08 \n#\nmean(p.vals&gt;=.05)               # the estimated probability of type II errors is 0\n# [1] 0\nc(0, 3/B)                       # using the rule of 3 to estimate the 95% CI\n# [1] 0e+00 3e-04\n</code></pre>\n\n<p>Given how far your data diverge from the null hypothesis in Fisher's exact test, and the amount of data you have, this simulation does not turn up a single type II error in 10,000 iterations.  Because each iteration can be understood as a draw from a binomial distribution with probability $p$ (which we are estimating as the proportion of type II errors observed), this simulation is actually an estimate with some stochastic variability.  We can form a 95% confidence interval bounding the true probability of a type II error.  To get around the fact that we didn't actually find any type II errors, we will use the <a href=\"http://en.wikipedia.org/wiki/Rule_of_three_%28statistics%29\" rel=\"nofollow\">rule of 3</a> ($3/N$) to estimate the upper limit of the CI.  Thus, the 95% CI for true type II error rate is $[0,\\ 0.0003]$.  </p>\n\n<hr>\n\n<p>On a different note, @rvl points out in the comments that \"[p]ost hoc power is a silly exercise\".  That is largely true.  I have seen people make the argument, in effect, 'my results are not significant, but I don't have any power, so there's no reason to believe my theory isn't right', which is fairly bizarre on any number of levels.  On the other hand, since your results are significant, it isn't clear what difference knowing the post-hoc power for your study is either.  I find that understanding post-hoc power can be useful pedagogically to help people begin to understand the topic.  And we can also take this as a starting point for a-priori power analyses for planning future studies.  </p>\n"
    ],
    "createdAt": "2025-07-12T08:02:25Z",
    "updatedAt": "2025-07-12T08:02:25Z",
    "similar_questions": [
      26748,
      57807,
      94521,
      190067,
      145657
    ]
  },
  {
    "id": 190106,
    "title": "On skewed distributions, should the test/train/cross validation set be similar to the real set",
    "body": "<p>On a skewed distribution (say there are two classes and the distribution is 2%-98%), given the small number of examples of one of the classes, would it be correct to have them distributed 50%-50% for the training, testing and cross validation data sets? or will that produce a useless predictive model as the real data will indeed contain only 2% of elements of a given class?</p>\n\n<p>The reason I am asking is because with such a distribution, the number of training examples of one category remain really small in comparison to the other. </p>\n\n<p>My intuition says that having testing set with a large number of training examples of both classes will help, however my (little) statistics knowledge seems to ring a HUGE alarm in my brain? </p>\n\n<p>Is my intuition wrong? If so, why?</p>\n",
    "tags": "regression,machine learning,distributions",
    "answers": [
      "<p>It depends on whether your learning algorithm can account for the unbalanced dataset. In my experience the option <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\" rel=\"nofollow\">class_weight='balanced' in SGDClassifier in sklearn</a> has shown much better performance as random downsampling of the majority class in 94%/6% to a 6%/6% (i.e 50/50) distribution.</p>\n\n<p>Generally one would expect that diminishing information by downsampling the majority class or inflating the minority class by upsampling should not outperform an algorithm that can make proper use of that information.</p>\n\n<p>When you can not explicitly embed this knowledge into your algorithm you might very well see better model performance in a 50/50 training set scenario. </p>\n\n<p>Just make sure that you still use the original ratio dataset for validation and testing. You might also adjust the scorer function to reflect the weight (and importance) of the classes if you are optimizing the hyper-parameters. </p>\n"
    ],
    "createdAt": "2025-07-12T08:02:25Z",
    "updatedAt": "2025-07-12T08:02:25Z",
    "similar_questions": [
      880,
      173,
      55718,
      133552,
      411
    ]
  },
  {
    "id": 77922,
    "title": "Show that if $X\\ge 0$ , $E(X)\\le \\sum_{n=0}^{\\infty}P(X>n)$",
    "body": "<p>If $X$ is a random variable and also let $X\\ge 0$. </p>\n\n<p>I want to show $E(X)\\le \\sum_{n=0}^{\\infty}P(X&gt;n)$.</p>\n",
    "tags": "probability,self study",
    "answers": [
      "<p>You don't specify anything about $X$. Is it the general case/what is its support? </p>\n\n<p>If it is for discrete r.v.s, can you say something about the relationship between </p>\n\n<p>$\\sum_{n = 0}^{\\infty} n P(X = n)$  and $\\sum_{n = 0}^{\\infty} P(X &gt; n)$?</p>\n\n<p>Consider:</p>\n\n<p>\\begin{eqnarray}\n&amp;0 P(0)&amp; +\\, 1&amp; P(1)&amp; +\\, 2 &amp;P(2)&amp; +\\, 3 &amp;P(3)&amp; +&amp; ...&amp;\\\\\n&amp; &amp;\\\\\n&amp;= &amp;  [ &amp;P(1)&amp; +&amp;P(2)&amp; +&amp;P(3)&amp; +&amp; ...&amp;]\\\\\n&amp; &amp;  [   &amp; &amp; +&amp;P(2)&amp; +&amp;P(3)&amp; + &amp;...&amp;]\\\\\n&amp; &amp;  [   &amp; &amp;  &amp;    &amp; +&amp;P(3)&amp; + &amp;...&amp;]\\\\\n&amp; &amp;  [&amp; &amp; &amp; &amp; &amp; &amp; &amp;...&amp;]\n\\end{eqnarray}</p>\n\n<p>Can you see a way to do it now? (Though I believe this approach establishes a stronger result than you have)</p>\n\n<p>To consider it more generally than the discrete case, see <a href=\"http://stats.stackexchange.com/questions/18438/does-a-univariate-random-variables-mean-always-equal-the-integral-of-its-quanti\">here</a>, and then adapt the above idea. </p>\n\n<p>That is, can you see how establish a relationship between $\u00e2\u0088\u0091^\u00e2\u0088\u009e_{n=0}P(X&gt;n)$ and a similar-looking integral that would the give the required inequality?</p>\n",
      "<p>You have</p>\n\n<p>$$EX=\\int_0^{\\infty}xdF(x)$$</p>\n\n<p>Notice that $dF(x)=-d(1-F(x))$ and that $P(X&gt;t)=1-F(t)$ and use integration by parts. </p>\n\n<p>Now show that for monotone decreasing positive function </p>\n\n<p>$$\\sum_{n=0}^\\infty f(n)\\ge\\int_0^{\\infty} f(t) dt$$</p>\n\n<p>Combine these two results and you get your desired result. Hint for the second, recall Riemman sums.</p>\n",
      "<p>Define the sets $A_n=\\{x\\in \\mathbb{R}:x&gt;n\\}$, for $n=0,1,2\\dots$. </p>\n\n<p>For any fixed $\\omega$, let $n_0$ be the smallest integer such that $X(\\omega)\\leq n_0$. Since $X(\\omega)\\geq 0$, we have\n$$\n  X(\\omega)\\leq n_0 = \\sum_{n=0}^{n_0} I_{A_n} (X(\\omega)) = \\sum_{n=0}^\\infty I_{A_n} (X(\\omega)) \\, ,\n$$\nyielding\n$$\n  \\mathrm{E}[X]\\leq \\sum_{n=0}^\\infty \\mathrm{E}[I_{A_n} (X)]=\\sum_{n=0}^\\infty P(X&gt;n) \\, .\n$$</p>\n"
    ],
    "createdAt": "2025-07-12T08:02:26Z",
    "updatedAt": "2025-07-12T08:02:26Z",
    "similar_questions": [
      133502,
      179329,
      94642,
      209369,
      104928
    ]
  }
]